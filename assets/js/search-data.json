{
  
    
        "post0": {
            "title": "MNIST Digit Classifier using PyTorch",
            "content": "Background . Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. These neural networks attempt to simulate the behavior of the human brain allowing it to “learn” from large amounts of data. Not only possible to apply in large amounts of data, but it also allows to deal with unstructured data such as image, text, and sound. . You can try to implement a neural network from scratch. But, do you think this is a good idea when building deep learning models on a real-world dataset? It is definitely possible if you have days or weeks to spare waiting for the model to build. But, in any conditions we have many constrain to do it e.g time and cost. . Here is the good news, now we can use deep learning frameworks that aim to simplify the implementation of complex deep learning models. Using these frameworks, we can implement complex models like convolutional neural networks in no time. . A deep learning framework is a tool that allows us to build deep learning models more easily and quickly. They provide a clear and concise way for defining models using a collection of pre-built and optimized components. Instead of writing hundreds of lines of code, we can use a suitable framework to help us to build such a model quickly. . . Keras PyTorch . Keras was released in March 2015 | While PyTorch was released in October 2016 | . Keras has a high level API | While PyTorch has a low level API | . Keras is comparatively slower in speed | While PyTorch has a higher speed than Keras, suitable for high performance | . Keras has a simple architecture, making it more readable and easy to use | While PyTorch has very low readablility due to a complex architecture | . Keras has a smaller community support | While PyTorch has a stronger community support | . Keras is mostly used for small datasets due to its slow speed | While PyTorch is preferred for large datasets and high performance | . Debugging in Keras is difficult due to presence of computational junk | While debugging in PyTorch is easier and faster | . Keras provides static computation graphs | While PyTorch provides dynamic computation graphs | . Backend for Keras include:TensorFlow, Theano and Microsoft CNTK backend | While PyTorch has no backend implementation | . Installation . Please ensure you have installed the following packages to run sample notebook. Here is the short instruction on how to create a new conda environment with those package inside it. . Open the terminal or Anaconda command prompt. | . Create new conda environment by running the following command. . conda create -n &lt;env_name&gt; python=3.7 . | Activate the conda environment by running the following command. . conda activate &lt;env_name&gt; . | Install additional packages such as pandas, numpy, matplotlib, seaborn, and sklearn inside the conda environment. . pip install pandas numpy matplotlib seaborn scikit-learn . | Install torch into the environment. . pip3 install torch torchvision torchaudio . | . Note: For more details, please visit PyTorch setup . Libraries . After the required package is installed, load the package into your workspace using the import . import pandas as pd # for read_csv import numpy as np # for np.Inf import matplotlib.pyplot as plt # visualization import seaborn as sns # heatmap visualization from sklearn.metrics import confusion_matrix, classification_report # evaluation metrics import pickle # serialization import torch from torch.utils.data import Dataset, DataLoader, random_split import torch.nn as nn import torch.nn.functional as F import torch.optim as optim plt.style.use(&quot;seaborn&quot;) torch.manual_seed(123) . &lt;torch._C.Generator at 0x7fecaa1c7b90&gt; . Workflow . Load Data . Working with Images . Let&#39;s start implement our existing knowledge of neural network using torch in Python to solve an image classification problem. We&#39;ll use famous MNIST Handwritten Digits Data as our training dataset. It consists of 28 by 28 pixels grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents. There are two files of data to be downloaded: train.csv is pixel data with actual digit label, whereas test.csv is pixel data without the actual digit label. Here are some sample image from dataset: . . It&#39;s evident that these images are relatively small in size, and recognizing the digits can sometimes be challenging even for the human eye. While it&#39;s useful to look at these images, there&#39;s just one problem here: PyTorch doesn&#39;t know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset. . PyTorch Dataset allow us to specify one or more transformation functions that are applied to the images as they are loaded. We&#39;ll use the torch.tensor() to convert the pixel values into tensors. . class MNISTDataset(Dataset): # constructor def __init__(self, file_path): # read data df = pd.read_csv(file_path) target_col = &quot;label&quot; self.label_exist = target_col in df.columns if self.label_exist: # split feature-target X = df.drop(columns=target_col).values y = df[target_col].values # convert numpy array to tensor self.y = torch.tensor(y) else: X = df.values self.X = torch.tensor(X, dtype=torch.float32) # scaling self.X /= 255 # for iteration def __getitem__(self, idx): if self.label_exist: return self.X[idx], self.y[idx] else: return self.X[idx] # to check num of observations def __len__(self): return self.X.size()[0] . mnist_data = MNISTDataset(&quot;data_input/mnist/train.csv&quot;) print(mnist_data.X.size()) . torch.Size([42000, 784]) . SPLIT_PROP = 0.8 BATCH_SIZE = 16 . Training and Validation Datasets . While building real-world machine learning models, it is quite common to split the dataset into three parts: . Training set - used to train the model, i.e., compute the loss and adjust the model&#39;s weights using gradient descent. | Validation set - used to evaluate the model during training, adjust hyperparameters (learning rate, etc.), and pick the best version of the model. | Test set - used to compare different models or approaches and report the model&#39;s final accuracy. | . In the MNIST dataset, there are 42,000 images in train.csv and 784 images in test.csv. The test set is standardized so that different researchers can report their models&#39; results against the same collection of images. . Since there&#39;s no predefined validation set, we must manually split the 42,000 images into training, validation, and test datasets. Let&#39;s set aside 42,000 randomly chosen images for validation. We can do this using the random_split method from torch. . train_val_size = int(SPLIT_PROP * len(mnist_data)) test_size = len(mnist_data) - train_val_size train_size = int(SPLIT_PROP * train_val_size) val_size = train_val_size - train_size # train-val-test split train_mnist, val_mnist, test_mnist = random_split(mnist_data, [train_size, val_size, test_size]) print(f&quot;Training set: {len(train_mnist):,} images&quot;) print(f&quot;Validation set: {len(val_mnist):,} images&quot;) print(f&quot;Testing set: {len(test_mnist):,} images&quot;) . Training set: 26,880 images Validation set: 6,720 images Testing set: 8,400 images . It&#39;s essential to choose a random sample for creating a validation and test set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model. . We can now create DataLoader to help us load the data in batches. We&#39;ll use a batch size of 16. We set shuffle=True for the data loader to ensure that the batches generated in each epoch are different. This randomization helps generalize &amp; speed up the training process. . train_loader = DataLoader(train_mnist, batch_size=BATCH_SIZE, shuffle=True) val_loader = DataLoader(val_mnist, batch_size=BATCH_SIZE, shuffle=True) test_loader = DataLoader(test_mnist, batch_size=BATCH_SIZE, shuffle=True) print(f&quot;Training data loader: {len(train_loader):,} images per batch&quot;) print(f&quot;Validation data loader: {len(val_loader):,} images per batch&quot;) print(f&quot;Testing data loader: {len(test_loader):,} images per batch&quot;) . Training data loader: 1,680 images per batch Validation data loader: 420 images per batch Testing data loader: 525 images per batch . Visualize Data . Let&#39;s check the proportion of each class labels, since it is important to ensure that the model can learn each digit in a balanced and fair manner. We check the distribution of digits in training, validation, and also test set. . # get target variable proportion train_prop = mnist_data.y[train_mnist.indices].bincount() val_prop = mnist_data.y[val_mnist.indices].bincount() test_prop = mnist_data.y[test_mnist.indices].bincount() # visualization fig, axes = plt.subplots(1, 3, figsize=(12, 3)) for ax, prop, title, col in zip(axes, [train_prop, val_prop, test_prop], [&#39;Train&#39;, &#39;Validation&#39;, &#39;Test&#39;], &#39;bgr&#39;): class_target = range(0, 10) ax.bar(class_target, prop, color=col) ax.set_title(title) ax.set_xticks(class_target) ax.set_xlabel(&quot;Label&quot;) axes[0].set_ylabel(&quot;Frequency&quot;) plt.show() . . The distribution of our class labels do seem to be spread out quite evenly, so there&#39;s no problem. Next, we visualize a couple of images from train_loader: . class_label = [&#39;zero&#39;, &#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;, &#39;seven&#39;, &#39;eight&#39;, &#39;nine&#39;] images, labels = next(iter(train_loader)) images = images.reshape((train_loader.batch_size, 28, 28)) fig, axes = plt.subplots(2, 8, figsize=(12, 4)) for ax, img, label in zip(axes.flat, images, labels): ax.imshow(img, cmap=&quot;gray&quot;) ax.axis(&quot;off&quot;) ax.set_title(class_label[label]) plt.tight_layout() . . Define Model Architecture . We&#39;ll create a neural network with three layers: two hidden and one output layer. Additionally, we&#39;ll use ReLu activation function between each layer. Let&#39;s create a nn.Sequential object, which consists of linear fully-connected layer: . The input size is 784 nodes as the MNIST dataset consists of 28 by 28 pixels | The hidden sizes are 128 and 64 respectively, this number can be increased or decreased to change the learning capacity of the model | The output size is 10 as the MNIST dataset has 10 target classes (0 to 9) | . input_size = 784 hidden_sizes = [128, 64] output_size = 10 model = nn.Sequential( nn.Linear(input_size, hidden_sizes[0]), nn.ReLU(), nn.Linear(hidden_sizes[0], hidden_sizes[1]), nn.ReLU(), nn.Linear(hidden_sizes[1], output_size) # logit score ) model . Sequential( (0): Linear(in_features=784, out_features=128, bias=True) (1): ReLU() (2): Linear(in_features=128, out_features=64, bias=True) (3): ReLU() (4): Linear(in_features=64, out_features=10, bias=True) ) . A little bit about the mathematical detail: The image vector of size 784 are transformed into intermediate output vector of length 128 then 64 by performing a matrix multiplication of inputs matrix. Thus, input and layer1 outputs have linear relationship, i.e., each element of layer outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs. . Activation function such as Rectified Linear Unit (ReLU) is used to introduce non-linearity to the model. It has the formula relu(x) = max(0,x) i.e. it simply replaces negative values in a given tensor with the value 0. We refer to ReLU as the activation function, because for each input certain outputs are activated (those with non-zero values) while others turned off (those with zero values). ReLU can be seen visually as follows: . . The output layer returns a batch of vectors of size 10. This predicted output is then being compared with actual label, quantified by using nn.CrossEntropyLoss(). It combines LogSoftmax and NLLLoss (Negative Log Likelihood Loss) in one single class. . The loss value is used to update the parameter weights in the model. The parameter update algorithm can be implemented via an optimizer. In this case, we are using torch.optim.Adam() with learning rate (lr=0.001). . criterion = nn.CrossEntropyLoss() # specify optimizer (Adam) and learning rate = 0.001 optimizer = torch.optim.Adam(model.parameters(), lr=0.001) . Train the Model . Using torch, we have to manually loop the data per epoch to train the model. Here are the training loop for one epoch: . Clear gradients of all optimized variables from optimizer | Forward pass: compute predicted output by passing input data to the model | Calculate the loss based on specified criterion | Backward pass: compute gradient of the loss with respect to model parameters | Perform parameter update using optimizer algorithm | Accumulate training loss and accuracy | Inside the training loop, we also validate the model by performing the following steps: . Forward pass: compute predicted output by passing input data to the model | Calculate the loss based on specified criterion | Accumulate validation loss and accuracy | Special notes: . If you use Dropout or BatchNorm layer, don&#39;t forget to use model.train() when training the model and model.eval() when validating/testing the model so that the layer behaves accordingly. | Don&#39;t forget to set torch.no_grad() before validating the model to disable the gradient calculation since we are not updating the parameters. | . But first let&#39;s us define evaluate_accuracy to calculate accuracy given the predicted logits and y_true actual label. . def evaluate_accuracy(logits, y_true): # get index with the largest logit value PER OBSERVATION _, y_pred = torch.max(logits, dim=1) # calculate proportion of correct prediction correct_pred = (y_pred == y_true).float() acc = correct_pred.sum() / len(correct_pred) return acc * 100 . Define training loop with the following parameters: . model: untrained model | train_loader: data train of a DataLoader object | val_loader: data validation of a DataLoader object | criterion: loss function to be optimized | optimizer: optimization algorithm to used on the model parameters | n_epochs: number of training epochs | model_file_name: file name of serialized model. During the training loop, model with the lowest validation loss will be saved as a serialized model with .pt extension. | . def train(model, train_loader, val_loader, criterion, optimizer, n_epochs, model_file_name=&#39;model.pt&#39;): # initialize container variable for model performance results per epoch history = { &#39;n_epochs&#39;: n_epochs, &#39;loss&#39;: { &#39;train&#39;: [], &#39;val&#39;: [] }, &#39;acc&#39;: { &#39;train&#39;: [], &#39;val&#39;: [] } } # initialize tracker for minimum validation loss val_loss_min = np.Inf # loop per epoch for epoch in range(n_epochs): # initialize tracker for training performance train_acc = 0 train_loss = 0 ################### # train the model # ################### # prepare model for training model.train() # loop for each batch for data, target in train_loader: # STEP 1: clear gradients optimizer.zero_grad() # STEP 2: forward pass output = model(data) # STEP 3: calculate the loss loss = criterion(output, target) # STEP 4: backward pass loss.backward() # STEP 5: perform parameter update optimizer.step() # STEP 6: accumulate training loss and accuracy train_loss += loss.item() * data.size(0) acc = evaluate_accuracy(output, target) train_acc += acc.item() * data.size(0) ###################### # validate the model # ###################### # disable gradient calculation with torch.no_grad(): # initialize tracker for validation performance val_acc = 0 val_loss = 0 # prepare model for evaluation model.eval() # loop for each batch for data, target in val_loader: # STEP 1: forward pass output = model(data) # STEP 2: calculate the loss loss = criterion(output, target) # STEP 3: accumulate validation loss and accuracy val_loss += loss.item() * data.size(0) acc = evaluate_accuracy(output, target) val_acc += acc.item() * data.size(0) #################### # model evaluation # #################### # calculate average loss over an epoch train_loss /= len(train_loader.sampler) val_loss /= len(val_loader.sampler) history[&#39;loss&#39;][&#39;train&#39;].append(train_loss) history[&#39;loss&#39;][&#39;val&#39;].append(val_loss) # calculate average accuracy over an epoch train_acc /= len(train_loader.sampler) val_acc /= len(val_loader.sampler) history[&#39;acc&#39;][&#39;train&#39;].append(train_acc) history[&#39;acc&#39;][&#39;val&#39;].append(val_acc) # print training progress per epoch print(f&#39;Epoch {epoch+1:03} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | Train Acc: {train_acc:.2f} | Val Acc: {val_acc:.2f}&#39;) # save model if validation loss has decreased if val_loss &lt;= val_loss_min: print( f&#39;Validation loss decreased ({val_loss_min:.5f} --&gt; {val_loss:.5f}) Saving model to {model_file_name}...&#39;) torch.save(model.state_dict(), model_file_name) val_loss_min = val_loss print() # return model performance history return history . history = train( model, train_loader, val_loader, criterion, optimizer, n_epochs=20, model_file_name=&#39;cache/model-mnist-digit.pt&#39; ) . Epoch 001 | Train Loss: 0.35883 | Val Loss: 0.20376 | Train Acc: 89.62 | Val Acc: 93.79 Validation loss decreased (inf --&gt; 0.20376) Saving model to model.pt... Epoch 002 | Train Loss: 0.15089 | Val Loss: 0.16123 | Train Acc: 95.35 | Val Acc: 95.07 Validation loss decreased (0.20376 --&gt; 0.16123) Saving model to model.pt... Epoch 003 | Train Loss: 0.10185 | Val Loss: 0.12946 | Train Acc: 96.93 | Val Acc: 95.89 Validation loss decreased (0.16123 --&gt; 0.12946) Saving model to model.pt... Epoch 004 | Train Loss: 0.07460 | Val Loss: 0.14482 | Train Acc: 97.63 | Val Acc: 95.64 Epoch 005 | Train Loss: 0.05526 | Val Loss: 0.12973 | Train Acc: 98.21 | Val Acc: 96.35 Epoch 006 | Train Loss: 0.04527 | Val Loss: 0.12478 | Train Acc: 98.54 | Val Acc: 96.56 Validation loss decreased (0.12946 --&gt; 0.12478) Saving model to model.pt... Epoch 007 | Train Loss: 0.03887 | Val Loss: 0.11851 | Train Acc: 98.80 | Val Acc: 97.02 Validation loss decreased (0.12478 --&gt; 0.11851) Saving model to model.pt... Epoch 008 | Train Loss: 0.03050 | Val Loss: 0.14870 | Train Acc: 98.92 | Val Acc: 96.53 Epoch 009 | Train Loss: 0.02214 | Val Loss: 0.14465 | Train Acc: 99.26 | Val Acc: 96.58 Epoch 010 | Train Loss: 0.02284 | Val Loss: 0.15870 | Train Acc: 99.21 | Val Acc: 96.50 Epoch 011 | Train Loss: 0.02006 | Val Loss: 0.15534 | Train Acc: 99.33 | Val Acc: 96.61 Epoch 012 | Train Loss: 0.01239 | Val Loss: 0.17391 | Train Acc: 99.59 | Val Acc: 96.56 Epoch 013 | Train Loss: 0.01864 | Val Loss: 0.16095 | Train Acc: 99.35 | Val Acc: 96.89 Epoch 014 | Train Loss: 0.01618 | Val Loss: 0.15708 | Train Acc: 99.48 | Val Acc: 96.92 Epoch 015 | Train Loss: 0.01298 | Val Loss: 0.17791 | Train Acc: 99.56 | Val Acc: 96.82 Epoch 016 | Train Loss: 0.01338 | Val Loss: 0.18565 | Train Acc: 99.55 | Val Acc: 96.77 Epoch 017 | Train Loss: 0.01254 | Val Loss: 0.19535 | Train Acc: 99.55 | Val Acc: 96.76 Epoch 018 | Train Loss: 0.01223 | Val Loss: 0.19136 | Train Acc: 99.61 | Val Acc: 96.47 Epoch 019 | Train Loss: 0.01066 | Val Loss: 0.19524 | Train Acc: 99.61 | Val Acc: 96.53 Epoch 020 | Train Loss: 0.01137 | Val Loss: 0.18050 | Train Acc: 99.64 | Val Acc: 96.79 . with open(&#39;cache/history-mnist-digit.pickle&#39;, &#39;wb&#39;) as f: pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL) . Visualize the loss and accuracy from history to see the model performance for each epoch. . # load previously saved history dictionary with open(&#39;cache/history-mnist-digit.pickle&#39;, &#39;rb&#39;) as f: history = pickle.load(f) # visualization epoch_list = range(1, history[&#39;n_epochs&#39;]+1) fig, axes = plt.subplots(1, 2, figsize=(18, 6)) for ax, metric in zip(axes, [&#39;loss&#39;, &#39;acc&#39;]): ax.plot(epoch_list, history[metric][&#39;train&#39;], label=f&quot;train_{metric}&quot;) ax.plot(epoch_list, history[metric][&#39;val&#39;], label=f&quot;val_{metric}&quot;) ax.set_xlabel(&#39;epoch&#39;) ax.set_ylabel(metric) ax.legend() . . From the visualization, we can deduct that the model is a good enough since the difference of model performance on training and validation is not too much different, and the accuracy is converging at 96-97%. . Test the Model . In this section, we are going to test the model performance by using confusion matrix. Here are the steps: . Forward pass: compute predicted output by passing input data to the trained model | Get predicted label by retrieve index with the largest logit value per observation | Append actual and predicted label to y_test and y_pred respectively | model.load_state_dict(torch.load(&#39;cache/model-mnist-digit.pt&#39;)) . &lt;All keys matched successfully&gt; . y_test = [] y_pred = [] # disable gradient calculation with torch.no_grad(): # prepare model for evaluation model.eval() # loop for each data for data, target in test_loader: # STEP 1: forward pass output = model(data) # STEP 2: get predicted label _, label = torch.max(output, dim=1) # STEP 3: append actual and predicted label y_test += target.numpy().tolist() y_pred += label.numpy().tolist() . Create a confusion matrix heatmap using seaborn and classification report by using sklearn to know the final model performance on test data. . plt.subplots(figsize=(10, 8)) ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=&quot;d&quot;) ax.set_xlabel(&quot;Predicted Label&quot;) ax.set_ylabel(&quot;Actual Label&quot;) plt.show() . . print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.99 0.99 0.99 811 1 0.98 0.97 0.98 938 2 0.96 0.98 0.97 870 3 0.98 0.92 0.95 864 4 0.98 0.97 0.97 857 5 0.92 0.98 0.95 681 6 0.99 0.98 0.98 782 7 0.97 0.95 0.96 883 8 0.95 0.96 0.95 860 9 0.92 0.96 0.94 854 accuracy 0.97 8400 macro avg 0.97 0.97 0.97 8400 weighted avg 0.97 0.97 0.97 8400 . Eventually, the model achieves 97% accuracy and other metrics reach &gt;= 92% on unseen data. . Predict the Unlabeled Data . In this last section, we use the trained model to predict the unlabeled data from test.csv, which only consists 784 columns of pixel without the actual label. . mnist_data_unlabeled = MNISTDataset(&quot;data_input/mnist/test.csv&quot;) unlabeled_loader = DataLoader(mnist_data_unlabeled, batch_size=1, shuffle=True) print(mnist_data_unlabeled.X.size()) . torch.Size([28000, 784]) . Each image is feeded into the model and F.softmax activation function will be applied on the output value to convert logit scores to probability. The visualization tells us the predicted probability given an image, and then class with the highest probability will be the final predicted label of that image. . for idx in range(3): # get image from loader image = next(iter(unlabeled_loader)) image = image.reshape((28, 28)) ############## # prediction # ############## with torch.no_grad(): # forward pass output = model(image.reshape(1, -1)) # calculate probability prob = F.softmax(output, dim=1) ################# # visualization # ################# fig, axes = plt.subplots(1, 2, figsize=(8, 4)) # show digit image axes[0].imshow(image, cmap=&quot;gray&quot;) axes[0].axis(&quot;off&quot;) # predicted probability barplot axes[1].barh(range(0, 10), prob.reshape(-1)) axes[1].invert_yaxis() axes[1].set_yticks(range(0, 10)) axes[1].set_xlabel(&quot;Predicted Probability&quot;) plt.tight_layout() plt.show() . References . PyTorch Installation | MNIST Digit Recognizer Dataset | Writing Custom Datasets, DataLoaders, and Transforms | . Further Readings . There&#39;s a lot of scope to experiment here, and I encourage you to use the interactive nature of Jupyter to play around with the various parameters. Here are a few ideas: . Try changing the size of the hidden layer, or add more hidden layers and see if you can achieve a higher accuracy. . | Try changing the batch size and learning rate to see if you can achieve the same accuracy in fewer epochs. . | Compare the training times on a CPU vs. GPU. Do you see a significant difference. How does it vary with the size of the dataset and the size of the model (no. of weights and parameters)? . | Try building a model for a different dataset, such as the CIFAR10 or CIFAR100 datasets. . | . Here are some references for further reading: . A visual proof that neural networks can compute any function, also known as the Universal Approximation Theorem. . | But what is a neural network? - A visual and intuitive introduction to what neural networks are and what the intermediate layers represent . | Stanford CS229 Lecture notes on Backpropagation - for a more mathematical treatment of how gradients are calculated and weights are updated for neural networks with multiple layers. . | .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/computer-vision/pytorch/2021/08/27/mnist-digit-pytorch.html",
            "relUrl": "/python/classification/computer-vision/pytorch/2021/08/27/mnist-digit-pytorch.html",
            "date": " • Aug 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Convolutional Neural Network (CNN) for Gender Determination by Morphometry of Eyes",
            "content": "Introduction . The anthropometric analysis of the human face is an important study for performing craniofacial plastic and reconstructive surgeries. Facial anthropometric is affected by various factors such as age, gender, ethnicity, socioeconomic status, environment, and region. Plastic surgeons who repair and reconstruct facial deformities find the anatomical dimensions of the facial structures useful for their surgeries. These dimensions result from the physical or facial appearance of an individual. Factors like culture, personality, ethnic background, age, eye appearance, and symmetry contribute majorly to facial appearance or aesthetics. . . Note: The following post is an extension from my notebook submitted to DPhi Challenges . . Download Image Data . We can use GoogleDriveDownloader from google_drive_downloader library in Python to download the shared files from Google Drive. The file_id of the Google Drive link is 1f7uslI-ZHidriQFZR966_aILjlkgDN76 . from google_drive_downloader import GoogleDriveDownloader as gdd gdd.download_file_from_google_drive( file_id=&#39;1f7uslI-ZHidriQFZR966_aILjlkgDN76&#39;, dest_path=&#39;content/eye_gender_data.zip&#39;, unzip=True) . Load Libraries . All Python capabilities are not loaded to our working environment by default. So, we import each and every library that we want to use. We chose alias names for our libraries for the sake of our convenience. . import pandas as pd # data visualization import matplotlib.pyplot as plt # linear algebra and multidimensional arrays import numpy as np # deep learning tool import tensorflow as tf from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout from keras.callbacks import ModelCheckpoint, EarlyStopping # operating system dependent functionality import os # image processing import cv2 import PIL . Exploratory Data Analysis . Let&#39;s load the list of the training data from Training_set.csv. Then extract the width and height of each image. . def get_image_dim(filename): image = PIL.Image.open(os.path.join(FOLDER_PATH, &quot;train&quot;, filename)) return image.size FOLDER_PATH = &quot;content/eye_gender_data&quot; train_df = pd.read_csv(os.path.join(FOLDER_PATH, &quot;Training_set.csv&quot;)) train_df[[&#39;width&#39;, &#39;height&#39;]] = train_df[&#39;filename&#39;].apply( lambda x: get_image_dim(x)).to_list() train_df[&#39;is_square&#39;] = train_df[&#39;width&#39;] == train_df[&#39;height&#39;] train_df.head() . . filename label width height is_square . 0 Image_1.jpg | male | 53 | 53 | True | . 1 Image_2.jpg | female | 58 | 58 | True | . 2 Image_3.jpg | female | 59 | 59 | True | . 3 Image_4.jpg | female | 57 | 57 | True | . 4 Image_5.jpg | male | 72 | 72 | True | . Explore the statistics of train_df: . train_df.describe(include=&#39;all&#39;) . filename label width height is_square . count 9220 | 9220 | 9220.000000 | 9220.000000 | 9220 | . unique 9220 | 2 | NaN | NaN | 1 | . top Image_2446.jpg | male | NaN | NaN | True | . freq 1 | 5058 | NaN | NaN | 9220 | . mean NaN | NaN | 56.547505 | 56.547505 | NaN | . std NaN | NaN | 7.682044 | 7.682044 | NaN | . min NaN | NaN | 41.000000 | 41.000000 | NaN | . 25% NaN | NaN | 52.000000 | 52.000000 | NaN | . 50% NaN | NaN | 56.000000 | 56.000000 | NaN | . 75% NaN | NaN | 61.000000 | 61.000000 | NaN | . max NaN | NaN | 113.000000 | 113.000000 | NaN | . Insight: . We have 9220 images for the training data | Count and unique of filename are the same, which means no duplicated data, great! | There are two classes of label: 5058 images are labeled as male and the rest are female | width and height (pixels) of training images are different, we need to resize it into one fixed shape | Unique of is_squares is 1, which means all training images are square in shape | . Visualization . Visualize the first 10 images of the training data: . fig, axes = plt.subplots(2, 5, figsize=(12, 5)) for ax, (idx, row) in zip(axes.flatten(), train_df.head(10).iterrows()): image = PIL.Image.open(os.path.join(FOLDER_PATH, &quot;train&quot;, row[&#39;filename&#39;])) ax.imshow(image) ax.set_title(row[&#39;label&#39;]) ax.axis(&#39;off&#39;) . . Training images have different colors, some are in grayscale but most commonly in RGB. Moreover, they have different skin colors, if we incorporate RGB in our model then we may create a bias when predicting a gender. Thus, we&#39;ll convert all images to grayscale before training the model. . Data Augmentation . We apply on-the-fly data augmentation, a technique to expand the training dataset size by creating a modified version of the original image which can improve model performance and the ability to generalize. . Applied transformation: rotation, shift, shear, zoom, flip | Rescale the pixel values to be in range 0 and 1 | Reserve 20% of the training data for validation, and the rest 80% for model fitting | . train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rotation_range=20, width_shift_range=0.01, height_shift_range=0.01, shear_range=0.01, zoom_range=0.01, horizontal_flip=True, rescale=1./255, validation_split=0.2, fill_mode=&#39;nearest&#39;) # no data augmentation for validation data val_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1./255, validation_split=0.2) . Data Generator . By using flow_from_dataframe, we don&#39;t have to worry about mismatching labels as this is an inbuilt tensorflow method. Also, the files are loaded as and when necessary which avoids cluttering the main memory. . IMAGE_SIZE = (100, 100) BATCH_SIZE = 128 SEED_NUMBER = 42 # for reproducible result gen_args = dict( dataframe=train_df, directory=os.path.join(FOLDER_PATH, &quot;train&quot;), x_col=&#39;filename&#39;, y_col=&#39;label&#39;, target_size=IMAGE_SIZE, class_mode=&quot;binary&quot;, color_mode=&quot;grayscale&quot;, # convert RGB to grayscale color channel batch_size=BATCH_SIZE, shuffle=True, seed=SEED_NUMBER) # flow from dataframe train_ds = train_datagen.flow_from_dataframe(subset=&#39;training&#39;, **gen_args) val_ds = val_datagen.flow_from_dataframe(subset=&#39;validation&#39;, **gen_args) . Found 7376 validated image filenames belonging to 2 classes. Found 1844 validated image filenames belonging to 2 classes. . We have 7376 images for model fitting and the rest 1844 images for validation. . Building Model &amp; Hyperparameter tuning . Now we are finally ready, and we can train the model. CNN is used as an automatic feature extractor from the images. It effectively uses the adjacent pixel to downsample the image and then use a prediction (fully-connected) layer to solve the classification problem. . Define Architecture . model = Sequential( [ # First convolutional layer Conv2D( filters=64, kernel_size=3, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;, input_shape=IMAGE_SIZE + (1, )), # First pooling layer MaxPooling2D( pool_size=2, strides=2), # Second convolutional layer Conv2D( filters=32, kernel_size=3, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;), # Second pooling layer MaxPooling2D( pool_size=2, strides=2), # Third convolutional layer Conv2D( filters=16, kernel_size=3, strides=1, padding=&quot;same&quot;, activation=&quot;relu&quot;), # Third pooling layer MaxPooling2D( pool_size=2, strides=2), # Flattening Flatten(), # Fully-connected layer Dense(512, activation=&quot;relu&quot;), Dropout(rate=0.2), Dense(128, activation=&quot;relu&quot;), Dropout(rate=0.2), Dense(32, activation=&quot;relu&quot;), Dropout(rate=0.2), Dense(1, activation=&quot;sigmoid&quot;) ] ) model.summary() . . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 100, 100, 64) 640 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 50, 50, 64) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 50, 50, 32) 18464 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 25, 25, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 25, 25, 16) 4624 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 12, 12, 16) 0 _________________________________________________________________ flatten (Flatten) (None, 2304) 0 _________________________________________________________________ dense (Dense) (None, 512) 1180160 _________________________________________________________________ dropout (Dropout) (None, 512) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 65664 _________________________________________________________________ dropout_1 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 32) 4128 _________________________________________________________________ dropout_2 (Dropout) (None, 32) 0 _________________________________________________________________ dense_3 (Dense) (None, 1) 33 ================================================================= Total params: 1,273,713 Trainable params: 1,273,713 Non-trainable params: 0 _________________________________________________________________ . Compile Model . Next, we specify how the model backpropagates or update the weights after each batch feed-forward. We use adam optimizer and a loss function binary cross-entropy since we are dealing with binary classification problem. The metrics used to monitor the training progress is accuracy. . model.compile( optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . Model Fitting . Fit the train generator to the model by using two additional callbacks: . ModelCheckpoint to automatically save trained model when val_accuracy is improved for each epoch | EarlyStopping to stop the training process if after 5 consecutive epochs the val_accuracy is not improved | STEPS = 500 checkpoint = ModelCheckpoint(&quot;model.hdf5&quot;, verbose=1, save_best_only=True, monitor=&quot;val_accuracy&quot;) es_callback = EarlyStopping(monitor=&#39;val_accuracy&#39;, patience=5, restore_best_weights=True, verbose=1) model.fit( x=train_ds, validation_data=val_ds, steps_per_epoch=STEPS, validation_steps=STEPS, callbacks=[checkpoint, es_callback], epochs=100) . Epoch 1/100 500/500 [==============================] - 124s 217ms/step - loss: 0.5003 - accuracy: 0.7358 - val_loss: 0.2545 - val_accuracy: 0.8953 Epoch 00001: val_accuracy improved from -inf to 0.89534, saving model to model.hdf5 Epoch 2/100 500/500 [==============================] - 107s 215ms/step - loss: 0.2387 - accuracy: 0.9038 - val_loss: 0.2005 - val_accuracy: 0.9197 Epoch 00002: val_accuracy improved from 0.89534 to 0.91973, saving model to model.hdf5 Epoch 3/100 500/500 [==============================] - 108s 215ms/step - loss: 0.1769 - accuracy: 0.9309 - val_loss: 0.1773 - val_accuracy: 0.9343 Epoch 00003: val_accuracy improved from 0.91973 to 0.93435, saving model to model.hdf5 Epoch 4/100 500/500 [==============================] - 108s 215ms/step - loss: 0.1441 - accuracy: 0.9431 - val_loss: 0.1994 - val_accuracy: 0.9226 Epoch 00004: val_accuracy did not improve from 0.93435 Epoch 5/100 500/500 [==============================] - 108s 217ms/step - loss: 0.1146 - accuracy: 0.9551 - val_loss: 0.1783 - val_accuracy: 0.9349 Epoch 00005: val_accuracy improved from 0.93435 to 0.93489, saving model to model.hdf5 Epoch 6/100 500/500 [==============================] - 107s 214ms/step - loss: 0.0971 - accuracy: 0.9635 - val_loss: 0.1860 - val_accuracy: 0.9327 Epoch 00006: val_accuracy did not improve from 0.93489 Epoch 7/100 500/500 [==============================] - 107s 214ms/step - loss: 0.0803 - accuracy: 0.9705 - val_loss: 0.1931 - val_accuracy: 0.9361 Epoch 00007: val_accuracy improved from 0.93489 to 0.93614, saving model to model.hdf5 Epoch 8/100 500/500 [==============================] - 107s 213ms/step - loss: 0.0663 - accuracy: 0.9753 - val_loss: 0.2160 - val_accuracy: 0.9344 Epoch 00008: val_accuracy did not improve from 0.93614 Epoch 9/100 500/500 [==============================] - 105s 211ms/step - loss: 0.0590 - accuracy: 0.9783 - val_loss: 0.2624 - val_accuracy: 0.9323 Epoch 00009: val_accuracy did not improve from 0.93614 Epoch 10/100 500/500 [==============================] - 106s 212ms/step - loss: 0.0512 - accuracy: 0.9812 - val_loss: 0.2730 - val_accuracy: 0.9316 Epoch 00010: val_accuracy did not improve from 0.93614 Epoch 11/100 500/500 [==============================] - 106s 213ms/step - loss: 0.0530 - accuracy: 0.9798 - val_loss: 0.2203 - val_accuracy: 0.9420 Epoch 00011: val_accuracy improved from 0.93614 to 0.94196, saving model to model.hdf5 Epoch 12/100 500/500 [==============================] - 106s 212ms/step - loss: 0.0416 - accuracy: 0.9849 - val_loss: 0.2495 - val_accuracy: 0.9343 Epoch 00012: val_accuracy did not improve from 0.94196 Epoch 13/100 500/500 [==============================] - 105s 210ms/step - loss: 0.0372 - accuracy: 0.9871 - val_loss: 0.2680 - val_accuracy: 0.9408 Epoch 00013: val_accuracy did not improve from 0.94196 Epoch 14/100 500/500 [==============================] - 107s 214ms/step - loss: 0.0344 - accuracy: 0.9874 - val_loss: 0.2975 - val_accuracy: 0.9323 Epoch 00014: val_accuracy did not improve from 0.94196 Epoch 15/100 500/500 [==============================] - 106s 212ms/step - loss: 0.0349 - accuracy: 0.9883 - val_loss: 0.2880 - val_accuracy: 0.9436 Epoch 00015: val_accuracy improved from 0.94196 to 0.94355, saving model to model.hdf5 Epoch 16/100 500/500 [==============================] - 106s 213ms/step - loss: 0.0316 - accuracy: 0.9885 - val_loss: 0.2981 - val_accuracy: 0.9362 Epoch 00016: val_accuracy did not improve from 0.94355 Epoch 17/100 500/500 [==============================] - 106s 213ms/step - loss: 0.0283 - accuracy: 0.9899 - val_loss: 0.3065 - val_accuracy: 0.9382 Epoch 00017: val_accuracy did not improve from 0.94355 Epoch 18/100 500/500 [==============================] - 105s 210ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.2887 - val_accuracy: 0.9370 Epoch 00018: val_accuracy did not improve from 0.94355 Epoch 19/100 500/500 [==============================] - 106s 213ms/step - loss: 0.0256 - accuracy: 0.9912 - val_loss: 0.2937 - val_accuracy: 0.9425 Epoch 00019: val_accuracy did not improve from 0.94355 Epoch 20/100 500/500 [==============================] - 106s 213ms/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 0.2445 - val_accuracy: 0.9355 Epoch 00020: val_accuracy did not improve from 0.94355 Restoring model weights from the end of the best epoch. Epoch 00020: early stopping . &lt;keras.callbacks.History at 0x7f4090722110&gt; . Model Evaluation . Evaluate the trained model using the compiled loss function and accuracy metrics. . fig, ax = plt.subplots(1, 2, figsize=(10, 4)) history_df = pd.DataFrame(model.history.history) history_df[[&#39;loss&#39;, &#39;val_loss&#39;]].plot(kind=&#39;line&#39;, ax=ax[0]) history_df[[&#39;accuracy&#39;, &#39;val_accuracy&#39;]].plot(kind=&#39;line&#39;, ax=ax[1]) . . Predict the Testing Set . We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. Test.csv). . Load the Data . Load the test data on which final submission is to be made. . test_df = pd.read_csv(os.path.join(FOLDER_PATH, &quot;Testing_set.csv&quot;)) test_df.head() . filename . 0 Image_1.jpg | . 1 Image_2.jpg | . 2 Image_3.jpg | . 3 Image_4.jpg | . 4 Image_5.jpg | . Data Preprocessing on Test Set . We have to rescale the test set by dividing the pixels by 255. But we must not shuffle and apply any transformation on the test set. . test_datagen = tf.keras.preprocessing.image.ImageDataGenerator( rescale=1./255) test_ds = test_datagen.flow_from_dataframe( dataframe=test_df, directory=os.path.join(FOLDER_PATH, &quot;test&quot;), x_col=&#39;filename&#39;, target_size=IMAGE_SIZE, class_mode=None, color_mode=&quot;grayscale&quot;, # convert RGB to grayscale color channel shuffle=False ) . Found 2305 validated image filenames. . Convert Probability to Label . The output of the model is probability, we use default threshold 0.5 to classify the label. If the probability is above 0.5, we will map the output to male. Otherwise, female. . test_ds.reset() y_pred = (model.predict(test_ds, steps=len(test_ds)) &gt; 0.5).astype(&quot;int32&quot;).flatten() y_pred . array([1, 1, 1, ..., 1, 1, 1], dtype=int32) . class_mapping = {v:k for k, v in train_ds.class_indices.items()} class_mapping . {0: &#39;female&#39;, 1: &#39;male&#39;} . predictions = np.vectorize(class_mapping.get)(y_pred) predictions . array([&#39;male&#39;, &#39;male&#39;, &#39;male&#39;, ..., &#39;male&#39;, &#39;male&#39;, &#39;male&#39;], dtype=&#39;&lt;U6&#39;) . Save the Predicted Output . The predicted label along with the filename is saved as a csv file. A file named submission.csv will be created in the current working directory. . res = pd.DataFrame({&#39;filename&#39;: test_ds.filenames, &#39;label&#39;: predictions}) res.to_csv(&quot;submission.csv&quot;, index=False) . Run the cell code below to download the submission.csv from Google Colab to local computer. . from google.colab import files files.download(&#39;submission.csv&#39;) . Done! &#128077; . We are all set to make a submission. Let&#39;s head to the challenge page to make the submission. . . . Note: Placed 17th out of 118 submission, with model accuracy of 95.3145% .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/computer-vision/keras/2021/08/05/gender-determination-by-morphometry-of-eyes-cnn.html",
            "relUrl": "/python/classification/computer-vision/keras/2021/08/05/gender-determination-by-morphometry-of-eyes-cnn.html",
            "date": " • Aug 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Interpretable Loan Approval Pipeline Model",
            "content": ". Note: This post is an extension of Loan Approval Application that I made using Flask web framework. . import pandas as pd import numpy as np # pipeline from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler # model from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report # diplays HTML representation in a jupyter context from sklearn import set_config set_config(display=&#39;diagram&#39;) # default: display=&#39;text&#39; . The dataset is downloaded from GitHub Repository which contains information about loan applicants. There are 12 independent columns and 1 dependent column as follow: . Loan_ID: A unique ID assigned to every loan applicant | Gender: Gender of the applicant (Male, Female) | Married: The marital status of the applicant (Yes, No) | Dependents: No. of people dependent on the applicant (0,1,2,3+) | Education: Education level of the applicant (Graduated, Not Graduated) | Self_Employed: If the applicant is self-employed or not (Yes, No) | ApplicantIncome: The amount of income the applicant earns | CoapplicantIncome: The amount of income the co-applicant earns | LoanAmount: The amount of loan the applicant has requested for | Loan_Amount_Term: The no. of days over which the loan will be paid | Credit_History: A record of a borrower&#39;s responsible repayment of debts (1- has all debts paid, 0- not paid) | Property_Area : The type of location where the applicant’s property lies (Rural, Semiurban, Urban) | Loan_Status: Loan granted or not (1 for granted, 0 for not granted) | . Data Preprocessing before Pipeline . There are several steps to be executed before we use sklearn pipeline: . Load the data: Read loan.csv file and set Loan_ID as index | Manual feature selection: Drop irrelevant or select relevant columns only | Data type conversion: This is done to make it easier to select columns in the Pipeline | Feature-target split: Separate the independent and dependent variable into two objects | Train-test split: Training set will be used to train model, while testing set will be treated as unseen data for model evaluation | loan = pd.read_csv(&quot;data_input/loan.csv&quot;, index_col=&#39;Loan_ID&#39;) # 2. Drop irrelevant column loan.drop(columns=[&#39;Unnamed: 0&#39;], inplace=True) # 3. Data type conversion loan[&#39;Credit_History&#39;] = loan[&#39;Credit_History&#39;].astype(&#39;bool&#39;).astype(&#39;object&#39;) # 4. Feature-target split X = loan.drop(&#39;Loan_Status&#39;, axis=1) y = loan.Loan_Status # 5. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123) print(f&quot;X_train shape: {X_train.shape}&quot;) print(f&quot;X_test shape: {X_test.shape}&quot;) print(f&quot;y_train shape: {y_train.shape}&quot;) print(f&quot;y_test shape: {y_test.shape}&quot;) . X_train shape: (368, 11) X_test shape: (123, 11) y_train shape: (368,) y_test shape: (123,) . Check which columns contain missing values: . na_values = X_train.isna().mean() * 100 na_values.sort_values(ascending=False).round(2).to_frame(&#39;Percentage of NA&#39;).astype(str) + &#39; %&#39; . . Percentage of NA . Self_Employed 5.98 % | . LoanAmount 2.99 % | . Loan_Amount_Term 2.72 % | . Gender 1.9 % | . Dependents 1.36 % | . Property_Area 0.0 % | . Credit_History 0.0 % | . CoapplicantIncome 0.0 % | . ApplicantIncome 0.0 % | . Education 0.0 % | . Married 0.0 % | . . Note: The missing values will be handle during pipeline implementation. The numerical columns will be imputed using median (LoanAmount, Loan_Amount_Term), where as the categorical columns will be imputed by a constant &quot;Undefined&quot; value (Credit_History, Self_Employed, Gender, Dependents). . Pipeline . Why is pipeline preferred? A pipeline is used to chain several estimators into one sequential process. The advantages are: . Shorten the workflow: we only need to call .fit() and .predict() at the end of the pipeline implementation. | Parameter selection: all parameters (including preprocessing parameters) in the pipeline can be combined to perform a hyperparameter search (e.g., If you want to compare the performance of the model between mean or median imputation strategies) | Ensure that data leakage does not occur, i.e., statistical leakage from test data to the model during cross-validation. The most common one is when we do imputation, supposed we use mean strategy. We should only calculate the mean only based on the training set, not the whole data. | . Note: all estimators in the pipeline must have .transform() method, except for the last estimator. . Build Pipeline . First, we define the transformation pipeline for numerical and categorical columns separately: . The numerical columns will be imputed using median strategy and then scaled using z-score | The categorical columns will be imputed using constant &quot;Undefined&quot; value and then one-hot encoding will be applied. | . num_features = [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] cat_features = [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] num_transformer = Pipeline( [ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler()) # z-score ] ) cat_transformer = Pipeline( [ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;Undefined&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ] ) data_transformer = ColumnTransformer( [ (&#39;num&#39;, num_transformer, num_features), (&#39;cat&#39;, cat_transformer, cat_features) ] ) data_transformer . &lt;div id=&quot;sk-45a63a3a-5b09-47ec-8269-45be6d8fd550&quot; class&quot;sk-top-container&quot;&gt;ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])]) . num[&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] . SimpleImputerSimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;) . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;) . Next, the transformation pipeline is appended to a classifier. In this case, we use a simple LogisticRegression model. . pipe = Pipeline( [ (&#39;preprocess&#39;, data_transformer), (&#39;classifier&#39;, LogisticRegression(max_iter=np.Inf, random_state=123)) ] ) pipe . &lt;div id=&quot;sk-bff6454a-c93a-4023-8da5-619f47ea2b6c&quot; class&quot;sk-top-container&quot;&gt;PipelinePipeline(steps=[(&#39;preprocess&#39;, ColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])])), (&#39;classifier&#39;, LogisticRegression(max_iter=inf, random_state=123))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])]) . num[&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] . SimpleImputerSimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;) . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;) . LogisticRegressionLogisticRegression(max_iter=inf, random_state=123) . &lt;/div&gt; . Use the Pipeline . By using .fit() method, the statistics of training data will be learned both by the transformation (for scaling and one-hot encoding) and classifier. . pipe.fit(X_train, y_train) . &lt;div id=&quot;sk-bc57611d-81a8-432d-8913-cb5e61b77a65&quot; class&quot;sk-top-container&quot;&gt;PipelinePipeline(steps=[(&#39;preprocess&#39;, ColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])])), (&#39;classifier&#39;, LogisticRegression(max_iter=inf, random_state=123))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])]) . num[&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] . SimpleImputerSimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;) . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;) . LogisticRegressionLogisticRegression(max_iter=inf, random_state=123) . &lt;/div&gt; . Then we use the trained pipeline to predict the target variable, the the trained model is evaluated using classification_report. . print(classification_report(y_train, pipe.predict(X_train))) print(classification_report(y_test, pipe.predict(X_test))) . precision recall f1-score support 0 0.88 0.43 0.58 113 1 0.79 0.97 0.87 255 accuracy 0.81 368 macro avg 0.83 0.70 0.73 368 weighted avg 0.82 0.81 0.78 368 precision recall f1-score support 0 0.90 0.51 0.65 35 1 0.83 0.98 0.90 88 accuracy 0.85 123 macro avg 0.87 0.75 0.78 123 weighted avg 0.85 0.85 0.83 123 . Up to this point, if we are satisfied with the model&#39;s performance, we can serialized the pipeline using joblib.dump(): . import joblib joblib.dump(pipe, &#39;cache/loan_pipeline.joblib&#39;) . [&#39;cache/loan_pipeline.joblib&#39;] . Reversely, we can de-serialized (read) the pickled pipeline using joblib.load(): . pipe = joblib.load(&#39;cache/loan_pipeline.joblib&#39;) pipe . &lt;div id=&quot;sk-aab38f93-2222-4105-ab20-c7dd4f258bc9&quot; class&quot;sk-top-container&quot;&gt;PipelinePipeline(steps=[(&#39;preprocess&#39;, ColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])])), (&#39;classifier&#39;, LogisticRegression(max_iter=inf, random_state=123))]) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])]) . num[&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] . SimpleImputerSimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;) . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;) . LogisticRegressionLogisticRegression(max_iter=inf, random_state=123) . &lt;/div&gt; . . Warning: joblib.load() relies on the pickle module and can execute arbitrary Python code. It should therefore never be used to load files from untrusted sources. . How to Access Fitted Pipeline? . Sometimes, we want a more in-depth inspection of what each component of the pipeline has learned from the training set. . Case 1: Print out the learned statistics of SimpleImputer . pd.DataFrame({ &#39;Numerical Features&#39;: num_features, &#39;Learned Statistics (Median)&#39;: pipe[&#39;preprocess&#39;].named_transformers_[&#39;num&#39;].named_steps[&#39;imputer&#39;].statistics_ }) . Numerical Features Learned Statistics (Median) . 0 ApplicantIncome | 3859.0 | . 1 CoapplicantIncome | 1293.5 | . 2 LoanAmount | 128.0 | . 3 Loan_Amount_Term | 360.0 | . . Note: This is the value that will be imputed to the unseen data. . Case 2: Transform new data using the fitted data_transformer . new_data = pd.DataFrame({ &#39;Gender&#39;: [&#39;Male&#39;], &#39;Married&#39;: [&#39;No&#39;], &#39;Dependents&#39;: [0], &#39;Education&#39;: [&#39;Graduate&#39;], &#39;Self_Employed&#39;: [&#39;No&#39;], &#39;ApplicantIncome&#39;: [10000], &#39;CoapplicantIncome&#39;: [np.nan], &#39;LoanAmount&#39;: [1000], &#39;Loan_Amount_Term&#39;: [36], &#39;Credit_History&#39;: [True], &#39;Property_Area&#39;: [np.nan] }) new_data . . Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Property_Area . 0 Male | No | 0 | Graduate | No | 10000 | NaN | 1000 | 36 | True | NaN | . First, we would like to extract the column name after OneHotEncoder is applied: . fitted_encoder = pipe[&#39;preprocess&#39;].named_transformers_[&#39;cat&#39;].named_steps[&#39;encoder&#39;] fitted_encoder.get_feature_names(cat_features) . array([&#39;Gender_Female&#39;, &#39;Gender_Male&#39;, &#39;Gender_Undefined&#39;, &#39;Married_No&#39;, &#39;Married_Yes&#39;, &#39;Dependents_0&#39;, &#39;Dependents_1&#39;, &#39;Dependents_2&#39;, &#39;Dependents_3+&#39;, &#39;Dependents_Undefined&#39;, &#39;Education_Graduate&#39;, &#39;Education_Not Graduate&#39;, &#39;Self_Employed_No&#39;, &#39;Self_Employed_Undefined&#39;, &#39;Self_Employed_Yes&#39;, &#39;Credit_History_False&#39;, &#39;Credit_History_True&#39;, &#39;Property_Area_Rural&#39;, &#39;Property_Area_Semiurban&#39;, &#39;Property_Area_Urban&#39;], dtype=object) . Second, we can immediately use .transform() method on the new data using trained transformation pipeline. . data_transformer.transform(new_data) . array([[ 0.67810314, -0.10995112, 10.06889387, -4.86515444, 0. , 1. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. , 0. ]]) . The two steps above can be combined into one cell code in the following dataframe: . pd.DataFrame( data_transformer.transform(new_data), columns=num_features + list(fitted_encoder.get_feature_names(cat_features)) ) . ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Gender_Female Gender_Male Gender_Undefined Married_No Married_Yes Dependents_0 ... Education_Graduate Education_Not Graduate Self_Employed_No Self_Employed_Undefined Self_Employed_Yes Credit_History_False Credit_History_True Property_Area_Rural Property_Area_Semiurban Property_Area_Urban . 0 0.678103 | -0.109951 | 10.068894 | -4.865154 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | . 1 rows × 24 columns . GridSearch over the Pipeline . One advantage of using pipeline is that we can perform grid search over all parameters inside the pipeline. Here is the list of parameters to be tuned based on previous pipeline. Parameter name that starts with classifier__ means we are tuning the model, where as preprocess__ means we are tuning the preprocessing (transformation) parameters. . sorted(list(pipe.get_params().keys())) . [&#39;classifier&#39;, &#39;classifier__C&#39;, &#39;classifier__class_weight&#39;, &#39;classifier__dual&#39;, &#39;classifier__fit_intercept&#39;, &#39;classifier__intercept_scaling&#39;, &#39;classifier__l1_ratio&#39;, &#39;classifier__max_iter&#39;, &#39;classifier__multi_class&#39;, &#39;classifier__n_jobs&#39;, &#39;classifier__penalty&#39;, &#39;classifier__random_state&#39;, &#39;classifier__solver&#39;, &#39;classifier__tol&#39;, &#39;classifier__verbose&#39;, &#39;classifier__warm_start&#39;, &#39;memory&#39;, &#39;preprocess&#39;, &#39;preprocess__cat&#39;, &#39;preprocess__cat__encoder&#39;, &#39;preprocess__cat__encoder__categories&#39;, &#39;preprocess__cat__encoder__drop&#39;, &#39;preprocess__cat__encoder__dtype&#39;, &#39;preprocess__cat__encoder__handle_unknown&#39;, &#39;preprocess__cat__encoder__sparse&#39;, &#39;preprocess__cat__imputer&#39;, &#39;preprocess__cat__imputer__add_indicator&#39;, &#39;preprocess__cat__imputer__copy&#39;, &#39;preprocess__cat__imputer__fill_value&#39;, &#39;preprocess__cat__imputer__missing_values&#39;, &#39;preprocess__cat__imputer__strategy&#39;, &#39;preprocess__cat__imputer__verbose&#39;, &#39;preprocess__cat__memory&#39;, &#39;preprocess__cat__steps&#39;, &#39;preprocess__cat__verbose&#39;, &#39;preprocess__n_jobs&#39;, &#39;preprocess__num&#39;, &#39;preprocess__num__imputer&#39;, &#39;preprocess__num__imputer__add_indicator&#39;, &#39;preprocess__num__imputer__copy&#39;, &#39;preprocess__num__imputer__fill_value&#39;, &#39;preprocess__num__imputer__missing_values&#39;, &#39;preprocess__num__imputer__strategy&#39;, &#39;preprocess__num__imputer__verbose&#39;, &#39;preprocess__num__memory&#39;, &#39;preprocess__num__scaler&#39;, &#39;preprocess__num__scaler__copy&#39;, &#39;preprocess__num__scaler__with_mean&#39;, &#39;preprocess__num__scaler__with_std&#39;, &#39;preprocess__num__steps&#39;, &#39;preprocess__num__verbose&#39;, &#39;preprocess__remainder&#39;, &#39;preprocess__sparse_threshold&#39;, &#39;preprocess__transformer_weights&#39;, &#39;preprocess__transformers&#39;, &#39;preprocess__verbose&#39;, &#39;steps&#39;, &#39;verbose&#39;] . In the cell below we prepare the parameter combinations as follows: . On preprocessing step: Try mean and median for the imputation stategy | Try not doing scaling (passthrough), standardization, and min-max scaling | . | On classifier step: Try to penalize model using C = 0.1, 1, and 10 | . | . param_grid = { # combination for numeric imputer &#39;preprocess__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], # combination for numeric scaling &#39;preprocess__num__scaler&#39;: [&#39;passthrough&#39;, StandardScaler(), MinMaxScaler()], # combination for logistic regression hyperparameter &#39;classifier__C&#39;: [0.1, 1, 10] } pipe_grid = GridSearchCV(pipe, param_grid, cv=3) pipe_grid.fit(X_train, y_train) . &lt;div id=&quot;sk-07f55928-8301-4537-bedc-c6f7f548effd&quot; class&quot;sk-top-container&quot;&gt;GridSearchCVGridSearchCV(cv=3, estimator=Pipeline(steps=[(&#39;preprocess&#39;, ColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant... OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])])), (&#39;classifier&#39;, LogisticRegression(max_iter=inf, random_state=123))]), param_grid={&#39;classifier__C&#39;: [0.1, 1, 10], &#39;preprocess__num__imputer__strategy&#39;: [&#39;mean&#39;, &#39;median&#39;], &#39;preprocess__num__scaler&#39;: [&#39;passthrough&#39;, StandardScaler(), MinMaxScaler()]}) . preprocess: ColumnTransformerColumnTransformer(transformers=[(&#39;num&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler())]), [&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;]), (&#39;cat&#39;, Pipeline(steps=[(&#39;imputer&#39;, SimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;)), (&#39;encoder&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;))]), [&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;])]) . num[&#39;ApplicantIncome&#39;, &#39;CoapplicantIncome&#39;, &#39;LoanAmount&#39;, &#39;Loan_Amount_Term&#39;] . SimpleImputerSimpleImputer(strategy=&#39;median&#39;) . StandardScalerStandardScaler() . cat[&#39;Gender&#39;, &#39;Married&#39;, &#39;Dependents&#39;, &#39;Education&#39;, &#39;Self_Employed&#39;, &#39;Credit_History&#39;, &#39;Property_Area&#39;] . SimpleImputerSimpleImputer(fill_value=&#39;Undefined&#39;, strategy=&#39;constant&#39;) . OneHotEncoderOneHotEncoder(handle_unknown=&#39;ignore&#39;) . LogisticRegressionLogisticRegression(max_iter=inf, random_state=123) . &lt;/div&gt; . Print out the grid search cross-validation results: . pd.DataFrame(pipe_grid.cv_results_).sort_values(&#39;rank_test_score&#39;)[[&#39;rank_test_score&#39;, &#39;params&#39;, &#39;mean_test_score&#39;, &#39;std_test_score&#39;]] . . rank_test_score params mean_test_score std_test_score . 2 1 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.806966 | 0.030669 | . 5 1 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.806966 | 0.030669 | . 8 3 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.801546 | 0.024111 | . 11 3 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.801546 | 0.024111 | . 1 5 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.798814 | 0.030696 | . 4 5 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.798814 | 0.030696 | . 0 7 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.793327 | 0.044085 | . 9 8 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.790684 | 0.021143 | . 10 8 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.790684 | 0.020074 | . 14 10 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.787974 | 0.020709 | . 15 10 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.787974 | 0.018458 | . 17 10 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.787974 | 0.020709 | . 7 13 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.787951 | 0.023938 | . 6 13 | {&#39;classifier__C&#39;: 1, &#39;preprocess__num__imputer... | 0.787951 | 0.024841 | . 12 15 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.785241 | 0.022270 | . 3 16 | {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imput... | 0.782487 | 0.034336 | . 16 17 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.777089 | 0.022301 | . 13 17 | {&#39;classifier__C&#39;: 10, &#39;preprocess__num__impute... | 0.777089 | 0.022301 | . . Note: We can conclude that scaling method doesn&#8217;t affect the performance of Logistic Regression. . We can print out the best combination of parameters based on the grid search: . pipe_grid.best_params_ . {&#39;classifier__C&#39;: 0.1, &#39;preprocess__num__imputer__strategy&#39;: &#39;mean&#39;, &#39;preprocess__num__scaler&#39;: MinMaxScaler()} . Lastly, we can directly apply .predict() method to the pipe_grid, automatically it will use the best_params_ combination: . print(classification_report(y_train, pipe_grid.predict(X_train))) print(classification_report(y_test, pipe_grid.predict(X_test))) . precision recall f1-score support 0 0.92 0.41 0.56 113 1 0.79 0.98 0.88 255 accuracy 0.81 368 macro avg 0.85 0.70 0.72 368 weighted avg 0.83 0.81 0.78 368 precision recall f1-score support 0 0.89 0.46 0.60 35 1 0.82 0.98 0.89 88 accuracy 0.83 123 macro avg 0.85 0.72 0.75 123 weighted avg 0.84 0.83 0.81 123 . References for sklearn pipeline . Pipeline component | Preprocessing data Scaling: Standarization, Normalization | Non-linear transformation | Encoding categorical | Binning (numerical to categorical) | Missing values imputation | Polynomial features | . | . . Note: The point in bold is the part that has been discussed in this post. . Explainable AI (XAI): Local Interpretable Model-Agnostic Explanations (LIME) . X_train_preprocessed is X_train data that has been preprocessed on the best Pipeline (previous GridSearchCV result) . X_train_preprocessed = pd.DataFrame( pipe_grid.best_estimator_[&#39;preprocess&#39;].transform(X_train), columns=num_features + list(fitted_encoder.get_feature_names(cat_features)), index=X_train.index ) X_train_preprocessed.head() . ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Gender_Female Gender_Male Gender_Undefined Married_No Married_Yes Dependents_0 ... Education_Graduate Education_Not Graduate Self_Employed_No Self_Employed_Undefined Self_Employed_Yes Credit_History_False Credit_History_True Property_Area_Rural Property_Area_Semiurban Property_Area_Urban . Loan_ID . LP001367 0.035894 | 0.030440 | 0.131122 | 0.74359 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . LP002137 0.076475 | 0.135443 | 0.382306 | 0.74359 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . LP002113 0.020779 | 0.000000 | 0.204194 | 0.74359 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | ... | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | . LP001664 0.049981 | 0.000000 | 0.162717 | 0.74359 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | . LP001421 0.067013 | 0.063303 | 0.249605 | 0.74359 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | . 5 rows × 24 columns . categorical_names_mapping is a dictionary used by OneHotEncoder when each column is being encoded. . fitted_encoder_best = pipe_grid.best_estimator_[&#39;preprocess&#39;].named_transformers_[&#39;cat&#39;].named_steps[&#39;encoder&#39;] categorical_names_mapping = dict(zip(cat_features, fitted_encoder.categories_)) categorical_names_mapping . {&#39;Gender&#39;: array([&#39;Female&#39;, &#39;Male&#39;, &#39;Undefined&#39;], dtype=object), &#39;Married&#39;: array([&#39;No&#39;, &#39;Yes&#39;], dtype=object), &#39;Dependents&#39;: array([&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3+&#39;, &#39;Undefined&#39;], dtype=object), &#39;Education&#39;: array([&#39;Graduate&#39;, &#39;Not Graduate&#39;], dtype=object), &#39;Self_Employed&#39;: array([&#39;No&#39;, &#39;Undefined&#39;, &#39;Yes&#39;], dtype=object), &#39;Credit_History&#39;: array([False, True], dtype=object), &#39;Property_Area&#39;: array([&#39;Rural&#39;, &#39;Semiurban&#39;, &#39;Urban&#39;], dtype=object)} . Setting up LIME Explainer for tabular data, the data used is the training data only. . . Note: Install with command pip install lime in your Python environment. . from lime.lime_tabular import LimeTabularExplainer explainer = LimeTabularExplainer( training_data=X_train_preprocessed.values, # must be a numeric array mode=&#39;classification&#39;, feature_names=X_train_preprocessed.columns, class_names=[&#39;Not Granted&#39;, &#39;Granted&#39;], categorical_names=categorical_names_mapping) . Next, provide the data that we want to interpret, namely data testing that has been preprocessed based on the best pipeline (the previous GridSearchCV result). . X_test_preprocessed = pd.DataFrame( pipe_grid.best_estimator_[&#39;preprocess&#39;].transform(X_test), columns=num_features + list(fitted_encoder.get_feature_names(cat_features)), index=X_test.index ) X_test_preprocessed.head() . ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Gender_Female Gender_Male Gender_Undefined Married_No Married_Yes Dependents_0 ... Education_Graduate Education_Not Graduate Self_Employed_No Self_Employed_Undefined Self_Employed_Yes Credit_History_False Credit_History_True Property_Area_Rural Property_Area_Semiurban Property_Area_Urban . Loan_ID . LP001279 0.027409 | 0.074800 | 0.187994 | 0.74359 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . LP001322 0.049264 | 0.000000 | 0.165877 | 0.74359 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . LP001086 0.015980 | 0.000000 | 0.028436 | 0.74359 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | ... | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | . LP002974 0.038120 | 0.057629 | 0.143760 | 0.74359 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | . LP001404 0.037316 | 0.067471 | 0.216430 | 0.74359 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | ... | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | . 5 rows × 24 columns . LIME only provides local interpretation (per each observation only). In the cell below, we want to see the interpretation results for the first observation (using .iloc[[0]]) on X_test_encoded. . predict_fn = lambda x: pipe_grid.best_estimator_[&#39;classifier&#39;].predict_proba(x).astype(float) exp = explainer.explain_instance( X_test_preprocessed.iloc[[1]].values[0], # must be a one-dimensional numpy array predict_fn, # prediction function that outputs probability num_features=5 # top n features to be explained ) print(f&quot;LIME Explanation for ID {X_test_preprocessed.iloc[[1]].index.values[0]}&quot;) exp.show_in_notebook(show_table=True, show_all=False) . LIME Explanation for ID LP001322 . . . Insight for the second observation (Loan ID LP001322): . Loan application is predicted to be granted with 80% probability | The reasons that support the loan should be granted are: The credit history is good (Credit_History_False = 0 means Credit_History = True (value 1)) | The property is in semiurban area, not in rural area (Property_Area_Semiurban = 1 dan Property_Area_Rural = 0) | Number of dependents is not 1 (Dependents_1 = 0) | . | The reason that contradict the loan should be granted is: The applicant is not married (Married_No = 1) | . | . There is a slightly truncated part of the bar plot visualization above, so we can see it more clearly in the form of a DataFrame: . pd.DataFrame(exp.as_list(), columns=[&#39;Condition&#39;, &#39;Influence Score&#39;]) . . Condition Influence Score . 0 Credit_History_False &lt;= 0.00 | 0.239573 | . 1 0.00 &lt; Property_Area_Semiurban &lt;= 1.00 | 0.064764 | . 2 Property_Area_Rural &lt;= 0.00 | 0.058795 | . 3 Dependents_1 &lt;= 0.00 | 0.050181 | . 4 Gender_Female &lt;= 0.00 | 0.042407 | . . Note: The influence score is not the coefficient of the logistic regression. It is used to be compared one condition to another. The larger the value, the more influential it is. Positive value ​​indicates the condition that support the positive class, while negative value indicates the condition that contradicts the positive class. . LIME creates a simple, interpretable model to study the relationship of features with predict_proba results. Therefore, there is a explanation fit value that describes how well LIME explains the black-box model (such as the R-squared value in linear model). The range of explanation fit is from 0 to 1 (the bigger the value, the better the fit). . print(f&quot;Explanation fit: {exp.__dict__[&#39;score&#39;]}&quot;) . Explanation fit: 0.7204616471931928 . . Note: The LimeTabularExplainer model can also be tuned like a classification model. Parameter reference . References for LIME . GitHub Repository LIME | LIME Tabular | E-book Interpretable ML | . &lt;/div&gt; .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/scikit-learn/explainable-ai/2021/04/15/loan-approval-pipeline-lime.html",
            "relUrl": "/python/classification/scikit-learn/explainable-ai/2021/04/15/loan-approval-pipeline-lime.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Sales Forecasting using Prophet",
            "content": "Introduction . Time series data is one of the most common form of data to be found in every industry. It is considered to be a significant area of interest for most industries: retail, telecommunication, logistic, engineering, finance, and socio-economic. Time series analysis aims to extract the underlying components of a time series to better understand the nature of the data. In this post, we will tackle a common industry case of business sales. . Library and Setup . We load some packages that will be used to analyze and forecast our time series data. . import numpy as np import pandas as pd # data visualization import matplotlib.pyplot as plt import seaborn as sns import plotly.io as pio # visualization settings plt.style.use(&#39;seaborn&#39;) plt.rcParams[&#39;figure.facecolor&#39;] = &#39;white&#39; pio.renderers.default = &#39;colab&#39; # prophet from fbprophet.diagnostics import cross_validation, performance_metrics from fbprophet.plot import plot_plotly, plot_components_plotly, plot_cross_validation_metric from fbprophet import Prophet, hdays # evaluation metric from sklearn.metrics import mean_squared_log_error # others import itertools from tqdm import tqdm . Workflow . Data Loading . We will be using provided data from one of the largest Russian software firms - 1C Company and is made available through Kaggle. This is a good example case of data since it contains seasonality and a particular noise in several data when special occurences happened. . sales = pd.read_csv(&#39;data_input/sales_train.csv&#39;) sales.head() . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . The following are the glossary provided in the Kaggle platform: . date is the date format provided in dd.mm.yyyy format | date_block_num is a consecutive month number used for convenience (January 2013 is 0, February 2013 is 1, and so on) | shop_id is the unique identifier of the shop | item_id is the unique identifier of the product | item_price is the price of the item on the specified date | item_cnt_day is the number of products sold on the specified date | . . Note: The variable of interest that we are trying to predict is the item_cnt_day and item_price, which we&#8217;ll see how to analyze the business demand from the sales record. . sales.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2935849 entries, 0 to 2935848 Data columns (total 6 columns): # Column Dtype -- 0 date object 1 date_block_num int64 2 shop_id int64 3 item_id int64 4 item_price float64 5 item_cnt_day float64 dtypes: float64(2), int64(3), object(1) memory usage: 134.4+ MB . Some insights we can get from the output are: . The data consist of 2,935,849 observations (or rows) | It has 6 variables (or columns) | . Data Preprocessing . Time series data is defined as data observations that are collected at regular time intervals. In this case, we are talking about software sales daily data. . We have to make sure our data is ready to be fitted into models, such as: . Convert date column data type from object to datetime64 | Sort the data ascending by date column | Feature engineering of total_revenue, which will be forecasted | . sales[&#39;date&#39;] = pd.to_datetime(sales[&#39;date&#39;], dayfirst=True) sales.sort_values(&#39;date&#39;, inplace=True) sales[&#39;total_revenue&#39;] = sales[&#39;item_price&#39;] * sales[&#39;item_cnt_day&#39;] sales.dtypes . date datetime64[ns] date_block_num int64 shop_id int64 item_id int64 item_price float64 item_cnt_day float64 total_revenue float64 dtype: object . . Important: The legal and cultural expectations for datetime format may vary between countries. In Indonesia for example, most people are used to storing dates in DMY order. pandas will infer date as a month first order by default. Since the sales date is stored in dd.mm.yyyy format, we have to specify parameter dayfirst=True inside pd.to_datetime() method. . Take a look on the third observation below; pandas converts it to 2nd January while the actual data represents February 1st. . date = pd.Series([&#39;30-01-2021&#39;, &#39;31-01-2021&#39;, &#39;01-02-2021&#39;, &#39;02-02-2021&#39;]) pd.to_datetime(date) # format of datetime64 is yyyy-mm-dd . 0 2021-01-30 1 2021-01-31 2 2021-01-02 3 2021-02-02 dtype: datetime64[ns] . Next, let&#39;s check the date range of sales data. Turns out it ranges from January 1st, 2013 to October 31st, 2015. . sales[&#39;date&#39;].apply([&#39;min&#39;, &#39;max&#39;]) . min 2013-01-01 max 2015-10-31 Name: date, dtype: datetime64[ns] . We are interested in analyzing the most popular shop (shop_id) of our sales data. The popularity of a shop is defined by the number of transaction that occur. Let&#39;s create a frequency table using .value_counts() as follows: . top_3_shop = sales[&#39;shop_id&#39;].value_counts().head(3) top_3_shop . 31 235636 25 186104 54 143480 Name: shop_id, dtype: int64 . We have gain the information that shop 31, 25, and 54 are the top three shops with the most record sales. Say we would like to analyze their time series attribute. To do that, we can apply conditional subsetting (filter) to sales data. . sales_top_3_shop = sales[sales[&#39;shop_id&#39;].isin(top_3_shop.index)] sales_top_3_shop.head() . date date_block_num shop_id item_id item_price item_cnt_day total_revenue . 84820 2013-01-01 | 0 | 54 | 12370 | 149.0 | 1.0 | 149.0 | . 85870 2013-01-01 | 0 | 54 | 13431 | 11489.7 | 1.0 | 11489.7 | . 87843 2013-01-01 | 0 | 54 | 8709 | 349.0 | 1.0 | 349.0 | . 76711 2013-01-01 | 0 | 54 | 20144 | 249.0 | 1.0 | 249.0 | . 87341 2013-01-01 | 0 | 54 | 10458 | 298.0 | 1.0 | 298.0 | . Now let’s highlight again the most important definition of a time series: . . Important: Time series is an observation that is recorded at a regular time interval. Notice that the records has a multiple samples of the same day. This must mean that our data frame violates the rules of a time series where the records is sampled multiple time a day. Based on the structure of our data, it is recording the sales of different items within the same day. An important aspect in preparing a time series is called a data aggregation, where we need to aggregate the sales from one day into one records. . Now let’s take a look at the codes: . daily_sales = sales_top_3_shop.groupby([&#39;date&#39;, &#39;shop_id&#39;])[[&#39;item_cnt_day&#39;, &#39;total_revenue&#39;]] .sum().reset_index() .rename(columns={&#39;item_cnt_day&#39;: &#39;total_qty&#39;}) daily_sales.head() . date shop_id total_qty total_revenue . 0 2013-01-01 | 54 | 415.0 | 316557.00 | . 1 2013-01-02 | 25 | 568.0 | 345174.13 | . 2 2013-01-02 | 31 | 568.0 | 396376.10 | . 3 2013-01-02 | 54 | 709.0 | 519336.00 | . 4 2013-01-03 | 25 | 375.0 | 249421.00 | . . Note: In performing data aggregation, we can only transform a more frequent data sampling to a more sparse frequency, for example hourly to daily, daily to weekly, daily to monthly, monthly to quarterly, and so on. . Visualization . One of an important aspect in time series analysis is performing a visual exploratory analysis. Python is known for its graphical capabilities and has a very popular visualization package called matplotlib and seaborn. Let’s take our daily_sales data frame we have created earlier and observe through the visualization. . . Important: There is a misconception between multiple and multivariate time series. In multiple time series, there is one variable from multiple objects being observed from time to time. In multivariate time series, there are multiple variables from only one object being observed from time to time. Typically for such series, the variables are closely interrelated. . Multiple Time Series . In our case, multiple time series is when we observed the fluctuation of total_qty over time, from the top three shops. . sns.relplot(data=daily_sales, kind=&#39;line&#39;, x=&#39;date&#39;, y=&#39;total_qty&#39;, col=&#39;shop_id&#39;, aspect=6/5) plt.show() . . From the visualization we can conclude that the fluctuation of total_qty is very distinct for each shop. There are some extreme spikes on shop_id 25 and 31 at the end of each year, while shop_id 54 doesn&#39;t have any spike. . Multivariate Time Series . In our case, multivariate time series is when we observed the fluctuation of total_qty and total_revenue over time, from only shop_id 31. . daily_sales_31 = daily_sales[daily_sales[&#39;shop_id&#39;] == 31].reset_index(drop=True) daily_sales_31.set_index(&#39;date&#39;)[[&#39;total_qty&#39;, &#39;total_revenue&#39;]].plot(subplots=True, figsize=(10, 5)) plt.suptitle(&#39;DAILY SOFTWARE SALES: STORE 31&#39;) plt.show() . . From the visualization we can conclude that the fluctuation of total_qty and total_revenue is quite similar for shop_id 31. . . Note: From the business perspective, variable quantity and revenue are closely related to each other. When the total_qty sold increases, logically, the total_revenue will also increases. We can use total_qty as the regressor when forecasting total_revenue. . Modeling using fbprophet . A very fundamental part in understanding time series is to be able to decompose its underlying components. A classic way in describing a time series is using General Additive Model (GAM). This definition describes time series as a summation of its components. As a starter, we will define time series with 3 different components: . Trend ($T$): Long term movement in its mean | Seasonality ($S$): Repeated seasonal effects | Residuals ($E$): Irregular components or random fluctuations not described by trend and seasonality | . The idea of GAM is that each of them is added to describe our time series: . $Y(t) = T(t) + S(t) + E(t)$ . . Important: When we are discussing time series forecasting there is one main assumption that needs to be remembered: We assume correlation among successive observations. Means that the idea of performing a forecasting for a time series is based on its past behavior. So in order to forecast the future values, we will take a look at any existing trend and seasonality of the time series and use it to generate future values. . Prophet enhanced the classical trend and seasonality components by adding a holiday effect. It will try to model the effects of holidays which occur on some dates and has been proven to be really useful in many cases. Take, for example: Lebaran Season. In Indonesia, it is really common to have an effect on Lebaran season. The effect, however, is a bit different from a classic seasonality effect because it shows the characteristics of an irregular schedule. . Minimal Workflow . Let&#39;s us start using fbprophet by creating a baseline model. . Prepare the data . To use the fbprophet package, we first need to prepare our time series data into a specific format data frame required by the package. The data frame requires 2 columns: . ds: the time stamp column, stored in datetime64 data type | y: the value to be forecasted | . In this example, we will be using the total_qty as the value to be forecasted. . daily_total_qty = daily_sales_31[[&#39;date&#39;, &#39;total_qty&#39;]].rename( columns={&#39;date&#39;: &#39;ds&#39;, &#39;total_qty&#39;: &#39;y&#39;}) daily_total_qty.head() . ds y . 0 2013-01-02 | 568.0 | . 1 2013-01-03 | 423.0 | . 2 2013-01-04 | 431.0 | . 3 2013-01-05 | 415.0 | . 4 2013-01-06 | 435.0 | . Fitting Model . Let’s initiate a fbprophet object using Prophet() and fit the daily_total_qty data. The idea of fitting a time series model is to extract the pattern information of a time series in order to perform a forecasting over the specified period of time. . model_31 = Prophet() model_31.fit(daily_total_qty) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fc138847450&gt; . Forecasting . Based on the existing data, let&#39;s say we would like to perform a forecasting for 1 years into the future. To do that, we will need to first prepare a data frame that consist of the future time stamp range. Conveniently, fbprophet has provided .make_future_dataframe() method that help us to prepare the data: . future_31 = model_31.make_future_dataframe(periods=365, freq=&#39;D&#39;) future_31.tail() . ds . 1391 2016-10-26 | . 1392 2016-10-27 | . 1393 2016-10-28 | . 1394 2016-10-29 | . 1395 2016-10-30 | . Now we have acquired a new future_31 data frame that consist of a date span of the beginning of a time series to 365 days into the future. We will then use this data frame is to perform the forecasting by using .predict() method of our model_31: . forecast_31 = model_31.predict(future_31) forecast_31[[&#39;ds&#39;, &#39;trend&#39;, &#39;weekly&#39;, &#39;yearly&#39;, &#39;yhat&#39;]] . ds trend weekly yearly yhat . 0 2013-01-02 | 376.014905 | -32.833816 | 234.600919 | 577.782008 | . 1 2013-01-03 | 375.942465 | -26.061642 | 215.077487 | 564.958311 | . 2 2013-01-04 | 375.870026 | 55.637544 | 193.971274 | 625.478844 | . 3 2013-01-05 | 375.797587 | 82.002893 | 171.625948 | 629.426428 | . 4 2013-01-06 | 375.725148 | -2.450713 | 148.403261 | 521.677695 | . ... ... | ... | ... | ... | ... | . 1391 2016-10-26 | 153.726469 | -32.833816 | -32.499417 | 88.393237 | . 1392 2016-10-27 | 153.545641 | -26.061642 | -30.534326 | 96.949673 | . 1393 2016-10-28 | 153.364812 | 55.637544 | -27.645116 | 181.357241 | . 1394 2016-10-29 | 153.183984 | 82.002893 | -23.842381 | 211.344496 | . 1395 2016-10-30 | 153.003156 | -2.450713 | -19.162378 | 131.390064 | . 1396 rows × 5 columns . Recall that in General Additive Model, we use time series components and perform a summation of all components. In this case, we can see that the model is extracting 3 types of components: trend, weekly seasonality, and yearly seasonality. Means, in forecasting future values it will use the following formula: $yhat(t) = T(t) + S_{weekly}(t) + S_{yearly}(t)$ . We can manually confirm from forecast_31 that the column yhat is equal to trend + weekly + yearly. . forecast_result = forecast_31[&#39;yhat&#39;] forecast_add_components = forecast_31[&#39;trend&#39;] + forecast_31[&#39;weekly&#39;] + forecast_31[&#39;yearly&#39;] (forecast_result.round(10) == forecast_add_components.round(10)).all() . True . . Note: .round() is performed to prevent floating point overflow. . Visualize . Now, observe how .plot() method take our model_31, and newly created forecast_31 object to create a matplotlib object that shows the forecasting result. The black points in the plot shows the actual time series, and the blue line shows the fitted time series along with its forecasted values 365 days into the future. . fig = model_31.plot(forecast_31, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) . We can also visualize each of the trend and seasonality components using .plot_components method. . fig = model_31.plot_components(forecast_31) . From the visualization above, we can get insights such as: . The trend shows that the total_qty sold is decreasing from time to time. | The weekly seasonality shows that sales on weekends are higher than weekdays. | The yearly seasonality shows that sales peaked at the end of the year. | . Interactive Visualization . An interactive figure of the forecast and components can be created with plotly. . . Note: You will need to install plotly 4.0 or above separately, as it will not by default be installed with fbprophet. You will also need to install the notebook and ipywidgets packages. . plot_plotly(model_31, forecast_31) . . . plot_components_plotly(model_31, forecast_31) . . . Trend Component . The trend components of our model, as plotted using .plot_components() method is producing a decreasing trend over the year. Trend is defined as a long term movement of average over the year. The methods that is implemented by Prophet is by default a linear model as shown below: . # for illustration purposes only from datetime import date # prepare data daily_sales_31_copy = daily_sales_31.copy() daily_sales_31_copy[&#39;date_ordinal&#39;] = daily_sales_31_copy[&#39;date&#39;].apply(lambda date: date.toordinal()) # visualize regression line plt.figure(figsize=(10, 5)) ax = sns.regplot(x=&#39;date_ordinal&#39;, y=&#39;total_qty&#39;, data=daily_sales_31_copy, scatter_kws={&#39;color&#39;: &#39;black&#39;, &#39;s&#39;: 2}, line_kws={&#39;color&#39;: &#39;red&#39;}) new_labels = [date.fromordinal(int(item)) for item in ax.get_xticks()] ax.set_xticklabels(new_labels) plt.xlabel(&#39;date&#39;) plt.show() . . Automatic Changepoint Detection . Prophet however implements a changepoint detection which tries to automatically detect a point where the slope has a significant change rate. It will tries to split the series using several points where the trend slope is calculated for each range. . Note: By default, prophet specifies 25 potential changepoints (n_changepoints=25) which are placed uniformly on the first 80% of the time series (changepoint_range=0.8). . # for illustration purposes only from fbprophet.plot import add_changepoints_to_plot fig = model_31.plot(forecast_31, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) a = add_changepoints_to_plot(fig.gca(), model_31, forecast_31, threshold=0) . . From the 25 potential changepoints, it will then calculate the magnitude of the slope change rate and decided the significant change rate. The model detected 3 significant changepoints and separate the series into 4 different trend slopes. . fig = model_31.plot(forecast_31, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) a = add_changepoints_to_plot(fig.gca(), model_31, forecast_31) . Adjusting Trend Flexibility . Prophet provided us a tuning parameter to adjust the detection flexibility: . n_changepoints (default = 25): The number of potential changepoints, not recommended to be tuned, this is better tuned by adjusting the regularization (changepoint_prior_scale) | changepoint_range (default = 0.8): Proportion of the history in which the trend is allowed to change. Recommended range: [0.8, 0.95] | changepoint_prior_scale (default = 0.05): The flexibility of the trend, and in particular how much the trend changes at the trend changepoints. Recommended range: [0.001, 0.5] | . . Tip: Increasing the default value of the parameter above will give extra flexibility to the trend line (overfitting the training data). On the other hand, decreasing the value will cause the trend to be less flexible (underfitting). . model_tuning_trend = Prophet( n_changepoints=25, # default = 25 changepoint_range=0.8, # default = 0.8 changepoint_prior_scale=0.5 # default = 0.05 ) model_tuning_trend.fit(daily_total_qty) # forecasting future = model_tuning_trend.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast = model_tuning_trend.predict(future) # visualize fig = model_tuning_trend.plot(forecast, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) a = add_changepoints_to_plot(fig.gca(), model_tuning_trend, forecast) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . Seasonality Component . Let’s talk about other time series component, seasonality. We will review the following plot components. . fig = model_31.plot_components(forecast_31) . By default, Prophet will try to determine existing seasonality based on existing data provided. In our case, the data provided is a daily data from early 2013 to end 2015. . Any daily sampled data by default will be detected to have a weekly seasonality. | While yearly seasonality, by default will be set as True if the provided data has more than 2 years of daily sample. | The other regular seasonality is a daily seasonality which tries to model an hourly pattern of a time series. Since our data does not accommodate hourly data, by default the daily seasonality will be set as False. | . Fourier Order . Prophet uses a Fourier series to approximate the seasonality effect. It is a way of approximating a periodic function as a (possibly infinite) sum of sine and cosine functions. . . Tip: The number of terms in the partial sum (the order) is a parameter that determines how quickly the seasonality can change. Increasing the fourier order will give extra flexibility to the seasonality (overfitting the training data), and vice versa. . An Interactive Introduction to Fourier Transforms . model_tuning_seasonality = Prophet( weekly_seasonality=3, # default = 3 yearly_seasonality=200 # default = 10 ) model_tuning_seasonality.fit(daily_total_qty) # forecasting future = model_tuning_seasonality.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast = model_tuning_seasonality.predict(future) # visualize fig = model_tuning_seasonality.plot_components(forecast) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . Custom Seasonalities . The default provided seasonality modelled by Prophet for a daily sampled data is: weekly and yearly. . Consider this case: a sales in your business is heavily affected by payday. Most customers tends to buy your product based on the day of the month. Since it did not follow the default seasonality of yearly and weekly, we will need to define a non-regular seasonality. There are two steps we have to do: . Remove default seasonality (eg: remove yearly seasonality) by setting False | Add seasonality (eg: add monthly seasonality) by using .add_seasonality() method before fitting the model | We ended up with formula: $yhat(t) = T(t) + S_{weekly}(t) + bf{S_{monthly}(t)}$ . model_custom_seasonality = Prophet( yearly_seasonality=False # remove seasonality ) # add seasonality model_custom_seasonality.add_seasonality( name=&#39;monthly&#39;, period=30.5, fourier_order=5) model_custom_seasonality.fit(daily_total_qty) # forecasting future = model_custom_seasonality.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast = model_custom_seasonality.predict(future) # visualize fig = model_custom_seasonality.plot_components(forecast) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . For monthly seasonality, we provided period = 30.5 indicating that there will be non-regular 30.5 frequency in one season of the data. The 30.5 is a common frequency quantifier for monthly seasonality, since there are some months with a total of 30 and 31 (some are 28 or 29). . . Tip: Recommended Fourier order according to the seasonality, 3 for weekly seasonality, 5 for monthly seasonality, and 10 for yearly seasonality. . Holiday Effects . One of the advantage in using Prophet is the ability to model a holiday effect. This holiday effect is defined as a non-regular effect that needs to be manually specified by the user. . Modeling Holidays and Special Events . Now let’s take a better look for our data. We could see that every end of a year, there is a significant increase of sales which exceeds 800 sales a day. . fig = model_31.plot(forecast_31, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) plt.axhline(y=800, color=&#39;red&#39;, ls=&#39;--&#39;) plt.show() . . Table below shows that the relatively large sales mostly happened at the very end of a year between 27th to 31st December. Now let’s assume that this phenomenon is the result of the new year eve where most people spent the remaining budget of their Christmas or End year bonus to buy our goods. . daily_total_qty[daily_total_qty[&#39;y&#39;] &gt; 800] . ds y . 64 2013-03-07 | 803.0 | . 359 2013-12-27 | 861.0 | . 360 2013-12-28 | 1028.0 | . 361 2013-12-29 | 962.0 | . 362 2013-12-30 | 1035.0 | . 363 2013-12-31 | 891.0 | . 723 2014-12-27 | 942.0 | . 725 2014-12-29 | 839.0 | . 726 2014-12-30 | 1080.0 | . 727 2014-12-31 | 912.0 | . We&#39;ll need to prepare a holiday data frame with the following column: . holiday: the holiday unique name identifier | ds: timestamp | lower_window: how many time unit behind the holiday that is assumed to to be affected (smaller or equal than zero) | upper_window: how many time unit after the holiday that is assumed to be affected (larger or equal to zero) | . . Important: It must include all occurrences of the holiday, both in the past (back as far as the historical data go) and in the future (out as far as the forecast is being made). . holiday = pd.DataFrame({ &#39;holiday&#39;: &#39;new_year_eve&#39;, &#39;ds&#39;: pd.to_datetime([&#39;2013-12-31&#39;, &#39;2014-12-31&#39;, # past date, historical data &#39;2015-12-31&#39;]), # future date, to be forecasted &#39;lower_window&#39;: -4, # include 27th - 31st December &#39;upper_window&#39;: 0}) holiday . holiday ds lower_window upper_window . 0 new_year_eve | 2013-12-31 | -4 | 0 | . 1 new_year_eve | 2014-12-31 | -4 | 0 | . 2 new_year_eve | 2015-12-31 | -4 | 0 | . Once we have prepared our holiday data frame, we can pass that into the Prophet() class: . model_holiday = Prophet(holidays=holiday) model_holiday.fit(daily_total_qty) # forecasting future = model_holiday.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast = model_holiday.predict(future) # visualize fig = model_holiday.plot(forecast, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . Now, observe how it has more confidence in capturing the holiday effect on the end of the year instead of relying on the yearly seasonality effect. If we plot the components, we could also get the holiday components listed as one of the time series components: . fig = model_holiday.plot_components(forecast) . Built-in Country Holidays . We can use a built-in collection of country-specific holidays using the .add_country_holidays() method before fitting model. For Indonesia, we can specify parameter country_name=&#39;ID&#39;. . model_holiday_indo = Prophet() model_holiday_indo.add_country_holidays(country_name=&#39;ID&#39;) model_holiday_indo.fit(daily_total_qty) model_holiday_indo.train_holiday_names . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. /usr/local/lib/python3.7/dist-packages/fbprophet/hdays.py:105: Warning: We only support Nyepi holiday from 2009 to 2019 . 0 New Year&#39;s Day 1 Chinese New Year 2 Day of Silence/ Nyepi 3 Ascension of the Prophet 4 Labor Day 5 Ascension of Jesus 6 Buddha&#39;s Birthday 7 Eid al-Fitr 8 Independence Day 9 Feast of the Sacrifice 10 Islamic New Year 11 Christmas 12 Birth of the Prophet dtype: object . . Tip: We can also manually populate Indonesia holidays by using hdays module. This is useful if we want to take a look on the holiday dates and then manually include only certain holidays. . # Reference code: https://github.com/facebook/prophet/blob/master/python/fbprophet/hdays.py holidays_indo = hdays.Indonesia() holidays_indo._populate(2021) pd.DataFrame([holidays_indo], index=[&#39;holiday&#39;]).T.rename_axis(&#39;ds&#39;).reset_index() . . /usr/local/lib/python3.7/dist-packages/fbprophet/hdays.py:105: Warning: We only support Nyepi holiday from 2009 to 2019 . ds holiday . 0 2021-01-01 | New Year&#39;s Day | . 1 2021-02-12 | Chinese New Year | . 2 2021-03-11 | Ascension of the Prophet | . 3 2021-05-01 | Labor Day | . 4 2021-05-13 | Ascension of Jesus | . 5 2021-05-26 | Buddha&#39;s Birthday | . 6 2021-06-01 | Pancasila Day | . 7 2021-05-14 | Eid al-Fitr | . 8 2021-08-17 | Independence Day | . 9 2021-07-20 | Feast of the Sacrifice | . 10 2021-08-10 | Islamic New Year | . 11 2021-10-19 | Birth of the Prophet | . 12 2021-12-25 | Christmas | . Adding Regressors . Additional regressors can be added to the linear part of the model using the .add_regressor() method, before fitting model. In this case, we want to forecast total_revenue based on its previous revenue components (trend, seasonality, holiday) and also total_qty sold as the regressor: . $revenue(t) = T_{revenue}(t) + S_{revenue}(t) + H_{revenue}(t) + bf{qty(t)}$ . . Important: The extra regressor must be known for both the history and for future dates. It thus must either be something that has known future values or something that has separately been forecasted with a time series model, such as Prophet. A note of caution around this approach: error in the forecast of regressor will produce error in the forecast of target value. . Forecast the Regressor (total_qty) . In this section, we separately create a Prophet model to forecast total_qty, before we forecast total_revenue. . model_total_qty = Prophet( n_changepoints=20, # trend flexibility changepoint_range=0.9, # trend flexibility changepoint_prior_scale=0.25, # trend flexibility weekly_seasonality=5, # seasonality fourier order yearly_seasonality=False, # remove seasonality holidays=holiday # new year eve effects ) # add monthly seasonality model_total_qty.add_seasonality(name=&#39;monthly&#39;, period=30.5, fourier_order=5) model_total_qty.fit(daily_total_qty) # forecasting future = model_total_qty.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast_total_qty = model_total_qty.predict(future) # visualize fig = model_total_qty.plot( forecast_total_qty, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . The table below shows the forecasted total quantity for the last 365 days (from November 1st, 2015 until October 30th, 2016). . forecasted_total_qty = forecast_total_qty[[&#39;ds&#39;, &#39;yhat&#39;]].tail(365) .rename(columns={&#39;yhat&#39;: &#39;total_qty&#39;}) forecasted_total_qty . ds total_qty . 1031 2015-11-01 | 193.255929 | . 1032 2015-11-02 | 128.734150 | . 1033 2015-11-03 | 164.798473 | . 1034 2015-11-04 | 158.194736 | . 1035 2015-11-05 | 178.743638 | . ... ... | ... | . 1391 2016-10-26 | 165.831597 | . 1392 2016-10-27 | 171.156393 | . 1393 2016-10-28 | 247.616489 | . 1394 2016-10-29 | 271.470601 | . 1395 2016-10-30 | 189.416426 | . 365 rows × 2 columns . On the other hand, the table below shows the actual total quantity which we used for training model. We have to rename the column exactly like the previous table. . actual_total_qty = daily_total_qty.rename(columns={&#39;y&#39;: &#39;total_qty&#39;}) actual_total_qty . ds total_qty . 0 2013-01-02 | 568.0 | . 1 2013-01-03 | 423.0 | . 2 2013-01-04 | 431.0 | . 3 2013-01-05 | 415.0 | . 4 2013-01-06 | 435.0 | . ... ... | ... | . 1026 2015-10-27 | 123.0 | . 1027 2015-10-28 | 117.0 | . 1028 2015-10-29 | 152.0 | . 1029 2015-10-30 | 267.0 | . 1030 2015-10-31 | 249.0 | . 1031 rows × 2 columns . Now, we have to prepare concatenated data of total_qty as the regressor values of total_revenue: . First 1031 observations: actual values of total_qty | Last 365 observations: forecasted values of total_qty | . future_with_regressor = pd.concat([actual_total_qty, forecasted_total_qty]) future_with_regressor . ds total_qty . 0 2013-01-02 | 568.000000 | . 1 2013-01-03 | 423.000000 | . 2 2013-01-04 | 431.000000 | . 3 2013-01-05 | 415.000000 | . 4 2013-01-06 | 435.000000 | . ... ... | ... | . 1391 2016-10-26 | 165.831597 | . 1392 2016-10-27 | 171.156393 | . 1393 2016-10-28 | 247.616489 | . 1394 2016-10-29 | 271.470601 | . 1395 2016-10-30 | 189.416426 | . 1396 rows × 2 columns . Forecast the Target Variable (total_revenue) . Next, we create a Prophet model to forecast total_revenue, using total_qty as the regressor. Make sure to rename the date as ds and the value to be forecasted as y. . daily_total_revenue = daily_sales_31[[&#39;date&#39;, &#39;total_revenue&#39;, &#39;total_qty&#39;]].rename( columns={&#39;date&#39;: &#39;ds&#39;, &#39;total_revenue&#39;: &#39;y&#39;}) daily_total_revenue.head() . ds y total_qty . 0 2013-01-02 | 396376.10 | 568.0 | . 1 2013-01-03 | 276933.11 | 423.0 | . 2 2013-01-04 | 286408.00 | 431.0 | . 3 2013-01-05 | 273245.00 | 415.0 | . 4 2013-01-06 | 260775.00 | 435.0 | . During fitting a model with regressor, make sure: . Apply .add_regressor() method before fitting | Forecast the value using future_with_regressor data frame that we have prepared before, containing ds and the regressor values | . model_total_revenue = Prophet( holidays=holiday # new year eve effects ) # add regressor model_total_revenue.add_regressor(&#39;total_qty&#39;) model_total_revenue.fit(daily_total_revenue) # forecasting # use dataframe with regressor, instead of just `ds` column forecast_total_revenue = model_total_revenue.predict(future_with_regressor) # visualize fig = model_total_revenue.plot( forecast_total_revenue, xlabel=&#39;Date&#39;, ylabel=&#39;Total Revenue&#39;) . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . If we plot the components, we could also get the extra regressors components listed as one of the time series components: . fig = model_total_revenue.plot_components(forecast_total_revenue) . . Important: By adding regressors, we lose the ability to interpret the other components (trend, seasonality, holiday) due to the fluctuation of the extra regressor value. . Forecasting Evaluation . Recall how we performed a visual analysis on how the performance of our forecasting model earlier. The technique was in fact, a widely used technique for model cross-validation. It involves splitting our data into two parts: . Train data is used to train our time series model in order to acquire the underlying patterns such as trend and seasonality. | Test data is purposely being kept for us to perform a cross-validation and see how our model perform on an unseen data. | . Train-Test Split . Recall that our data has the range of early 2013 to end 2015. Say, we are going to save the records of 2015 as a test data and use the rest for model training. The points in red will now be treated as unseen data and will not be passed in to our Prophet model. . cutoff = pd.to_datetime(&#39;2015-01-01&#39;) daily_total_qty[&#39;type&#39;] = daily_total_qty[&#39;ds&#39;].apply( lambda date: &#39;train&#39; if date &lt; cutoff else &#39;test&#39;) plt.figure(figsize=(10, 5)) sns.scatterplot(x=&#39;ds&#39;, y=&#39;y&#39;, hue=&#39;type&#39;, s=5, palette=[&#39;black&#39;, &#39;red&#39;], data=daily_total_qty) plt.axvline(x=cutoff, color=&#39;gray&#39;, label=&#39;cutoff&#39;) plt.legend() plt.show() . . We can split at a cutoff using conditional subsetting as below: . train = daily_total_qty[daily_total_qty[&#39;ds&#39;] &lt; cutoff] test = daily_total_qty[daily_total_qty[&#39;ds&#39;] &gt;= cutoff] print(f&#39;Train length: {train.shape[0]} days&#39;) print(f&#39;Test length: {test.shape[0]} days&#39;) . Train length: 728 days Test length: 303 days . Now let&#39;s train the model using data from 2013-2014 only, and forecast 303 days into the future (until October 31st, 2015). . model_final = Prophet( holidays=holiday, # holiday effect yearly_seasonality=True) # add monthly seasonality model_final.add_seasonality(name=&#39;monthly&#39;, period=30.5, fourier_order=5) model_final.fit(train) # only training set # forecasting future_final = model_final.make_future_dataframe( periods=303, freq=&#39;D&#39;) # 303 days (test size) forecast_final = model_final.predict(future_final) # visualize fig = model_final.plot(forecast_final, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) plt.scatter(x=test[&#39;ds&#39;], y=test[&#39;y&#39;], s=10, color=&#39;red&#39;) plt.show() . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . Evaluation Metrics . Based on the plot above, we can see that the model is capable in forecasting the actual future value. But for most part, we will need to quantify the error to be able to have a conclusive result. To quantify an error, we need to calculate the difference between actual demand and the forecasted demand. However, there are several metrics we can use to express the value. Some of them are: . Root Mean Squared Error | . $RMSE = displaystyle{ sqrt{ frac{1}{n} sum_{i=1}^{n}{(p_i - a_i)^2}}}$ Root Mean Squared Logarithmic Error | . $RMSLE = displaystyle{ sqrt{ frac{1}{n} sum_{i=1}^{n}{(log(p_i + 1) - log(a_i + 1))^2}}}$ Notation: . $n$: length of time series | $p_i$: predicted value at time $i$ | $a_i$: actual value at time $i$ | . # for illustration purposes only # calculation err = np.arange(-500, 500) p = np.arange(0, 1000) a = p - err rmse_plot = np.power(err, 2) ** 0.5 rmsle_plot = np.power(np.log1p(p) - np.log1p(a), 2) ** 0.5 # visualization fig, axes = plt.subplots(1, 2, figsize=(12, 5)) for idx, (ax, y) in enumerate(zip(axes, [rmse_plot, rmsle_plot])): ax.plot(err[err &lt; 0], y[err &lt; 0], color=&#39;red&#39;, label=&quot;Underestimate&quot;) ax.plot(err[err &gt; 0], y[err &gt; 0], color=&#39;blue&#39;, label=&quot;Overestimate&quot;) ax.set_xlabel(&quot;Error (Prediction - Actual)&quot;) ax.set_ylabel(f&quot;RMS{&#39;L&#39; if idx else &#39;&#39;}E&quot;) ax.set_title(f&quot;Root Mean Squared {&#39;Logarithmic &#39; if idx else &#39;&#39;}Error&quot;) ax.legend() plt.show() . . . Important: The main reason RMSLE is prefered over RMSE: It incurs a larger penalty for the underestimation of the actual value than the overestimation. This is useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated. . forecast_train = forecast_final[forecast_final[&#39;ds&#39;] &lt; cutoff] train_rmsle = mean_squared_log_error(y_true=train[&#39;y&#39;], y_pred=forecast_train[&#39;yhat&#39;]) ** 0.5 train_rmsle . 0.18125248249390455 . forecast_test = forecast_final[forecast_final[&#39;ds&#39;] &gt;= cutoff] test_rmsle = mean_squared_log_error(y_true=test[&#39;y&#39;], y_pred=forecast_test[&#39;yhat&#39;]) ** 0.5 test_rmsle . 0.36602550027367353 . . Note: Any of the metrics can be used to benchmark a forecasting model, as long as we are consistent in using it. Other regression metrics can be seen on scikit-learn documentation. . Expanding Window Cross Validation . Instead of only doing one time train-test split, we can do cross validation as shown below: . . This cross validation procedure is called as expanding window and can be done automatically by using the cross_validation() method. There are three parameters to be specified: . initial: the length of the initial training period | horizon: forecast length | period: spacing between cutoff dates | . df_cv = cross_validation(model_total_revenue, initial=&#39;730 days&#39;, horizon=&#39;30 days&#39;, period=&#39;90 days&#39;) df_cv . INFO:fbprophet:Making 4 forecasts with cutoffs between 2015-01-04 00:00:00 and 2015-10-01 00:00:00 . ds yhat yhat_lower yhat_upper y cutoff . 0 2015-01-05 | 453243.686249 | 343139.314528 | 555740.657548 | 357996.0 | 2015-01-04 | . 1 2015-01-06 | 528839.864534 | 416678.540820 | 641381.035848 | 562368.0 | 2015-01-04 | . 2 2015-01-07 | 400103.526338 | 289172.328596 | 514448.356461 | 290563.0 | 2015-01-04 | . 3 2015-01-08 | 282273.692327 | 176986.342806 | 394101.768913 | 285423.0 | 2015-01-04 | . 4 2015-01-09 | 255507.073608 | 143118.443054 | 367993.065885 | 232971.0 | 2015-01-04 | . ... ... | ... | ... | ... | ... | ... | . 115 2015-10-27 | 111576.408050 | 7996.709968 | 213987.808389 | 111851.0 | 2015-10-01 | . 116 2015-10-28 | 91895.680477 | -5108.655465 | 196370.841312 | 180557.0 | 2015-10-01 | . 117 2015-10-29 | 126869.389853 | 23990.467195 | 226052.458306 | 103456.0 | 2015-10-01 | . 118 2015-10-30 | 237405.154426 | 142924.554223 | 335557.351306 | 204317.0 | 2015-10-01 | . 119 2015-10-31 | 187605.724288 | 83049.868234 | 293944.613439 | 237587.0 | 2015-10-01 | . 120 rows × 6 columns . The cross validation process above will be carried out for 4 folds, where at each fold a forecast will be made for the next 30 days (horizon) from the cutoff dates. Below is the illustration for each fold: . # for illustration purposes only df_copy = daily_total_qty[[&#39;ds&#39;, &#39;y&#39;]].copy() df_cutoff_horizon = df_cv.groupby(&#39;cutoff&#39;)[[&#39;ds&#39;]].max() for i, (cutoff, horizon) in enumerate(df_cutoff_horizon.iterrows()): horizon_cutoff = horizon[&#39;ds&#39;] df_copy[&#39;type&#39;] = df_copy[&#39;ds&#39;].apply( lambda date: &#39;train&#39; if date &lt; cutoff else &#39;test&#39; if date &lt; horizon_cutoff else &#39;unseen&#39;) plt.figure(figsize=(10, 5)) sns.scatterplot(x=&#39;ds&#39;, y=&#39;y&#39;, hue=&#39;type&#39;, s=5, palette=[&#39;black&#39;, &#39;red&#39;, &#39;gray&#39;], data=df_copy) plt.axvline(x=cutoff, color=&#39;gray&#39;, label=&#39;cutoff&#39;) plt.axvline(x=horizon_cutoff, color=&#39;gray&#39;, ls=&#39;--&#39;, label=&#39;horizon&#39;) plt.legend(bbox_to_anchor=(1, 1), loc=&#39;upper left&#39;) plt.title( f&quot;FOLD {i+1} nTRAIN SIZE: {df_copy[&#39;type&#39;].value_counts()[&#39;train&#39;]} DAYS&quot;) plt.show() . . Cross validation error metrics can be evaluated for each folds, here shown for RMSLE. . cv_rmsle = df_cv.groupby(&#39;cutoff&#39;).apply( lambda x: mean_squared_log_error(y_true=x[&#39;y&#39;], y_pred=x[&#39;yhat&#39;]) ** 0.5) cv_rmsle . cutoff 2015-01-04 0.235920 2015-04-04 0.304658 2015-07-03 0.236829 2015-10-01 0.308821 dtype: float64 . We can aggregate the metrics by using its mean. In other words, we are calculating the mean of RMSLE to represent the overall model performance. . cv_rmsle.mean() . 0.27155703183152 . Error Diagnostics . Prophet has provide us several frequently used evaluation metrics by using performance_metrics(): . Mean Squared Error (MSE) | Root Mean Squared Error (RMSE) | Mean Absolute Error (MAE) | Mean Absolute Percentage Error (MAPE) | Median Absolute Percentage Error (MDAPE) | Coverage: Percentage of actual data that falls on the forecasted uncertainty (confidence) interval | . df_p = performance_metrics(df_cv, rolling_window=0) df_p.tail() . horizon mse rmse mae mape mdape coverage . 25 26 days | 3.791062e+09 | 61571.598733 | 50099.136212 | 0.269882 | 0.274819 | 1.0 | . 26 27 days | 2.667743e+09 | 51650.199465 | 40674.234599 | 0.198813 | 0.127268 | 1.0 | . 27 28 days | 6.598579e+08 | 25687.698689 | 25620.623111 | 0.207142 | 0.215036 | 1.0 | . 28 29 days | 1.654698e+09 | 40677.980863 | 37825.473664 | 0.276104 | 0.250000 | 1.0 | . 29 30 days | 2.441526e+09 | 49411.803495 | 46637.639525 | 0.405376 | 0.345830 | 1.0 | . Cross validation performance metrics can be visualized with plot_cross_validation_metric, here shown for RMSE. . Dots show the root squared error (RSE) for each prediction in df_cv. | The blue line shows the RMSE for each horizon. | . fig = plot_cross_validation_metric(df_cv, metric=&#39;rmse&#39;, rolling_window=0) . . Note: Unfortunately, Prophet has not implement RMSLE metric in their library. Therefore, we have to calculate it manually according to its mathematical formula, or simply use sklearn.metrics module. . Dots show the root squared logarithmic error (RSLE) for each prediction in df_cv. | The blue line shows the RMSLE for each horizon. | . # calculation df_cv[&#39;horizon&#39;] = (df_cv[&#39;ds&#39;] - df_cv[&#39;cutoff&#39;]).dt.days df_cv[&#39;sle&#39;] = np.power((np.log1p(df_cv[&#39;yhat&#39;]) - np.log1p(df_cv[&#39;y&#39;])), 2) # squared logarithmic error df_cv[&#39;rsle&#39;] = df_cv[&#39;sle&#39;] ** 0.5 horizon_rmsle = df_cv.groupby(&#39;horizon&#39;)[&#39;sle&#39;].apply(lambda x: x.mean() ** 0.5) # root mean # visualization plt.figure(figsize=(10, 5)) sns.scatterplot(x=&#39;horizon&#39;, y=&#39;rsle&#39;, s=15, color=&#39;gray&#39;, data=df_cv) plt.plot(horizon_rmsle.index, horizon_rmsle, color=&#39;blue&#39;) plt.grid(b=True, which=&#39;major&#39;, color=&#39;gray&#39;, linestyle=&#39;-&#39;) plt.xlabel(&#39;Horizon (days)&#39;) plt.ylabel(&#39;RMSLE&#39;) plt.show() . . Hyperparameter Tuning . In this section, we implement a Grid search algorithm for model tuning by using for-loop. It builds model for every combination from specified hyperparameters and then evaluate it. The goal is to choose a set of optimal hyperparameters which minimize the forecast error (in this case, smallest RMSLE). . . Tip: Visit the documentation here for a list of recommended hyperparameters to be tuned. . param_grid = { &#39;changepoint_prior_scale&#39;: [0.001, 0.01, 0.1], &#39;changepoint_range&#39;: [0.8, 0.95] } # Generate all combinations of parameters all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())] rmsles = [] # Store the RMSLEs for each params here # Use cross validation to evaluate all parameters for params in tqdm(all_params): # fitting model # (TO DO: change the data and add other components: seasonality, holiday, regressor) model = Prophet(**params, holidays=holiday) model.fit(daily_total_qty) # Expanding window cross validation (TO DO: use different values) cv = cross_validation(model, initial=&#39;730 days&#39;, period=&#39;90 days&#39;, horizon=&#39;60 days&#39;, parallel=&#39;processes&#39;) # Evaluation metrics: RMSLE rmsle = cv.groupby(&#39;cutoff&#39;).apply( lambda x: mean_squared_log_error(y_true=x[&#39;y&#39;], y_pred=x[&#39;yhat&#39;]) ** 0.5) mean_rmsle = rmsle.mean() rmsles.append(mean_rmsle) . 0%| | 0/6 [00:00&lt;?, ?it/s]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc13488c790&gt; 17%|█▋ | 1/6 [00:06&lt;00:30, 6.14s/it]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc13496a9d0&gt; 33%|███▎ | 2/6 [00:12&lt;00:24, 6.02s/it]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc136479510&gt; 50%|█████ | 3/6 [00:18&lt;00:18, 6.26s/it]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc136fba310&gt; 67%|██████▋ | 4/6 [00:24&lt;00:12, 6.29s/it]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc1347e73d0&gt; 83%|████████▎ | 5/6 [00:31&lt;00:06, 6.28s/it]INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. INFO:fbprophet:Making 3 forecasts with cutoffs between 2015-03-05 00:00:00 and 2015-09-01 00:00:00 INFO:fbprophet:Applying in parallel with &lt;concurrent.futures.process.ProcessPoolExecutor object at 0x7fc13488c790&gt; 100%|██████████| 6/6 [00:37&lt;00:00, 6.28s/it] . . Note: In this section, we only forecast daily_total_qty for the sake of explanation simplicity. . We can observe the error metrics for each hyperparameter combination, and sort by ascending: . tuning_results = pd.DataFrame(all_params) tuning_results[&#39;rmsle&#39;] = rmsles tuning_results.sort_values(by=&#39;rmsle&#39;) . changepoint_prior_scale changepoint_range rmsle . 5 0.100 | 0.95 | 0.264595 | . 4 0.100 | 0.80 | 0.264672 | . 3 0.010 | 0.95 | 0.265335 | . 2 0.010 | 0.80 | 0.266934 | . 0 0.001 | 0.80 | 0.268179 | . 1 0.001 | 0.95 | 0.275176 | . Best hyperparameter combination can be extracted as follows: . best_params = all_params[np.argmin(rmsles)] best_params . {&#39;changepoint_prior_scale&#39;: 0.1, &#39;changepoint_range&#39;: 0.95} . Lastly, re-fit the model and use it for forecasting. . model_best = Prophet(**best_params, holidays=holiday) model_best.fit(daily_total_qty) model_best . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7fc13496a610&gt; . . Note: ** is an operator for dictionary unpacking. It delivers key-value pairs in a dictionary into a function&#8217;s arguments. . Final Forecasting Result . In this last section, we use model_best to forecast daily total quantity sold for 1 year to the future, i.e. from November 1st, 2015 to October 30th, 2016. . future_best = model_best.make_future_dataframe(periods=365, freq=&#39;D&#39;) forecast_best = model_best.predict(future_best) # visualize fig = model_best.plot(forecast_best, xlabel=&#39;Date&#39;, ylabel=&#39;Quantity Sold&#39;) . The final forecasting result then can be used as a sales target for one year ahead. . References . Prophet Documentation | Paper: Forecasting at Scale | Time Series Forecasting using prophet in R | .",
            "url": "https://tomytjandra.github.io/blogs/python/time-series/forecasting/prophet/2021/01/15/sales-forecasting-using-prophet.html",
            "relUrl": "/python/time-series/forecasting/prophet/2021/01/15/sales-forecasting-using-prophet.html",
            "date": " • Jan 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Explainable Heart Disease Classifier with Shapley Additive Explanations (SHAP)",
            "content": "Problem Statement . Heart disease describes a range of conditions that affect your heart. With growing stress, the number of cases of heart diseases are increasing rapidly. . According to the World Health Organisation (WHO), Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year. 17.9 million people die each year from CVDs, an estimated 31% of all deaths worldwide. . The doctors of Health Hospital in Zastra wish to incorporate Data Science into their workings. Seeing the rising cases of heart diseases, they are specially interested in predicting the presence of heart disease in a person using some existing data. . . Note: This case is taken from DPhi 51st Challenge . Objective . Build a Machine Learning model to determine if heart disease is present or not. | Build Shapley Additive Explanations (SHAP) explainer to explain the conditional interaction between the presence of heart disease and its predictor. | . Import Libraries . import numpy as np import pandas as pd # data visualization import matplotlib.pyplot as plt import seaborn as sns plt.style.use(&#39;seaborn&#39;) # modeling from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score # explainable AI import shap from IPython.core.display import display, HTML . Data Loading . Load the data which is downloaded from here . heart_data = pd.read_csv(&quot;data_input/heart_disease.csv&quot;) heart_data.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 48 | 1 | 2 | 124 | 255 | 1 | 1 | 175 | 0 | 0.0 | 2 | 2 | 2 | 1 | . 1 68 | 0 | 2 | 120 | 211 | 0 | 0 | 115 | 0 | 1.5 | 1 | 0 | 2 | 1 | . 2 46 | 1 | 0 | 120 | 249 | 0 | 0 | 144 | 0 | 0.8 | 2 | 0 | 3 | 0 | . 3 60 | 1 | 0 | 130 | 253 | 0 | 1 | 144 | 1 | 1.4 | 2 | 1 | 3 | 0 | . 4 43 | 1 | 0 | 115 | 303 | 0 | 1 | 181 | 0 | 1.2 | 1 | 0 | 2 | 1 | . Data description: . age: Age in years | sex: 1 = male, 0 = female | cp: Chest pain type | trestbps: Resting blood pressure (in mm Hg on admission to the hospital) | chol: serum cholesterol in mg/dl | fbs: fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false) | restecg: Resting electrocardiographic results | thalach: Maximum heart rate achieved | exang: Exercise induced angina (1 = yes; 0 = no) | oldpeak: ST depression induced by exercise relative to rest | slope: The slope of the peak exercise ST segment | ca: Number of major vessels (0-4) colored by fluoroscopy | thal: 0 = null, 1 = fixed defect found, 2 = blood flow is normal, 3 = reversible defect found | target: 1 = Heart disease present, 0 = Heart disease not present | . Exploratory Data Analysis . Check Missing Values . heart_data.isna().sum() . age 0 sex 0 cp 0 trestbps 0 chol 0 fbs 0 restecg 0 thalach 0 exang 0 oldpeak 0 slope 0 ca 0 thal 0 target 0 dtype: int64 . Great, there is no missing value in our data. . Class Proportion . class_prop = heart_data.target.value_counts() class_prop.plot.bar() plt.xticks([0, 1], labels=[&#39;Have Heart Disease&#39;, &#39;No Heart Disease&#39;], rotation=0) plt.show() . . (class_prop / class_prop.sum() * 100).round(2).astype(&#39;str&#39;) + &#39; %&#39; . . 1 54.25 % 0 45.75 % Name: target, dtype: object . The target variable is considered as balanced with 54:46 proportion. . Correlation Heatmap . plt.subplots(figsize=(10, 10)) sns.heatmap(heart_data.corr(), annot=True, linewidths=.5) plt.show() . . . Note: There are no alarming (strong) correlation between each features, which is great when we are going to build a linear model to avoid strong multicollinearity. . Distribution of Numerical Features . num_col = [&#39;age&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;] fig, axes = plt.subplots(2, 4, figsize=(20, 5)) for ax, col in zip(axes.flat, num_col): sns.kdeplot(data=heart_data, x=col, hue=&#39;target&#39;, ax=ax) ax.set_title(f&#39;{col.upper()} DISTRIBUTION&#39;) plt.tight_layout() plt.show() . . . Note: Based on the visualization above, heart disease is more likely present in lower age, higher restecg result, higher thalach, lower oldpeak, higher slope, and lower ca. There are only slight difference of trestbps and chol distribution between patient with and without heart disease. . cat_col = [&#39;sex&#39;, &#39;cp&#39;, &#39;fbs&#39;, &#39;exang&#39;, &#39;thal&#39;] # label for x-axis xlab_list = [[&#39;Female&#39;, &#39;Male&#39;], [&#39;0 (Typical)&#39;, &#39;1 (Atypical)&#39;, &#39;2 (Non-anginal)&#39;, &#39;3 (Asymptomatic)&#39;], [&#39;False&#39;, &#39;True&#39;], [&#39;No&#39;, &#39;Yes&#39;], [&#39;0 (Null)&#39;, &#39;1 (Fixed)&#39;, &#39;2 (Normal)&#39;, &#39;3 (Reversible)&#39;]] fig, axes = plt.subplots(2, 3, figsize=(15, 5)) for ax, col, xlab in zip(axes.flat, cat_col, xlab_list): sns.countplot(data=heart_data, x=col, hue=&#39;target&#39;, ax=ax) ax.set_title(f&#39;{col.upper()} PROPORTION&#39;) ax.set_xticklabels(xlab, rotation=10) axes[-1, -1].set_visible(False) # turn off last axis plt.tight_layout() plt.show() . . Based on the visualization above: . sex: Proportion of female with heart disease is more than those who doesn&#39;t have, while male is nearly the same. | cp: Patient with cp=0 is more likely doesn&#39;t have heart disease than other cp type. | fbs: The proportion is nearly the same. | exang: Patient with exang is more likely doesn&#39;t have heart disease than those who no exang. | thal: Patient with thal=2 is more likely have heart disease than other thal type. | . Data Preprocessing . Let&#39;s prepare the data before model fitting: . Feature-target splitting | Train-test splitting | # Feature-target splitting X = heart_data.drop(&#39;target&#39;, axis=1) y = heart_data.target # Train-test splitting X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) print(f&#39;X_train shape: {X_train.shape}&#39;) print(f&#39;X_test shape: {X_test.shape}&#39;) print(f&#39;y_train shape: {y_train.shape}&#39;) print(f&#39;y_test shape: {y_test.shape}&#39;) . . X_train shape: (169, 13) X_test shape: (43, 13) y_train shape: (169,) y_test shape: (43,) . Scale the data using StandardScaler to its Z-score. | scaler = StandardScaler() X_train_scale = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns) X_test_scale = pd.DataFrame(scaler.transform(X_test), columns=X.columns) . . Important: Only apply .fit() method on the training set, then use the fitted scaler to transform the testing set. . After standarization, the mean of X_train should be around 0 and the standard deviation (std) should be around 1. . pd.DataFrame({&#39;mean&#39;: X_train_scale.mean(), &#39;std&#39;: X_train_scale.std()}) . . mean std . age 1.716247e-16 | 1.002972 | . sex 5.124106e-17 | 1.002972 | . cp -9.591276e-17 | 1.002972 | . trestbps -4.795638e-17 | 1.002972 | . chol -5.091259e-18 | 1.002972 | . fbs 5.781043e-17 | 1.002972 | . restecg 2.496359e-17 | 1.002972 | . thalach -3.416071e-17 | 1.002972 | . exang -1.510954e-16 | 1.002972 | . oldpeak -1.267888e-16 | 1.002972 | . slope 1.327012e-16 | 1.002972 | . ca -1.313873e-17 | 1.002972 | . thal -3.521181e-16 | 1.002972 | . Logistic Regression . We fit the scaled X_train to the default LogisticRegression model. . lr = LogisticRegression() lr.fit(X_train_scale, y_train) . LogisticRegression() . Evaluate the model using the F1-score. This is the harmonic mean between precision and recall as follows: . $F_1 = 2 dfrac{Precision times Recall}{Precision + Recall}$ . y_pred_train = lr.predict(X_train_scale) y_pred_test = lr.predict(X_test_scale) print(f&#39;F1 score on train set: {f1_score(y_train, y_pred_train):.5f}&#39;) print(f&#39;F1 score on test set: {f1_score(y_test, y_pred_test):.5f}&#39;) . . F1 score on train set: 0.86813 F1 score on test set: 0.84000 . The performance of our model is quite good and doesn&#39;t overfit the training data. . SHAP Explainer . The SHAP values explain the output of a model (function) as a sum of the effect of each feature being introduced into a conditional expectation. It results from averaging over all possible orderings, since the order of introduced feature matters. The use of SHAP values can be detailed into two: . Global interpretability - the collective SHAP values can show the contribution of each predictor, either positively or negatively, to the target variable. . | Local interpretability - each observation have its own set of SHAP values. This enable us to pinpoint and contrast the impact of predictor for each individual case. . | explainer = shap.LinearExplainer(model=lr, masker=X_train_scale) explain_result = explainer(X_test_scale) shap_values = explain_result.values # explainer.shap_values(X_test_scale) shap_df = pd.DataFrame(shap_values, columns=X.columns) shap_df.head() . The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, or maskers.Impute) . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal . 0 0.086223 | -0.279619 | 1.159063 | -0.240417 | -0.034219 | -0.055162 | -0.673186 | 0.473878 | 0.570792 | 0.372988 | 0.318451 | 0.317311 | 0.357128 | . 1 -0.023755 | 0.622378 | 1.159063 | 0.037522 | -0.030002 | 0.005456 | -0.673186 | -1.113122 | 0.570792 | 0.372988 | 0.318451 | 0.317311 | 3.002523 | . 2 0.020236 | 0.622378 | 0.159871 | -0.055124 | 0.076274 | 0.005456 | 0.412598 | 0.505618 | 0.570792 | -0.254762 | 0.318451 | 0.317311 | 0.357128 | . 3 -0.287704 | -0.279619 | 0.159871 | 0.454429 | -0.013976 | 0.005456 | 0.412598 | 0.092998 | 0.570792 | 0.372988 | 0.318451 | 0.317311 | 0.357128 | . 4 0.218197 | -0.279619 | 2.158256 | -0.935263 | -0.020724 | 0.005456 | -0.673186 | 0.156478 | 0.570792 | 0.059113 | -0.359105 | 0.317311 | -0.965569 | . logit = explainer.expected_value odds = np.exp(logit) prob = odds/(1+odds) print(f&#39;BASE VALUE LOGIT (Expected Value): {logit}&#39;) print(f&#39;BASE VALUE PROB (Base Value): {prob}&#39;) . . BASE VALUE LOGIT (Expected Value): 0.2977444318944053 BASE VALUE PROB (Base Value): 0.5738910320745784 . The expected value refers to the average prediction on X_train_scale, also called as the base value on the force plot. . SHAP Force Plot . Individual SHAP Force Plot . Create a force plot for the first observation (index 0) of the test data. . X_test.iloc[[0]] . . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal . 206 58 | 1 | 2 | 140 | 211 | 1 | 0 | 165 | 0 | 0.0 | 2 | 0 | 2 | . shap.initjs() shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values[0], # first row of test data features=X_test.iloc[0], # first row of test data link=&#39;logit&#39;) # transforms log-odds to probability . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. . Tip: To visualize the force plot on fastpages, we have to manually add bundle.js inside _includes/custom-head.html. . The visualization above is called as an individual SHAP force plot for local interpretability, with components: . $f(x)$ or output value is the prediction for that observation. The predicted probability of the first patient of X_test_scale is 0.94. | Base value is the explainer expected value. The mean prediction probability is 0.5739. | Color of the bar indicates if the feature support/contradicts the prediction value. Red means that the feature is supporting the positive class (higher the chances of heart disease). Meanwhile, blue means that the feature is contradicting the positive class (lower the chances of heart disease). | . Note: Based on the color of the force plot, we can infer that the features cp, exang, thalach, oldpeak, thal, slope, ca, and age impact the output value of the first row X_test in positive way, meanwhile the other features (restecg, sex, trestbps, fbs, chol) impact negatively. . The force plot can be visualized as a waterfall plot to get the ranking of local important features. . . Note: ignore all the numbers inside the plot below, since the SHAP values is in logit (not probability) and the feature values is standardized, we just interested in the ranking. . shap.plots.waterfall(explain_result[0], # first row of test data max_display=len(X.columns)) . Let&#39;s infer each of the features by comparing it by the average values (whether the value is higher or lower). Remember the SHAP model is built on the training data set, so we compare with X_train.mean(). . (X_test.iloc[0] &gt; X_train.mean()).apply(lambda x: &quot;Higher&quot; if x else &quot;Lower&quot;) . . age Higher sex Higher cp Higher trestbps Higher chol Lower fbs Higher restecg Lower thalach Higher exang Lower oldpeak Lower slope Higher ca Lower thal Lower dtype: object . Based on the waterfall plot, it turns out that the top three most important feature for the first patient on testing data is cp, restecg, and exang. Let&#39;s interpret it as follows: . cp has a positive direction on first patient&#39;s probability of heart disease. The chest pain is type 2, which is higher than the average data. | restecg has a negative direction on first patient&#39;s probability of heart disease. The resting electrocardiographic results is 0, which is lower than the average data. | exang has a positive direction on first patient&#39;s probability of heart disease. The exercise induced angina is 0, which is lower than the average data. | . Multiple SHAP Force Plot . Create a force plot for the all observations of the test data. . shap.initjs() shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, # all rows of test data features=X_test, # all rows of test data link=&#39;logit&#39;) # transforms log-odds to probability . Visualization omitted, Javascript library not loaded! Have you run `initjs()` in this notebook? If this notebook was from another user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing this notebook on github the Javascript has been stripped for security. If you are using JupyterLab this error is because a JupyterLab extension has not yet been written. Instead of plotting force plot for one observation, we can plot it for all observation as shown above. The x-axis refer to the observation index, the y-axis refer to the predicted probability (which centered around its base value, 0.5739). . There are three main options for the x-axis: . Sample order by similarity, this order determined by the clustering algorithm behind the function (Reference). If we see carefully, the index 10 until 20 have lower probability of heart disease and have more blue than red region. Meaning that a lot of their features value decrease the probability of heart disease. | Sample order by output value, this order the observation in descending manner. | Original sample ordering, this shows the order as it is according to the row index on X_test. | The interpretation for the plot above is just the same as the section before. When hovered, the features labeled in red means pulling the probability of heart disease higher. On the other hand, the features labeled in blue means pulling the probability of heart disease lower. . The y-axis can be alter so that we can see the partial SHAP values for each features. Example for the first three options: . cp effects: Having cp = 0 (blue) decrease the predicted probability by 0.2061 (from base value to 0.3678), the other cp (red) will increase it to the corresponding hovered probability instead. | thal effects: Having thal = 3 (blue) decrease the predicted probability by 0.2349 (from base value to 0.339), the other thal (red) will increase it to the corresponding hovered probability instead. | exang effects: Having exang = 0 (red) increase the predicted probability by 0.1305 (from base value to 0.7044), exang = 1 (blue) decrease it by 0.2661 (from base value to 0.3078) instead. | . SHAP Summary Plot . shap.plots.bar(explain_result, # all rows of test data max_display=len(X.columns)) # shap.summary_plot(shap_values, features=X_test, plot_type=&#39;bar&#39;) . The visualization above is called as a summary or variable importance plot for global interpretability. It lists the most significant features in descending order by the mean absolute SHAP values of all observations. So, in this case, top three features that have high predictive power are: cp, thal, and exang. Meanwhile, the bottom three features are: fbs, chol, and age. . The summary plot above can further be plotted as a violin plot to show the positive and negative relationships with the target variable. . shap.summary_plot(shap_values, features=X_test, plot_type=&#39;violin&#39;) . The violin plot shows the distribution of SHAP values for each feature. Red indicates a higher feature values, while blue indicates a lower feature values. For example, the higher the cp value, the more positive SHAP value, means it has positive impact globally on the model output value. On the other hand, the higher the thal value, the more negative SHAP value, means it has negative impact globally on the model output value. . So we can conclude that globally cp, restecg, thalach, slope, age, chol have positive impact while the other features (thal, exang, oldpeak, ca, sex, trestbps, fbs) have negative impact. . SHAP Dependence Plot . fig, axes = plt.subplots(2, 7, figsize=(20, 5)) for ax, col in zip(axes.flat, X_test.columns): shap.dependence_plot(ind=col, interaction_index=col, shap_values=shap_values, # all rows of test data features=X_test, # all rows of test data display_features=X_test, # all rows of test data show=False, ax=ax) ax.set_title(f&#39;{col.upper()} DEPENDENCE PLOT&#39;) axes[-1,-1].set_visible(False) plt.tight_layout() plt.show() . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . The plot above shows how the value of each features correlate with its SHAP value. The trend is linear since the model we built is Logistic Regression which is a linear model. . The conclusion is exactly the same thing as before, on the violin plot of summary plot: We can conclude that globally cp, restecg, thalach, slope, age, chol have positive impact while the other features (thal, exang, oldpeak, ca, sex, trestbps, fbs) have negative impact. The difference is that, using summary plot we can ranked the features based on its importance too. . SHAP Decision plot . This is the section to explore what more we can do with SHAP, one of them is decision plot. It shows how the model make decisions and arrive at the prediction value. . r = shap.decision_plot(base_value=explainer.expected_value, shap_values=shap_values, # all rows of test data features=X_test, # all rows of test data link=&#39;logit&#39;, # transforms log-odds to probability highlight=0, # highlight first row observation return_objects=True) # returning the plot structures . The visualization above is the decision plot for all rows in test data. Moving from the bottom to the top, SHAP values for each feature are added to the model&#39;s base value. This shows how each feature contributes to the overall prediction. . The x-axis represents the model output value, in this case is the probability of heart disease (transformed using link=&#39;logit&#39;). The plot is centered on the x-axis at base value. | The y-axis lists the features, ordered by descending importance. | The colored line represents each observation&#39;s decision of prediction based on the features. | . We also highlight the first row on the test set, represented by a dotted line. To make it appear clear, we can separate it on a different decision plot as below: . shap.decision_plot(base_value=explainer.expected_value, shap_values=shap_values[0], # first row of test data features=X_test.iloc[0,:], # first row of test data link=&#39;logit&#39;, # transforms log-odds to probability feature_order=r.feature_idx) # preserving order of features from previous plot . The plot above is basically the combination of force plot and local importance plot, where we can know: . The impact of each feature positively (to the right) or negatively (to the left) | The ranking of features based on the importance (SHAP values) | . A decision plot can expose a model’s typical prediction paths. Here, we plot all of the predictions in the probability interval [0.9, 1.0] to see what high-scoring predictions have in common. We use feature_order=&#39;hclust&#39; to group similar prediction paths. . high_prob = np.where(lr.predict_proba(X_test_scale)[:,1] &gt;= 0.9)[0] shap.decision_plot(base_value=explainer.expected_value, shap_values=shap_values[high_prob], # rows with high probability features=X_test.iloc[high_prob,:], # rows with high probability feature_order=&#39;hclust&#39;, # hierarchical clustering link=&#39;logit&#39;) # transforms log-odds to probability . There are other uses of decision plot, such as: . Show a large number of feature effects clearly | Visualize multioutput predictions | Display the cumulative effect of interactions | Explore feature effects for a range of feature values | Identify outliers | Compare and contrast predictions for several models | . . Note: Reference of SHAP Decision Plot .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/scikit-learn/explainable-ai/2020/12/21/heart-disease-classification-logreg-shap.html",
            "relUrl": "/python/classification/scikit-learn/explainable-ai/2020/12/21/heart-disease-classification-logreg-shap.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Survival Analysis for Predictive Maintenance",
            "content": "import pandas as pd import numpy as np # data visualization import matplotlib.pyplot as plt import seaborn as sns # statistical test from scipy import stats # supress warning import warnings warnings.filterwarnings(&quot;ignore&quot;) . # reproducible model import os import random import numpy as np import torch # reference: https://github.com/pytorch/pytorch/issues/7068#issuecomment-487907668 def seed_torch(seed): random.seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if you are using multi-GPU torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True SEED = 123 seed_torch(SEED) . . Introduction . Survival Analysis arises because of the many concerns about something that might happen and be detrimental to life. We will undoubtedly prepare meticulous planning through this survival analysis only if we know when the worst possibility occurs. In addition, we also want to know what factors are most correlated with these hazards. The first question that will always arise is how long an object will last? . Survival Analysis is a statistical method used to analyze longitudinal data regarding events or events, such as . What is the probability that a patient will survive after doctor&#39;s statement of diagnosis? | How long will a customer stay with the products we produce until the customer churns? | How long can a production machine last after three years of use? | How is the retention rate of different marketing channels? | etc | . The example above is very interesting to be discussed further. For a deeper understanding, this post will explain the basic concepts and workings of Survival Analysis and how it is applied in industry. . . Concept . Event and Time . Survival Analysis is also known as time-to-event analysis. To better understand the concept of events and time, let&#39;s take a look the following application: . Marketing Analysis: The goals is to evaluate retention rates on each marketing channel. As we know, a company must have several services offered to customers according to the marketing targets that have been made. Here, we define the event as the act at which the customer unsubscribes from the marketing channel. Time is defined when the customer initiates a marketing channel service/subscription. The time scale can be months or weeks. . | Predictive Maintenance: This analysis is used to determine the validity period for mechanical parts/machines. An event is defined as a situation where the machine breaks down. Time is defined as when the machine can operate continuously in the production process. The time scale can be weeks, months, or years. . | . Censoring . Survival Analysis sounds very similar to regression. So, why we use a different modeling technique instead? Why not just use a simple linear regression? . Survival analysis is explicitly designed to handle data about terminal events where some observations may experience those events and others may not. Such observations are called &quot;censored&quot;. For example, the target variable represents the time to a terminal event, and the duration of study is constrained. Therefore, some observations will not experience the event. If we relate to the predictive maintenance case, some equipment will probably fail during performance monitoring, but some will not. . . There are two types of censoring, namely right censored data and left censored data: . Right censored data is the most common type of censoring that occurs when the observation ends or the individual is removed from the observation before the event occurs. For example, some individuals may be alive at the end of an observational trial or maybe dropped out before the study is terminated. | Data will be called as left censored if the initial time at risk is unknown. This will happen if we do not know when the individual first experienced the observed event. For example, when the individual is infected with a disease. | . Censoring concept slightly complicates the estimation of the survival function. Therefore, we need a different technique other than simple regression model. Thus, in addition to the target variable in the survival analysis, we need a categorical variable that indicates for each observation whether or not the event occurred at the time of censor. . Mathematical Concept . Probability Density Function (PDF) dan Cumulative Distribution Function (CDF) . Let $T$ be a positive continuous random variable which represents time until the event happens. . Probability density function (PDF) $f_T(t)$ represents the probability value of an event occurring at any time $t$ for the random variable $T$. | Cumulative distribution function (CDF) $F_T(t)$ represents a function that adds up the probability value of an event occurring up to time $t$ for the random variable $T$. | . Mathematically, $F_T(t) = P(T &lt; t) = int_0^t f_T(s) ds$ . # generate dummy data T = np.round(np.random.normal(loc=100, scale=10, size=10000)) t = np.linspace(T.min(), T.max(), dtype=&#39;int&#39;) kde = stats.gaussian_kde(T) df = pd.Series(kde.pdf(t), index=t, name=&#39;PDF&#39;).to_frame() df[&#39;CDF&#39;] = df[&#39;PDF&#39;].cumsum()/df[&#39;PDF&#39;].sum() df = df.round(10) # plotting fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True) ax[0].plot(df[&#39;PDF&#39;]) ax[0].set_xlabel(&quot;Time $t$&quot;) ax[0].set_ylabel(&quot;$f_T(t)$&quot;) ax[0].set_title(&quot;Probability Density Function&quot;) ax[1].plot(df[&#39;CDF&#39;]) ax[1].set_xlabel(&quot;Time $t$&quot;) ax[1].set_ylabel(&quot;$F_T(t)$&quot;) ax[1].set_title(&quot;Cumulative Distribution Function&quot;) plt.show() . . Survival Function . Survival function $S(t)$ defines the probability that the event has not occurred or the object can survive up to time $t$. Mathematically it can be written as $S(t) = P(T geq t)$, with the following properties: . $0 leq S(t) leq 1$ | $S(t) = 1 - F_T(t)$ where $F_T(t)$ is CDF for the random variable $T$ | $F_T(t)$ is a monotonically increasing function, so inversely $S(t)$ is a monotonically decreasing function | fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True) ax[0].plot(df[&#39;CDF&#39;]) ax[0].set_xlabel(&quot;Time $t$&quot;) ax[0].set_ylabel(&quot;$F_T(t)$&quot;) ax[0].set_title(&quot;Cumulative Distribution Function&quot;) df[&#39;Survival&#39;] = 1-df[&#39;CDF&#39;] ax[1].plot(df[&#39;Survival&#39;]) ax[1].set_xlabel(&quot;Time $t$&quot;) ax[1].set_ylabel(&quot;$S(t)$&quot;) ax[1].set_title(&quot;Survival Function&quot;) plt.show() . . Hazard Function . The hazard function $h(t)$ defines the conditional probability that the event will occur at the interval $[t, t+dt)$ given that the event is not occurred. Mathematically it can be written as: . $h(t) = displaystyle{ lim_{dt to 0} frac{P(t leq T &lt; t+dt | T geq t)}{dt}}$ . The above formula can be derived into (attached below this post): . $h(t) = dfrac{f_T(t)}{S(t)} = - dfrac{d}{dt} ln S(t)$ . So, survival function can be written in terms of hazard function as: $S(t) = e^{- int_0^t h(s) ds}$ . $H(t) = int_0^t h(s) ds$ is further defined as cumulative hazard function. . fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True) df[&#39;Hazard&#39;] = df[&#39;PDF&#39;]/df[&#39;Survival&#39;] ax[0].plot(df[&#39;Hazard&#39;]) ax[0].set_xlabel(&quot;Time $t$&quot;) ax[0].set_ylabel(&quot;$h(t)$&quot;) ax[0].set_title(&quot;Hazard Function&quot;) df[&#39;CumHazard&#39;] = df[&#39;Hazard&#39;].cumsum() ax[1].plot(df[&#39;CumHazard&#39;]) ax[1].set_xlabel(&quot;Time $t$&quot;) ax[1].set_ylabel(&quot;$H(t)$&quot;) ax[1].set_title(&quot;Cumulative Hazard Function&quot;) plt.show() . . Risk Score . However, the hazard function is rarely used in its original form. Usually, the $t$ axis is discretized into $J$ parts and the risk score $r(x)$ is calculated for each sample $x$: . $r(x) = displaystyle{ sum_{j=1}^J H(t_j, x)}$ . . Note: The faster an object experiences an event (smaller $t$), the higher the risk score it has. . J = 20 j = np.linspace(T.min(), T.max(), J, dtype=&#39;int&#39;) j[-1] -= 1 df2 = df.reindex(index=range(int(T.min()), int(T.max())), method=&#39;ffill&#39;) plt.plot(df[&#39;CumHazard&#39;], label=&#39;Cumulative Hazard&#39;) risk = 0 for j in j: y = df2.loc[j, &#39;CumHazard&#39;] plt.vlines(x=j, ymin=0, ymax=y, linestyles=&#39;--&#39;, color=&#39;red&#39;) plt.plot(j, y, marker=&#39;x&#39;, color=&#39;red&#39;) risk += y plt.legend() plt.xlabel(&quot;Time $t$&quot;) plt.ylabel(&quot;$H(t)$&quot;) plt.title(f&quot;Risk Score: {risk:.2f} with {J} partitions&quot;) plt.show() . . Predictive Maintenance . Predictive maintenance is a maintenance strategy to predict the chances of when a machine will be damaged, it can be used to assist technicians in making repairs as a preventive measure for damage to a machine. According to a survey conducted by Deloitte, predictive maintenance analysis can reduce factory equipment maintenance costs by up to 40%. . . Predictive maintenance is built based on an analysis of a machine equipment condition that is monitored in real-time. The monitoring process is assisted by sensors on the engine that can capture pressure, humidity, and temperature information on the engine, where some of these factors can be used to measure the performance of an engine. . Real-time data captured by several sensors is collected which is then stored in a cloud. The data can then be analyzed and visualized into a dashboard to be displayed to technicians. Up to this point, technicians only know the state of a machine. Then what about the predictive analysis process? . To perform predictive analysis, the existing data is further analyzed and a machine learning algorithm is applied, namely survival analysis which can produce a model. This model can be used as a decision parameter to determine the probability of damage to a machine over time. After the model is successfully created and produces the right results according to the business question, the maintenance specialist can determine the action plan to take preventive action on machine damage. . Data Loading . With the help of Internet of Things (IoT), maintenance dataset can be collected from sensors. There are several measurements such as pressure, moisture, and temperature. . from pysurvival.datasets import Dataset maintenance = Dataset(&#39;maintenance&#39;).load() maintenance.head(10) . lifetime broken pressureInd moistureInd temperatureInd team provider . 0 56 | 0 | 92.178854 | 104.230204 | 96.517159 | TeamA | Provider4 | . 1 81 | 1 | 72.075938 | 103.065701 | 87.271062 | TeamC | Provider4 | . 2 60 | 0 | 96.272254 | 77.801376 | 112.196170 | TeamA | Provider1 | . 3 86 | 1 | 94.406461 | 108.493608 | 72.025374 | TeamC | Provider2 | . 4 34 | 0 | 97.752899 | 99.413492 | 103.756271 | TeamB | Provider1 | . 5 30 | 0 | 87.678801 | 115.712262 | 89.792105 | TeamA | Provider1 | . 6 68 | 0 | 94.614174 | 85.702236 | 142.827001 | TeamB | Provider2 | . 7 65 | 1 | 96.483303 | 93.046797 | 98.316190 | TeamB | Provider3 | . 8 23 | 0 | 105.486158 | 118.291997 | 96.028822 | TeamB | Provider2 | . 9 81 | 1 | 99.178235 | 99.138717 | 95.492965 | TeamC | Provider4 | . Here are the data description of maintenance: . Time component . lifetime: machine uptime, in weeks | . | Event component . broken: indicated whether a machine has broken or not | . | Features . pressureInd: pressure index, measurement of the flow of liquid through a pipe. A sudden drop in pressure may indicate a leakage. | moistureInd: moisture index. Excessive humidity can cause mold and damage the equipment. | temperatureInd: temperature index from thermocouple. Improper temperature can cause electrical circuit damage, fire, or even explosion. | team: the team operating the machine | provider: machine manufacturer | . | maintenance[[&#39;broken&#39;, &#39;team&#39;, &#39;provider&#39;]] = maintenance[[&#39;broken&#39;, &#39;team&#39;, &#39;provider&#39;]].astype(&#39;category&#39;) maintenance.dtypes . lifetime int64 broken category pressureInd float64 moistureInd float64 temperatureInd float64 team category provider category dtype: object . Exploratory Data Analysis . Before doing the modeling, it would be better if we explore the maintenance data in the form of visualization. . Machine Record . First, let&#39;s see how much data has been recorded for each team and provider respectively. . maintenance.groupby([&#39;team&#39;, &#39;provider&#39;]).count()[&#39;broken&#39;].unstack().plot.bar() plt.xticks(rotation=0) plt.xlabel(&#39;&#39;) plt.ylabel(&#39;&#39;) plt.title(&#39;Number of Records by Team and Provider&#39;) plt.legend(bbox_to_anchor=(1, 1), loc=&#39;upper left&#39;, title=&#39;Provider&#39;) plt.show() . . The frequency of recorded machine is not much difference for each team and provider, in other words the distribution is quite uniform. Next, let&#39;s plot the distribution of pressureInd, moistureInd, and temperatureInd for both provider and broken. . maintenance_melt = maintenance.melt(id_vars=[&#39;provider&#39;, &#39;broken&#39;], value_vars=[&#39;pressureInd&#39;, &#39;moistureInd&#39;, &#39;temperatureInd&#39;]) g = sns.FacetGrid(maintenance_melt, row=&#39;variable&#39;, col=&#39;provider&#39;, hue=&#39;broken&#39;, margin_titles=True) g.map(sns.distplot, &#39;value&#39;, hist_kws=dict(alpha=0.2)) g.add_legend() plt.show() . . At a glance, it can be concluded that the bell curve plot shows that the three variables are normally distributed, and the distribution is more or less similar for broken and not broken. We can perform hypothesis testing to test whether the two distributions are statistically equal or not. . Mann-Whitney U: Tests whether the distribution of two independent samples is equal or not. . Assumptions: . Observations in each sample are independent and identically distributed. | Observations in each sample can be ranked. | . Hypothesis: . $H_0$: distribution of both samples is the same. | $H_1$: distribution of the two samples is not the same. | . from scipy.stats import mannwhitneyu H0 = &quot;same&quot; H1 = &quot;different&quot; alpha = 0.05 ks_result = [] for var in [&#39;pressureInd&#39;, &#39;moistureInd&#39;, &#39;temperatureInd&#39;]: for provider in maintenance[&#39;provider&#39;].cat.categories: group = maintenance[&#39;provider&#39;] == provider broken = maintenance[(maintenance[&#39;broken&#39;] == 0) &amp; (group)] not_broken = maintenance[(maintenance[&#39;broken&#39;] == 1) &amp; (group)] stat, p = mannwhitneyu(broken[var], not_broken[var]) ks_result.append( { &#39;provider&#39;: provider, &#39;variable&#39;: var, &#39;statistic&#39;: stat, &#39;pvalue&#39;: p, f&#39;result (alpha={alpha})&#39;: H1 if p &lt; alpha else H0 } ) pd.DataFrame(ks_result) . . provider variable statistic pvalue result (alpha=0.05) . 0 Provider1 | pressureInd | 7770.0 | 0.344450 | same | . 1 Provider2 | pressureInd | 7492.0 | 0.214888 | same | . 2 Provider3 | pressureInd | 6774.0 | 0.168687 | same | . 3 Provider4 | pressureInd | 5090.0 | 0.015711 | different | . 4 Provider1 | moistureInd | 7835.0 | 0.386328 | same | . 5 Provider2 | moistureInd | 7357.0 | 0.154727 | same | . 6 Provider3 | moistureInd | 7186.0 | 0.420178 | same | . 7 Provider4 | moistureInd | 6005.0 | 0.380593 | same | . 8 Provider1 | temperatureInd | 7484.0 | 0.186542 | same | . 9 Provider2 | temperatureInd | 7608.0 | 0.276022 | same | . 10 Provider3 | temperatureInd | 6919.0 | 0.244274 | same | . 11 Provider4 | temperatureInd | 5596.0 | 0.129266 | same | . Using alpha = 5%, the majority of distributions are the same except for pressureInd from Provider4. This indicates that it means that we cannot rely solely on numerical variables and providers in the modeling stage. We must also involve the lifetime time component when we want to predict the broken state of a machine. . Correlation Heatmap . Here, we are interested in looking at the correlations between numeric variables in the maintenance data. From the correlation coefficient, it turns out that there is no strong correlation which indicates that there is no linear relationship between all numerical variables. . sns.heatmap(maintenance.corr(), annot=True, cmap=&#39;Reds&#39;) plt.title(&quot;Correlation Heatmap&quot;) plt.show() . . Machine Status (Time to Event) . Let&#39;s explore the lifetime (time component) to observe when the machine breaks down. . sns.histplot(x=&#39;lifetime&#39;, hue=&#39;broken&#39;, bins=25, alpha=0.2, data=maintenance) plt.title(&quot;Number of Record by Machine Status&quot;) plt.show() . . It turns out that the failure begins when the machine has been active for at least 60 weeks. Then, what if we plot the above in detail for each team and provider? . def multihist(x, hue, n_bins=10, color=None, **kws): bins = np.linspace(x.min(), x.max(), n_bins) for label, x_i in x.groupby(hue): plt.hist(x_i, bins, label=label, **kws) g = sns.FacetGrid(maintenance, col=&#39;provider&#39;, margin_titles=True) g.map(multihist, &#39;lifetime&#39;, &#39;broken&#39;, alpha=0.5) g.add_legend(title=&#39;broken&#39;) plt.show() . . We can clearly see the initial time when the machine failure occurred for each provider. We can sort the provider from the smallest lifetime: Provider3, Provider1, Provider4, followed by Provider2. . g = sns.FacetGrid(maintenance, col=&#39;team&#39;, margin_titles=True) g.map(multihist, &#39;lifetime&#39;, &#39;broken&#39;, alpha=0.5) g.add_legend(title=&#39;broken&#39;) plt.show() . . On the other hand, for each team, it can be seen that the initial machine failure time for team C is faster than for the other two. To be more confident with the two visualizations above, let&#39;s test the hypothesis as follows: . Chi-Squared: Tests whether two categorical variables are related or independent. . Assumptions: . Observations used in the calculation of the contingency table are independent. | 25 or more records in each value in the contingency table. | . Hypothesis: . $H_0$: two samples are independent of each other. | $H_1$: two samples are mutually dependent. | . from scipy.stats import chi2_contingency H0 = &quot;independent with broken&quot; H1 = &quot;dependent with broken&quot; alpha = 0.05 chi2_result = [] for var in [&#39;provider&#39;, &#39;team&#39;]: table = pd.pivot_table(data=maintenance, index=&#39;broken&#39;, columns=var, values=&#39;lifetime&#39;, aggfunc=&#39;count&#39;) stat, p, dof, expected = chi2_contingency(table) chi2_result.append( { &#39;variable&#39;: var, &#39;statistic&#39;: stat, &#39;pvalue&#39;: p, f&#39;result (alpha={alpha})&#39;: H1 if p &lt; alpha else H0 } ) pd.DataFrame(chi2_result) . . variable statistic pvalue result (alpha=0.05) . 0 provider | 18.673817 | 0.000319 | dependent with broken | . 1 team | 2.264541 | 0.322301 | independent with broken | . Using alpha = 5%, we can conclude that the provider is dependent on the broken status of the machine, while the team is independent. . Kaplan-Meier . Kaplan-Meier is the simplest method for estimating survival function for each category in a population. Calculation of estimates in the Kaplan-Meier method involves the probability of an event occurring up to a certain time, then successively multiplied by the previous probability to produce the final estimate. Mathematically can be written as follows: . $S(t) = displaystyle{ prod_{j=1}^k frac{n_j - d_j}{d_j}}$ . where $n_j$ represents the number of individuals at time $t$ and $d_j$ is the number of individuals experiencing the event at time $t$. . from lifelines import KaplanMeierFitter kmf = KaplanMeierFitter() for provider in maintenance[&#39;provider&#39;].cat.categories: group = maintenance[&#39;provider&#39;] == provider kmf.fit(maintenance[&#39;lifetime&#39;][group], maintenance[&#39;broken&#39;][group], label=provider) kmf.plot(ci_show=False) plt.xlabel(&#39;Lifetime&#39;) plt.ylabel(&#39;Survival Probability&#39;) plt.title(&#39;Survival Function by Provider&#39;) plt.show() . . The plot above is in line with our previous findings where the fastest broken machine was from Provider3 and the slowest was Provider2. We can test the hypothesis to be more sure whether each provider has a different survival function. . Log-Rank: Tests whether two processes are of different intensity. That is, given two sequences of events, test whether the processes producing data differ statistically. . Hypothesis: . $H_0$: two survival functions are the same. | $H_1$: two different survival functions. | . from lifelines.statistics import pairwise_logrank_test def logrank_test(cat, alpha): H0 = &quot;same&quot; H1 = &quot;different&quot; lr_test = pairwise_logrank_test(event_durations=maintenance[&#39;lifetime&#39;], groups=maintenance[cat], event_observed=maintenance[&#39;broken&#39;]).summary lr_test.drop(&#39;-log2(p)&#39;, axis=1, inplace=True) lr_test[f&#39;result (alpha={alpha})&#39;] = lr_test[&#39;p&#39;].apply( lambda x: H1 if x &lt; alpha else H0) lr_test.index.set_names([&#39;First&#39;, &#39;Second&#39;], inplace=True) return lr_test.reset_index() logrank_test(&#39;provider&#39;, alpha=0.05) . . First Second test_statistic p result (alpha=0.05) . 0 Provider1 | Provider2 | 204.519279 | 2.156144e-46 | different | . 1 Provider1 | Provider3 | 259.180962 | 2.588498e-58 | different | . 2 Provider1 | Provider4 | 191.174684 | 1.761680e-43 | different | . 3 Provider2 | Provider3 | 251.730255 | 1.089516e-56 | different | . 4 Provider2 | Provider4 | 66.552210 | 3.407479e-16 | different | . 5 Provider3 | Provider4 | 230.643017 | 4.316305e-52 | different | . It truns out that all survival functions were statistically significant. We can tell the maintenance team to pay more attention to the machines from Provider2, as they require early maintenance to prevent unexpected failures. . kmf = KaplanMeierFitter() for team in maintenance[&#39;team&#39;].cat.categories: group = maintenance[&#39;team&#39;] == team kmf.fit(maintenance[&#39;lifetime&#39;][group], maintenance[&#39;broken&#39;][group], label=team) kmf.plot(ci_show=False) plt.xlabel(&#39;Lifetime&#39;) plt.ylabel(&#39;Survival Probability&#39;) plt.title(&#39;Survival Function by Team&#39;) plt.legend(loc=&#39;lower left&#39;) plt.show() . . The plot above is also in line with the previous finding where the survival function of the machine operated by team C is different from that of A and B. Let&#39;s do a log-rank test for the above visualization: . logrank_test(&#39;team&#39;, alpha=0.05) . First Second test_statistic p result (alpha=0.05) . 0 TeamA | TeamB | 0.054725 | 8.150366e-01 | same | . 1 TeamA | TeamC | 34.698639 | 3.849005e-09 | different | . 2 TeamB | TeamC | 34.640870 | 3.964939e-09 | different | . It turns out that the survival function of the machine operated by TeamC is very different from that of TeamA and TeamB. The maintenance team can monitor whether the TeamC has operated the machine correctly or not, then can conduct training on machine operation to prevent premature machine failure. . Data Preprocessing . At this section, we will prepare the data before modeling as follows: . One-hot encoding for categorical features | Cross-validation and train-test splitting | Split data into three components: feature, time, event | . Perform one-hot encoding for provider and team with pd.get_dummies(). . categories = [&#39;provider&#39;, &#39;team&#39;] maintenance_dummy = pd.get_dummies(maintenance, columns=categories, drop_first=True) maintenance_dummy.head() . lifetime broken pressureInd moistureInd temperatureInd provider_Provider2 provider_Provider3 provider_Provider4 team_TeamB team_TeamC . 0 56 | 0 | 92.178854 | 104.230204 | 96.517159 | 0 | 0 | 1 | 0 | 0 | . 1 81 | 1 | 72.075938 | 103.065701 | 87.271062 | 0 | 0 | 1 | 0 | 1 | . 2 60 | 0 | 96.272254 | 77.801376 | 112.196170 | 0 | 0 | 0 | 0 | 0 | . 3 86 | 1 | 94.406461 | 108.493608 | 72.025374 | 1 | 0 | 0 | 0 | 1 | . 4 34 | 0 | 97.752899 | 99.413492 | 103.756271 | 0 | 0 | 0 | 1 | 0 | . Train-test splitting with 80:20 proportion . from sklearn.model_selection import train_test_split index_train, index_test = train_test_split(range(maintenance_dummy.shape[0]), test_size=0.2, random_state=SEED) data_train = maintenance_dummy.loc[index_train].reset_index(drop=True) data_test = maintenance_dummy.loc[index_test].reset_index(drop=True) . Different from the typical supervised machine learning model, which only divides the data into X features and y targets. In survival analysis we divide the data into three components: . X feature contains numeric and categorical predictor columns | Time T contains the column when event E occurred | Event E contains the class of events, in this case whether the machine is broken or not | features = np.setdiff1d(maintenance_dummy.columns, [&#39;lifetime&#39;, &#39;broken&#39;]).tolist() X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[&#39;lifetime&#39;].values, data_test[&#39;lifetime&#39;].values E_train, E_test = np.array(data_train[&#39;broken&#39;].values), np.array(data_test[&#39;broken&#39;].values) . Survival Analysis Model . Cox Proportional Hazard (CoxPH) . The CoxPH model is widely used in statistics for multivariate survival functions because of its relatively easy implementation and high interpretability. CoxPH describes the relationship between the distribution of survival functions and covariates. The predictor variables are expressed by the hazard function as follows: . $$ lambda(t|x) = lambda_0(t) e^{ beta_1x_1 + ... + beta_nx_n}$$ . The CoxPH method is a semi-parametric model because it consists of 2 components: . The non-parametric component $ lambda_0(t)$ which is referred to as the baseline hazard, that is, the hazard when all covariates are zero. | The parametric component $e^{ beta_1x_1 + ... + beta_nx_n}$ is called a time-independent partial hazard. | In general, the Cox model estimates the log-risk function $ lambda(t|x)$ as a linear combination of static covariates and baseline hazard. | . The following is an implementation of the CoxPH model in survival analysis: . from pysurvival.models.semi_parametric import CoxPHModel coxph = CoxPHModel() coxph.fit(X_train, T_train, E_train, lr=0.5, l2_reg=1e-2, init_method=&#39;zeros&#39;) . Performing Newton-Raphson optimization * Iteration #1 - Loss = 1588.772 - ||grad||^2 = 319.12143 * Iteration #2 - Loss = 1260.426 - ||grad||^2 = 164.46688 * Iteration #3 - Loss = 1135.155 - ||grad||^2 = 99.67192 * Iteration #4 - Loss = 1053.192 - ||grad||^2 = 61.15221 * Iteration #5 - Loss = 996.584 - ||grad||^2 = 37.31715 * Iteration #6 - Loss = 959.229 - ||grad||^2 = 22.80550 * Iteration #7 - Loss = 935.345 - ||grad||^2 = 14.12548 * Iteration #8 - Loss = 920.224 - ||grad||^2 = 8.92736 * Iteration #9 - Loss = 910.730 - ||grad||^2 = 5.73887 * Iteration #10 - Loss = 904.914 - ||grad||^2 = 3.71561 * Iteration #11 - Loss = 901.465 - ||grad||^2 = 2.40797 * Iteration #12 - Loss = 899.464 - ||grad||^2 = 1.55605 * Iteration #13 - Loss = 898.312 - ||grad||^2 = 0.99669 * Iteration #14 - Loss = 897.650 - ||grad||^2 = 0.62547 * Iteration #15 - Loss = 897.272 - ||grad||^2 = 0.37932 * Iteration #16 - Loss = 897.059 - ||grad||^2 = 0.22022 * Iteration #17 - Loss = 896.941 - ||grad||^2 = 0.12224 * Iteration #18 - Loss = 896.878 - ||grad||^2 = 0.06530 * Iteration #19 - Loss = 896.845 - ||grad||^2 = 0.03393 * Iteration #20 - Loss = 896.828 - ||grad||^2 = 0.01732 * Iteration #21 - Loss = 896.819 - ||grad||^2 = 0.00876 * Iteration #22 - Loss = 896.815 - ||grad||^2 = 0.00440 * Iteration #23 - Loss = 896.812 - ||grad||^2 = 0.00221 * Iteration #24 - Loss = 896.811 - ||grad||^2 = 0.00111 * Iteration #25 - Loss = 896.811 - ||grad||^2 = 0.00055 Converged after 25 iterations. . CoxPHModel . Model Interpretation . The coef value in the summary model can be interpreted just like a linear regression model. A positive coefficient increases the baseline hazard value $ lambda_0(t)$ and indicates that the predictor increases the risk of the event occurring, in this case broken. Conversely, a negative coefficient will reduce the risk of an event occurring if the value is increased. . H0 = &quot;not significant&quot; H1 = &quot;significant&quot; alpha = 0.05 coxph_summary = coxph.get_summary() coxph_summary[f&#39;result (alpha={alpha})&#39;] = coxph_summary[&#39;p_values&#39;].astype(&#39;float64&#39;).apply(lambda x: H1 if x &lt; alpha else H0) coxph_summary[[&#39;variables&#39;, &#39;coef&#39;, &#39;p_values&#39;, f&#39;result (alpha={alpha})&#39;]] . . variables coef p_values result (alpha=0.05) . 0 moistureInd | -0.158 | 0.0 | significant | . 1 pressureInd | 0.024 | 0.0 | significant | . 2 provider_Provider2 | -13.342 | 0.015 | significant | . 3 provider_Provider3 | 10.077 | 0.072 | not significant | . 4 provider_Provider4 | -9.616 | 0.024 | significant | . 5 team_TeamB | 0.044 | 0.748 | not significant | . 6 team_TeamC | 6.957 | 0.023 | significant | . 7 temperatureInd | 0.502 | 0.0 | significant | . The assumption in the CoxPH model is proportionality assumption, the hazard function for two objects will always be proportional at the same time and the ratio does not change from the beginning to the end of time. For example: if machine A has a broken risk of 2x compared to machine B, then for the next time the risk ratio remains 2x. . Properties of CoxPH model: . Independence of observation: The time of occurrence of an event on one object will be independent of other objects. | No hazard curves that intersect each other. | There is a linear multiplication effect of the estimated covariate value on the hazard function. | . Performance Metric . We can evaluate survival analysis model using two metrics, namely Concordance Idex (C-index) and Integrated Brier Score (IBS). . C-index indicates the model&#39;s ability to correctly rank survival time based on individual risk scores. C-index is a generalization of the AUC value. The closer the C-index is to the value of one, the better the model predicts, whereas when the value is 0.5, it represents a random prediction. . | IBS measures the average difference between event labels and predicted survival probabilities. As a benchmark, a good model will have a Brier score below 0.25. IBS values ​​always have a range between 0-1, with 0 being the best value. . | from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score, compare_to_actual def evaluate(model, X, T, E, model_name=&quot;&quot;): errors = compare_to_actual(model, X, T, E, is_at_risk=False) c_index = concordance_index(model, X, T, E) ibs = integrated_brier_score(model, X, T, E) metrics = {&#39;C-index&#39;: c_index, &#39;IBS&#39;: ibs} eval_df = pd.DataFrame(data={**metrics, **errors}, index=[model_name]) return eval_df.rename(columns={&#39;root_mean_squared_error&#39;: &#39;RMSE&#39;, &#39;median_absolute_error&#39;: &#39;MADE&#39;, &#39;mean_absolute_error&#39;: &#39;MAE&#39;}) . . eval_coxph = evaluate(coxph, X_test, T_test, E_test, model_name=&quot;Cox PH&quot;) eval_coxph . C-index IBS RMSE MADE MAE . Cox PH 0.962313 | 0.020073 | 22.858351 | 19.684227 | 18.125846 | . Multi Task Logistic Regression Models . The Multi Task Logistic Regression (MTLR) model was first introduced by Chun-Nam Yu in 2011 to predict survival time in cancer patients (survival analysis). This model is an alternative to the Cox Proportional Hazard model which has several shortcomings, one of which is that the Cox model works using a hazard function rather than a survival function, so the model is less able to provide accurate predictions to analyze life expectancy. . MTLR has two approaches for its implementation of survival analysis modeling. The first model is Linear-Multi Task Logistic Regression and the second model is Neural-Multi Task Logistic Regression. The following is an implementation of the two models: . Linear-Multi Task Logistic Regression . Linear-Multi Task Logistic Regression is a set of logistic regression models built at different time intervals to estimate the probability of events occurring in that time span. . Several stages for the Linear MTLR process can be defined as follows: . Divide the time variable into certain intervals | Build a logistic regression model at each time interval | Calculating the loss function to determine the optimum model | At this modeling stage, we create a MLTR model with the number of bins logistic regression as many as 100 models. We used adamax optimizer and 50 epochs to train the logistic regression. . from pysurvival.models.multi_task import LinearMultiTaskModel l_mtlr = LinearMultiTaskModel(bins=100) l_mtlr.fit(X_train, T_train, E_train, lr=1e-3, l2_reg=1e-6, l2_smooth=1e-6, init_method=&#39;orthogonal&#39;, optimizer=&#39;adamax&#39;, num_epochs=50) . . LinearMultiTaskModel . eval_l_mtlr = evaluate(l_mtlr, X_test, T_test, E_test, model_name = &quot;Linear MTLR&quot;) eval_l_mtlr . C-index IBS RMSE MADE MAE . Linear MTLR 0.938971 | 0.044073 | 5.185306 | 0.711647 | 2.276439 | . The result above shows that the Linear MTLR model has a value of C-index = 0.938971 and a value of IBS = 0.044073. When the C-index is getting closer to 1 and the IBS is getting closer to 0, then the model can be said to have very good prediction results. . Neural-Multi Task Logistic Regression . Neural-Multi Task Logistic Regression (N-MTLR) is a model that was developed based on the previous MTLR model. The previous two models (CoxPH and regular MTLR) failed to capture non-linear patterns from the data and consequently can&#39;t satisfy certain cases. This model is supported by a deep learning architecture and can overcome the shortcomings of the previous model. . In the N-MTLR model, there are two improvements from the previous MTLR model: . N-MTLR uses a deep learning framework through multi-layer perceptron (MLP). By replacing the linear core, this model is more flexible because it will not rely on assumptions like the CoxPH model. . | The model is implemented in Python using the open-source TensorFlow and Keras allowing users to combine many techniques in deep learning such as: . | Initialization: Xavier uniform, Xavier gaussian, etc | Optimization: Adam, RMSprop, etc | Activation function: SeLU, ReLU, Softplus, tanh, etc | Miscellaneous Operation: Batch Normalization, Dropout, etc | . from pysurvival.models.multi_task import NeuralMultiTaskModel # simple structure structure = [{&#39;activation&#39;: &#39;ReLU&#39;, &#39;num_units&#39;: 100}, ] # fitting the model n_mtlr = NeuralMultiTaskModel(structure=structure, bins=100) n_mtlr.fit(X_train, T_train, E_train, lr=1e-3, num_epochs=500, l2_reg=1e-6, l2_smooth=1e-6, init_method=&#39;orthogonal&#39;, optimizer=&#39;rprop&#39;) . . NeuralMultiTaskModel( Layer(1): activation = ReLU, units = 100 ) . eval_mtlr_1 = evaluate(n_mtlr, X_test, T_test, E_test, model_name = &quot;Neural MTLR 1-hidden layer&quot;) eval_mtlr_1 . C-index IBS RMSE MADE MAE . Neural MTLR 1-hidden layer 0.853441 | 0.003763 | 2.275712 | 3.622909e-20 | 0.871271 | . We can also inspect the loss values for each epoch using display_loss_values function: . from pysurvival.utils.display import display_loss_values display_loss_values(n_mtlr, figure_size=(7, 4)) . Model Comparison . Up to this stage, we have built 3 different models, namely the CoxPH, L-MTLR, and N-MTLR. The performance of each of these models can be described in the table below. . eval_all = pd.concat([eval_coxph, eval_l_mtlr, eval_mtlr_1]) eval_all . . C-index IBS RMSE MADE MAE . Cox PH 0.962313 | 0.020073 | 22.858351 | 1.968423e+01 | 18.125846 | . Linear MTLR 0.938971 | 0.044073 | 5.185306 | 7.116467e-01 | 2.276439 | . Neural MTLR 1-hidden layer 0.853441 | 0.003763 | 2.275712 | 3.622909e-20 | 0.871271 | . Referring to the C-index value, we can see that the CoxPH model is the best performance compared to other models, which is 0.96. However, from the error, it turns out that this model actually has the highest RMSE value compared to other models. So we can say that the CoxPH model is not very good at predicting because this high bias could indicate an overfitting in the model. . Just like the CoxPH model, the L-MTLR model also has a fairly good C-index performance but the error generated is still larger than the N-MTLR model. So in this case, we use the N-MTLR model as the best model because in terms of the C-Index value which is quite good and have the smallest error compared to other models. . best_model = n_mtlr . Results . Risk Score . At the model comparison, we conclude that N-MTLR is the best model. In this section, we&#39;ll use this model for individual prediction and grouping of individuals based on their risk score. . The first step, we will calculate the risk score of each machine. The value of this risk score will later be used for grouping, both for the score distribution or other grouping methods. . risk_profile = X_test.copy() risk_profile[&#39;risk_score&#39;] = best_model.predict_risk(risk_profile, use_log=True) risk_profile.head() . moistureInd pressureInd provider_Provider2 provider_Provider3 provider_Provider4 team_TeamB team_TeamC temperatureInd risk_score . 0 102.846778 | 102.709720 | 0 | 0 | 0 | 0 | 0 | 108.324299 | 5.858471 | . 1 102.892983 | 96.436456 | 0 | 1 | 0 | 0 | 1 | 114.853392 | 7.807072 | . 2 106.214236 | 68.850651 | 0 | 0 | 0 | 1 | 0 | 115.711970 | 6.776986 | . 3 104.742757 | 112.893971 | 0 | 1 | 0 | 0 | 0 | 107.360631 | 7.345346 | . 4 96.371860 | 75.997301 | 1 | 0 | 0 | 1 | 0 | 98.934087 | 6.183013 | . We group the risk_score using 1 dimensional K-Means. From the grouping results, we obtained 3 clusters, namely low, medium, and high. Since the C-index is quite high and the model error is low, the model can be said to be quite good in determining the survival time ranking of random individuals in each group, so it is found that $t_{high}&lt;t_{medium}&lt;t_{low}$ . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3, random_state=SEED).fit(risk_profile[[&#39;risk_score&#39;]]) risk_profile[&#39;risk_group&#39;] = kmeans.labels_ risk_group_bound = risk_profile.groupby(&#39;risk_group&#39;)[&#39;risk_score&#39;].min().sort_values().to_frame() risk_group_bound.index = [&#39;low&#39;, &#39;medium&#39;, &#39;high&#39;] risk_group_bound.columns = [&#39;lower_bound&#39;] risk_group_bound[&#39;upper_bound&#39;] = risk_group_bound[&#39;lower_bound&#39;].shift(periods=-1).fillna(risk_profile[&#39;risk_score&#39;].max()) risk_group_bound[&#39;color&#39;] = [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;] risk_group_bound . . lower_bound upper_bound color . low 5.373734 | 6.090510 | red | . medium 6.090510 | 7.324216 | green | . high 7.324216 | 8.157679 | blue | . We can illustrate the risk group using the following histogram: . from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups( model=best_model, X=X_test, num_bins=50, **risk_group_bound.T.to_dict()) . Predict Survival Function . Here is the survival function for all records with broken status on X_test for each risk group: . fig, axes = plt.subplots(3, 1, figsize=(15, 10)) for i, (ax, (label, (color, idxs))) in enumerate(zip(axes.flat, risk_groups.items())): X = X_test.values[idxs, :] T = T_test[idxs] E = E_test[idxs] broken = np.argwhere((E == 1)).flatten() for j in broken: survival = best_model.predict_survival(X[j, :]).flatten() ax.plot(best_model.times, survival, color=color) ax.set_title(f&quot;{label.title()} Risk&quot;) plt.show() . . Instead of looking for all survival function, let&#39;s take one random observation from each group that has experienced failure (event = 1). . plt.figure(figsize=(15, 5)) for i, (label, (color, idxs)) in enumerate(risk_groups.items()): record_idx = X_test.iloc[idxs, :].index X = X_test.values[idxs, :] T = T_test[idxs] E = E_test[idxs] # choose a machine at random that has experienced failure choices = np.argwhere((E == 1)).flatten() k = np.random.choice(choices, 1)[0] # predict survival function survival = best_model.predict_survival(X[k, :]).flatten() plt.plot(best_model.times, survival, color=color, label=f&#39;Record {record_idx[k]} ({label} risk)&#39;) # actual failure time actual_t = T[k] plt.axvline(x=actual_t, color=color, ls=&#39;--&#39;) plt.annotate(f&#39;T={actual_t:.1f}&#39;, xy=(actual_t, 0.5*(1+0.2*i)), xytext=(actual_t, 0.5*(1+0.2*i)), fontsize=12) plt.title(&quot;Survival Functions Comparison between High, Medium, Low Risk Machine&quot;) plt.legend() plt.show() . . The plot above shows the survival function for the three risk groups by taking 1 random machine record. As can be seen, the model successfully predicts the broken event. There is a sudden decrease in the survival value, according to the vertical dotted line, which is the real broken time. . Optional: Mathematical proof for hazard function . It is defined that $h(t) = displaystyle{ lim_{dt to 0} frac{P(t leq T &lt; t+dt | T geq t)}{dt}}$ . According to the conditional probability rule $P(A|B) = dfrac{P(A cap B)}{P(B)}$ then: . $h(t) = displaystyle{ lim_{dt to 0} frac{P( {t leq T &lt; t+dt } cap {T geq t })}{P(T geq t) dt}}$ . Since $T geq t$ is a subset of the interval of $t leq T &lt; t+dt$ then $P( {t leq T &lt; t+dt } cap {T geq t }) = P(t leq T &lt; t+dt)$ | Definition of CDF: $P(t leq T &lt; t+dt) = F_T(t+dt) - F_T(t)$ | Definition of survival function: $P(T geq t) = S(t)$ | . From the three definitions above: $h(t) = dfrac{1}{S(t)} displaystyle{ lim_{dt to 0} frac{F_T(t+dt) - F_T(t )}{dt}}$ . According to the definition of the derived function: $h(t) = dfrac{1}{S(t)} dfrac{d}{dt}F_T(t)$ . Using the relationship between PDF and CDF we get the first equation: $h(t) = dfrac{f_T(t)}{S(t)}$ . Using the CDF relationship and the survival function: $h(t) = dfrac{1}{S(t)} dfrac{d}{dt}[1 - S(t)] = - dfrac{S&#39;( t)}{S(t)}$ . According to the derivative of the logarithmic function $ dfrac{d}{dx} ln f(x) = dfrac{f&#39;(x)}{f(x)}$, then we get the second equation: $h( t) = - dfrac{d}{dt} ln S(t)$ . References . PySurvival package | Lifelines: Introduction to Survival Analysis | Deep Learning for Survival Analysis. Laura Löschmann, Daria Smorodina. (2020) | Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework. Fotso, S. (2018). arXiv:1801.05512. | Multi-Task Logistic Regression (MTLR) model created by Yu, Chun-Nam, et al. in 2011 | .",
            "url": "https://tomytjandra.github.io/blogs/python/survival-analysis/pysurvival/2020/11/27/predictive-maintenance-pysurvival.html",
            "relUrl": "/python/survival-analysis/pysurvival/2020/11/27/predictive-maintenance-pysurvival.html",
            "date": " • Nov 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Breast Cancer Wisconsin Classification",
            "content": "Load Libraries . import pandas as pd import numpy as np # data visualization import matplotlib.pyplot as plt import seaborn as sns import graphviz # modeling from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier, export_graphviz from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import recall_score, precision_score, f1_score, classification_report from sklearn.model_selection import GridSearchCV # other from IPython.display import display, Math, HTML # setting plt.style.use(&#39;seaborn&#39;) pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.5f&#39; % x) pd.set_option(&#39;display.max_colwidth&#39;, None) pd.options.plotting.backend = &quot;plotly&quot; . Data Loading . The cancer dataset used is provided from Kaggle. . cancer = pd.read_csv(&quot;data_input/data.csv&quot;) cancer.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): # Column Non-Null Count Dtype -- -- 0 id 569 non-null int64 1 diagnosis 569 non-null object 2 radius_mean 569 non-null float64 3 texture_mean 569 non-null float64 4 perimeter_mean 569 non-null float64 5 area_mean 569 non-null float64 6 smoothness_mean 569 non-null float64 7 compactness_mean 569 non-null float64 8 concavity_mean 569 non-null float64 9 concave points_mean 569 non-null float64 10 symmetry_mean 569 non-null float64 11 fractal_dimension_mean 569 non-null float64 12 radius_se 569 non-null float64 13 texture_se 569 non-null float64 14 perimeter_se 569 non-null float64 15 area_se 569 non-null float64 16 smoothness_se 569 non-null float64 17 compactness_se 569 non-null float64 18 concavity_se 569 non-null float64 19 concave points_se 569 non-null float64 20 symmetry_se 569 non-null float64 21 fractal_dimension_se 569 non-null float64 22 radius_worst 569 non-null float64 23 texture_worst 569 non-null float64 24 perimeter_worst 569 non-null float64 25 area_worst 569 non-null float64 26 smoothness_worst 569 non-null float64 27 compactness_worst 569 non-null float64 28 concavity_worst 569 non-null float64 29 concave points_worst 569 non-null float64 30 symmetry_worst 569 non-null float64 31 fractal_dimension_worst 569 non-null float64 32 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB . There are 569 observations and 33 columns with the following description: . id: ID number | diagnosis: M = Malignant, B = Benign (target variable) | . Ten real-valued features are computed for each cell nucleus: . radius: mean of distances from center to points on the perimeter | texture: standard deviation of gray-scale values | perimeter | area | smoothness: local variation in radius lengths | compactness: perimeter^2 / area - 1.0 | concavity: severity of concave portions of the contour | concave points: number of concave portions of the contour | symmetry | fractal dimension: &quot;coastline approximation&quot; - 1 | . The feature above is calculated based on their mean, se (standard error), and worst (largest). . Drop Columns . There is no missing value in the data, except for the empty column Unnamed: 32. . missing = cancer.isnull().sum() missing[missing&gt;0] . Unnamed: 32 569 dtype: int64 . We can remove the id column because it is not used as a feature, and the Unnamed: 32 column because it is an empty column. . cancer.drop([&#39;id&#39;, &#39;Unnamed: 32&#39;], axis=1, inplace=True) . Data Visualization . Target Proportion . Target variable diagnosis: 212 Malignant and 357 Benign observations. In this case, we assume the proportion is quite balanced. . sns.countplot(x=&#39;diagnosis&#39;, data=cancer); . . Correlation . From the correlation heatmap, we gain insights that there are some measurements that are actually calculations from other columns. Suppose radius, parameter, and area are related to each other. . plt.figure(figsize=(15, 15)) sns.heatmap(cancer.corr(), annot=True, linewidths=0.5, fmt=&#39;.2f&#39;); . . Distribution . cancer_longer = cancer.melt(id_vars=&#39;diagnosis&#39;, var_name=&#39;features&#39;) g = sns.FacetGrid(data=cancer_longer, col=&#39;features&#39;, col_wrap=10, sharey=False) g.map_dataframe(sns.boxplot, x=&#39;diagnosis&#39;, y=&#39;value&#39;, hue=&#39;diagnosis&#39;) g.add_legend() plt.show() . . median = cancer_longer.groupby([&#39;diagnosis&#39;, &#39;features&#39;]).quantile(0.5)[&#39;value&#39;].unstack(level=&#39;diagnosis&#39;) median[&#39;Median Difference&#39;] = median[&#39;M&#39;] - median[&#39;B&#39;] median = median.sort_values(&#39;Median Difference&#39;) pd.concat([median.head(3), median.tail(3)]) . . diagnosis B M Median Difference . features . texture_se 1.10800 | 1.10250 | -0.00550 | . symmetry_se 0.01909 | 0.01770 | -0.00139 | . smoothness_se 0.00653 | 0.00621 | -0.00032 | . perimeter_worst 86.92000 | 138.00000 | 51.08000 | . area_mean 458.40000 | 932.00000 | 473.60000 | . area_worst 547.40000 | 1303.00000 | 755.60000 | . The table above shows the median value of each features for Benign and Malignant cancer. The median difference is calculated by subtracting median of Benign from Malignant. In general, the median value for majority of the features is greater for Malignant than Benign except for texture_se, symmetry_se, and smoothness_se. The most obvious feature is area, because the median value of Malignant is more than twice the median value of Benign. . Data Preprocessing . We have to encode the target variable diagnosis using LabelEncoder. Encoding is not needed for the features since all value are numeric. . X = cancer.drop(columns=[&#39;diagnosis&#39;]) le = LabelEncoder() y = le.fit_transform(cancer[&#39;diagnosis&#39;]) . le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_))) le_name_mapping . {&#39;B&#39;: 0, &#39;M&#39;: 1} . . Note: Using LabelEncoder, the target value Benign is mapped into 0, while Malignant is mapped into 1 . Train-test splitting with 75:25 proportion. . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) print(f&quot;X_train shape: {X_train.shape}&quot;) print(f&quot;X_test shape: {X_test.shape}&quot;) print(f&quot;y_train shape: {y_train.shape}&quot;) print(f&quot;y_test shape: {y_test.shape}&quot;) . X_train shape: (426, 30) X_test shape: (143, 30) y_train shape: (426,) y_test shape: (143,) . Data Modeling . We are going to compare four binary classification models: logistic regression, support vector, decision tree, and random forest classfier. The positive target class is Malignant (1), while the negative is Benign (0). . We are going to consider the following evaluation metrics: . Recall: what percentage of patients with Malignant can the model detect? | Precision: of the patients predicted to be Malignant, what percentage were truly Malignant? | F1 score: weighted average of Recall and Precision. | . $F1 = 2 times frac{Recall times Precision}{Recall+Precision}$ . Types of errors: . False Positive: Patients who actually have benign cancer (Benign), predicted by the model as Malignant. This can cause panic in the patient, but the patient will receive further consultation or treatment as a preventive measure, so that patient safety is guaranteed. | False Negative: Patients who actually have malignant cancer (Malignant), predicted by the model to be Benign. This can jeopardize the patients&#39; safety because they are not taken seriously. | . . Note: If we want to minimize False Negative cases, high recall are expected. Precision is also expected to remain high, but could be a second priority. If we want both metrics to be prioritized, use the F1-score. . Logistic Regression . logreg = LogisticRegression(max_iter=np.inf, random_state=123) logreg.fit(X_train, y_train) . LogisticRegression(max_iter=inf, random_state=123) . We define a function threshold_tuning to further tune the threshold to get the largest F1 score. . def threshold_tuning(model, X, y): y_pred_prob = model.predict_proba(X)[:, 1] # probability eval_list = [] for threshold in np.linspace(0, 0.99, 100): y_pred = (y_pred_prob &gt; threshold).astype(int) # threshold cut-off # evaluation metric eval_dict = { &#39;Threshold&#39;: threshold, &#39;Recall&#39;: recall_score(y, y_pred), &#39;Precision&#39;: precision_score(y, y_pred), &#39;F1&#39;: f1_score(y, y_pred) } eval_list.append(eval_dict) # tuning result eval_df = pd.DataFrame(eval_list).set_index(&#39;Threshold&#39;) eval_df.columns = eval_df.columns.set_names(&#39;Metrics&#39;) max_f1 = eval_df.sort_values(&#39;F1&#39;, ascending=False).head(1) optimal_threshold = max_f1.index.values[0] # plotting fig = eval_df.plot(title=&quot;Threshold Tuning&quot;) fig.add_shape(dict(type=&quot;line&quot;, x0=optimal_threshold, y0=0, x1=optimal_threshold, y1=1)) # print classification report using max F1 y_pred_optimal = (y_pred_prob &gt; optimal_threshold).astype(int) print(classification_report(y, y_pred_optimal, target_names=[&#39;Benign&#39;, &#39;Malignant&#39;])) return eval_df.sort_values(&#39;F1&#39;, ascending=False), fig . . logreg_tuning, fig = threshold_tuning(logreg, X_test, y_test) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . precision recall f1-score support Benign 0.97 0.99 0.98 90 Malignant 0.98 0.94 0.96 53 accuracy 0.97 143 macro avg 0.97 0.97 0.97 143 weighted avg 0.97 0.97 0.97 143 . . . For logistic regression, we use threshold = 0.84 to get the largest F1 score on the testing set. Next, we would like to interpret the model via estimate (intercept and coefficient). The following predictors have been sorted by coefficient from largest to smallest: . logreg_interpret = pd.DataFrame(np.append(logreg.intercept_, logreg.coef_), columns=[&#39;Estimate (Logit)&#39;]) logreg_interpret.index = [&#39;intercept&#39;] + list(X.columns) logreg_interpret[&#39;Odds Ratio&#39;] = logreg_interpret[&#39;Estimate (Logit)&#39;].transform(np.exp) logreg_interpret.sort_values(&#39;Estimate (Logit)&#39;, ascending=False) . . Estimate (Logit) Odds Ratio . concavity_worst 1.07945 | 2.94308 | . symmetry_worst 0.62870 | 1.87517 | . concave points_worst 0.46480 | 1.59169 | . compactness_worst 0.44890 | 1.56659 | . texture_worst 0.39236 | 1.48047 | . concavity_mean 0.32357 | 1.38205 | . symmetry_mean 0.31655 | 1.37239 | . smoothness_worst 0.27173 | 1.31223 | . perimeter_worst 0.23309 | 1.26249 | . concave points_mean 0.21682 | 1.24212 | . smoothness_mean 0.15523 | 1.16793 | . perimeter_mean 0.15401 | 1.16651 | . compactness_mean 0.13147 | 1.14050 | . area_se 0.11032 | 1.11663 | . fractal_dimension_worst 0.06806 | 1.07043 | . concave points_se 0.02301 | 1.02327 | . symmetry_se 0.02242 | 1.02267 | . fractal_dimension_mean 0.02136 | 1.02159 | . radius_se 0.01969 | 1.01989 | . smoothness_se 0.01406 | 1.01416 | . area_worst 0.01009 | 1.01014 | . perimeter_se -0.00461 | 0.99540 | . concavity_se -0.01838 | 0.98179 | . fractal_dimension_se -0.02138 | 0.97885 | . area_mean -0.02385 | 0.97644 | . compactness_se -0.09884 | 0.90588 | . texture_mean -0.16468 | 0.84816 | . radius_worst -0.28772 | 0.74997 | . radius_mean -0.50493 | 0.60355 | . texture_se -0.98308 | 0.37416 | . intercept -30.78763 | 0.00000 | . . Note: Coefficient interpretation: A positive estimate value will increase the possibility of malignant cancer by the Odds Ratio value, if the predictor value increases by 1 unit. On the other hand, a negative estimate value will reduce the possibility of malignant cancer by the Odds Ratio value, if the predictor value increases by 1 unit. . Examples: . An increase of 1 unit of concativity_worst will increase the possibility of a cancer to be malignant by 194.308% (its Odds Ratio - 1) | An increase of 1 unit of texture_se will reduce the possibility of a cancer to be malignant by 62.584% (1 - its Odds Ratio) | . Support Vector Classifier . Use grid search cross-validation for the support vector model to get the largest F1 value. . svc_parameters = { &#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;], &#39;C&#39;: [0.1, 1, 10] } svc = SVC(random_state=123) svc_grid = GridSearchCV(svc, svc_parameters, cv=3, scoring=&#39;f1&#39;) svc_grid.fit(X_train, y_train) . GridSearchCV(cv=3, estimator=SVC(random_state=123), param_grid={&#39;C&#39;: [0.1, 1, 10], &#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;]}, scoring=&#39;f1&#39;) . pd.DataFrame(svc_grid.cv_results_).sort_values(&#39;rank_test_score&#39;).head(3)[[&#39;rank_test_score&#39;, &#39;params&#39;, &#39;mean_test_score&#39;, &#39;std_test_score&#39;]] . rank_test_score params mean_test_score std_test_score . 8 1 | {&#39;C&#39;: 10, &#39;kernel&#39;: &#39;linear&#39;} | 0.93955 | 0.02547 | . 4 2 | {&#39;C&#39;: 1, &#39;kernel&#39;: &#39;linear&#39;} | 0.93936 | 0.01144 | . 0 3 | {&#39;C&#39;: 0.1, &#39;kernel&#39;: &#39;linear&#39;} | 0.93888 | 0.01207 | . Re-training the support vector model with a combination of parameters that produces the largest F1. . . Note: Actually we can use svc_grid directly to predict a class, but in this case we would like to predict a probability to tune the threshold. It will take a long time if we use probability=True during the grid search because for all combinations of parameters the probability will be calculated. What we really want is the probability only for the best combination of parameters. . svc_best = SVC(**svc_grid.best_params_, probability=True, random_state=123) svc_best.fit(X_train, y_train) . SVC(C=10, kernel=&#39;linear&#39;, probability=True, random_state=123) . We further tune the support vector model using defined threshold_tuning function. . svc_tuning, fig = threshold_tuning(svc_best, X_test, y_test) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . precision recall f1-score support Benign 0.99 0.97 0.98 90 Malignant 0.95 0.98 0.96 53 accuracy 0.97 143 macro avg 0.97 0.97 0.97 143 weighted avg 0.97 0.97 0.97 143 . . . For support vector, we use threshold = 0.42 to get the largest F1 score on the testing set. . Decision Tree Classifier . Use grid search cross-validation for the decision tree model to get the largest F1 value. . dtc_parameters = { &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;splitter&#39;: [&#39;best&#39;, &#39;random&#39;], &#39;max_features&#39;: [2, &#39;sqrt&#39;, &#39;log2&#39;], &#39;min_samples_leaf&#39;: range(1, 10, 2) } dtc = DecisionTreeClassifier(random_state=123) dtc_grid = GridSearchCV(dtc, dtc_parameters, cv=3, scoring=&#39;f1&#39;) dtc_grid.fit(X_train, y_train) . GridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=123), param_grid={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [2, &#39;sqrt&#39;, &#39;log2&#39;], &#39;min_samples_leaf&#39;: range(1, 10, 2), &#39;splitter&#39;: [&#39;best&#39;, &#39;random&#39;]}, scoring=&#39;f1&#39;) . pd.DataFrame(dtc_grid.cv_results_).sort_values(&#39;rank_test_score&#39;).head(3)[[&#39;rank_test_score&#39;, &#39;params&#39;, &#39;mean_test_score&#39;, &#39;std_test_score&#39;]] . rank_test_score params mean_test_score std_test_score . 56 1 | {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;min_samples_leaf&#39;: 7, &#39;splitter&#39;: &#39;best&#39;} | 0.91301 | 0.01684 | . 50 2 | {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;min_samples_leaf&#39;: 1, &#39;splitter&#39;: &#39;best&#39;} | 0.91081 | 0.00144 | . 0 3 | {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: 2, &#39;min_samples_leaf&#39;: 1, &#39;splitter&#39;: &#39;best&#39;} | 0.90929 | 0.02982 | . We further tune the decision tree model using defined threshold_tuning function. We don&#39;t need to re-train the best model as in the previous support vector section, because dtc_grid can be directly used to predict probability. . dtc_tuning, fig = threshold_tuning(dtc_grid, X_test, y_test) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . precision recall f1-score support Benign 0.95 0.93 0.94 90 Malignant 0.89 0.92 0.91 53 accuracy 0.93 143 macro avg 0.92 0.93 0.93 143 weighted avg 0.93 0.93 0.93 143 . . . For decision tree, we use threshold = 0.5 to get the largest F1 score on the testing set. Next, we would like to interpret the model visually. . dot_data = export_graphviz( dtc_grid.best_estimator_, feature_names=X.columns, class_names=[&#39;Benign&#39;, &#39;Malignant&#39;], impurity=False, leaves_parallel=True, filled=True, rounded=True) graph = graphviz.Source(dot_data) # display(graph) graph.format = &quot;png&quot; graph.render(&quot;assets/2020-10-26-breast-cancer-wisconsin-classification/decision_tree_breast_cancer&quot;); . . . The decision tree can be traversed from the root (top) to leaf (bottom). The predicted class is labeled for each leaf. Each node will be separated by a condition, if True then traverse to the left, otherwise to the right. . Random Forest Classifier . Use grid search cross-validation for the random forest model to get the largest F1 value. . rfc_parameters = { &#39;n_estimators&#39;: [100, 200, 300], &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [2, &#39;sqrt&#39;, &#39;log2&#39;] } rfc = RandomForestClassifier(random_state=123) rfc_grid = GridSearchCV(rfc, rfc_parameters, cv=3, scoring=&#39;f1&#39;) rfc_grid.fit(X_train, y_train) . GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=123), param_grid={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [2, &#39;sqrt&#39;, &#39;log2&#39;], &#39;n_estimators&#39;: [100, 200, 300]}, scoring=&#39;f1&#39;) . pd.DataFrame(rfc_grid.cv_results_).sort_values(&#39;rank_test_score&#39;).head(3)[[&#39;rank_test_score&#39;, &#39;params&#39;, &#39;mean_test_score&#39;, &#39;std_test_score&#39;]] . rank_test_score params mean_test_score std_test_score . 1 1 | {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 200} | 0.93867 | 0.01034 | . 10 2 | {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 200} | 0.93862 | 0.01725 | . 13 3 | {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 200} | 0.93626 | 0.00933 | . We further tune the random forest model using defined threshold_tuning function. We don&#39;t need to re-train the best model as in the previous support vector section, because rfc_grid can be directly used to predict probability. . rfc_tuning, fig = threshold_tuning(rfc_grid, X_test, y_test) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . precision recall f1-score support Benign 0.99 0.97 0.98 90 Malignant 0.95 0.98 0.96 53 accuracy 0.97 143 macro avg 0.97 0.97 0.97 143 weighted avg 0.97 0.97 0.97 143 . . . For random forest, we use threshold = 0.46 to get the largest F1 score on the testing set. Next, we would like to interpret the model based on feature importances. Here are the top 10 features that are most important in classifying whether a cancer is benign or malignant: . pd.options.plotting.backend = &quot;matplotlib&quot; var_imp = pd.DataFrame({ &#39;Feature&#39;: X.columns, &#39;Importance&#39;: rfc_grid.best_estimator_.feature_importances_ }).set_index(&#39;Feature&#39;) var_imp.sort_values(&#39;Importance&#39;, ascending=False).head(10).sort_values( &#39;Importance&#39;).plot.barh(title=&#39;Top 10 Feature Importances&#39;); . . Conclusion . The following are the performance results of each binary classification model: . final_result = pd.concat([ logreg_tuning.head(1), svc_tuning.head(1), dtc_tuning.head(1), rfc_tuning.head(1)]).reset_index() final_result.index = [&#39;Logistic Regression&#39;, &#39;Support Vector Classifier&#39;, &#39;Decision Tree Classifier&#39;, &#39;Random Forest Classifier&#39;] final_result . Metrics Threshold Recall Precision F1 . Logistic Regression 0.84000 | 0.94340 | 0.98039 | 0.96154 | . Support Vector Classifier 0.42000 | 0.98113 | 0.94545 | 0.96296 | . Decision Tree Classifier 0.50000 | 0.92453 | 0.89091 | 0.90741 | . Random Forest Classifier 0.46000 | 0.98113 | 0.94545 | 0.96296 | . Each model has its advantages and disadvantages as shown in the image below: . . If we have to choose between the four models, we prefer the model that results in high performance in classifying cancer since the safety and life of a patient are crucial in the medical field. Thus, we choose Random Forest Classifier with a threshold of 0.46 because it has the highest F1 score, and the importance of each feature can be measured using Feature Importances (although the influence of positive or negative directions is unknown as in Logistic Regression). .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/scikit-learn/2020/10/26/breast-cancer-wisconsin-classification.html",
            "relUrl": "/python/classification/scikit-learn/2020/10/26/breast-cancer-wisconsin-classification.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Hotel Booking Cancellation Prediction using Random Forest",
            "content": "Introduction . In this post, we&#39;ll be using Hotel Booking Demand data from Kaggle. This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. We are going to visualize the data by using plotly and also predict the possibility of hotel bookings using random forest model using sklearn. . Import Libraries . As usual, before we begin any analysis and modeling, let&#39;s import several necessary libraries to work with the data. There are two additional packages used in this notebook: . pycountry: contains mapping of ISO country, subdivision, language, currency and script definitions and their translations | ppscore: implementation of the Predictive Power Score (PPS) | . import pandas as pd import numpy as np # visualization import matplotlib.pyplot as plt import seaborn as sns import plotly.offline as py import plotly.io as pio import plotly.express as px sns.set() pio.templates.default = &quot;plotly_white&quot; pio.renderers.default = &quot;notebook&quot; # modeling from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV from sklearn.metrics import classification_report, roc_curve, roc_auc_score from sklearn.utils import resample import multiprocessing import pickle # additional packages import pycountry import pycountry_convert as pc import ppscore as pps from IPython.display import Image, HTML from tqdm.notebook import tqdm_notebook import warnings warnings.filterwarnings(&#39;ignore&#39;) . Data Wrangling . Before we jump into any visualization or modeling step, we have to make sure our data is ready. . Import Data . Load the downloaded csv and inspect the structure of the data. . hotel = pd.read_csv(&quot;data_input/hotel_bookings.csv&quot;) hotel.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 119390 entries, 0 to 119389 Data columns (total 32 columns): # Column Non-Null Count Dtype -- -- 0 hotel 119390 non-null object 1 is_canceled 119390 non-null int64 2 lead_time 119390 non-null int64 3 arrival_date_year 119390 non-null int64 4 arrival_date_month 119390 non-null object 5 arrival_date_week_number 119390 non-null int64 6 arrival_date_day_of_month 119390 non-null int64 7 stays_in_weekend_nights 119390 non-null int64 8 stays_in_week_nights 119390 non-null int64 9 adults 119390 non-null int64 10 children 119386 non-null float64 11 babies 119390 non-null int64 12 meal 119390 non-null object 13 country 118902 non-null object 14 market_segment 119390 non-null object 15 distribution_channel 119390 non-null object 16 is_repeated_guest 119390 non-null int64 17 previous_cancellations 119390 non-null int64 18 previous_bookings_not_canceled 119390 non-null int64 19 reserved_room_type 119390 non-null object 20 assigned_room_type 119390 non-null object 21 booking_changes 119390 non-null int64 22 deposit_type 119390 non-null object 23 agent 103050 non-null float64 24 company 6797 non-null float64 25 days_in_waiting_list 119390 non-null int64 26 customer_type 119390 non-null object 27 adr 119390 non-null float64 28 required_car_parking_spaces 119390 non-null int64 29 total_of_special_requests 119390 non-null int64 30 reservation_status 119390 non-null object 31 reservation_status_date 119390 non-null object dtypes: float64(4), int64(16), object(12) memory usage: 29.1+ MB . Our dataframe hotel contains 119390 rows of bookings and 32 columns with data description as follows: . hotel: Hotel (H1 = Resort Hotel or H2 = City Hotel) | is_canceled: Value indicating if the booking was canceled (1) or not (0) | lead_time: Number of days that elapsed between the entering date of the booking into the PMS and the arrival date | arrival_date_year: Year of arrival date | arrival_date_month: Month of arrival date | arrival_date_week_number: Week number of year for arrival date | arrival_date_day_of_month: Day of arrival date | stays_in_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel | stays_in_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel | adults: Number of adults | children: Number of children | babies: Number of babies | meal: Type of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC – no meal package; | BB – Bed &amp; Breakfast; | HB – Half board (breakfast and one other meal – usually dinner); | FB – Full board (breakfast, lunch and dinner) | . | country: Country of origin. Categories are represented in the ISO 3155–3:2013 format | market_segment: Market segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators” | distribution_channel: Booking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators” | is_repeated_guest: Value indicating if the booking name was from a repeated guest (1) or not (0) | previous_cancellations: Number of previous bookings that were cancelled by the customer prior to the current booking | previous_bookings_not_canceled: Number of previous bookings not cancelled by the customer prior to the current booking | reserved_room_type: Code of room type reserved. Code is presented instead of designation for anonymity reasons. | assigned_room_type: Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons. | booking_changes: Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation | deposit_type: Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit – no deposit was made; | Non Refund – a deposit was made in the value of the total stay cost; | Refundable – a deposit was made with a value under the total cost of stay. | . | agent: ID of the travel agency that made the booking | company: ID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons | days_in_waiting_list: Number of days the booking was in the waiting list before it was confirmed to the customer | customer_type: Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; | Group – when the booking is associated to a group; | Transient – when the booking is not part of a group or contract, and is not associated to other transient booking; | Transient-party – when the booking is transient, but is associated to at least other transient booking | . | adr: Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights | required_car_parking_spaces: Number of car parking spaces required by the customer | total_of_special_requests: Number of special requests made by the customer (e.g. twin bed or high floor) | reservation_status: Reservation last status, assuming one of three categories: Canceled – booking was canceled by the customer; | Check-Out – customer has checked in but already departed; | No-Show – customer did not check-in and did inform the hotel of the reason why | . | reservation_status_date: Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel. | . Missing Values . Check if there are any missing values in hotel . hotel.isna().sum().sort_values(ascending=False).head(10) . . company 112593 agent 16340 country 488 children 4 lead_time 0 arrival_date_year 0 arrival_date_month 0 arrival_date_week_number 0 is_canceled 0 market_segment 0 dtype: int64 . There are four columns with missing values, here&#39;s how we handle them: . Drop columns agent and company, since the missing values are too many and we won&#39;t use them for prediction | Create &quot;UNKNOWN&quot; category for country | Fill children with the value 0 | . hotel.drop(columns=[&#39;agent&#39;, &#39;company&#39;], inplace=True) hotel[&#39;country&#39;].fillna(&quot;UNKNOWN&quot;, inplace=True) hotel[&#39;children&#39;].fillna(0, inplace=True) . . Check whether there is another missing values. . hotel.isna().values.any() . False . Data Type Conversion . Categorical . We convert object to category data types to save memory. Also map the boolean columns is_canceled and is_repeated_guest into category for readability. . # list of columns to be casted category_cols = [&#39;hotel&#39;, &#39;meal&#39;, &#39;country&#39;, &#39;market_segment&#39;, &#39;distribution_channel&#39;, &#39;reserved_room_type&#39;, &#39;assigned_room_type&#39;, &#39;deposit_type&#39;, &#39;customer_type&#39;, &#39;reservation_status&#39;] boolean_cols = [&#39;is_canceled&#39;, &#39;is_repeated_guest&#39;] # map values boolean_map = {0: &#39;No&#39;, 1: &#39;Yes&#39;} hotel[&#39;is_canceled&#39;] = hotel[&#39;is_canceled&#39;].map(boolean_map) hotel[&#39;is_repeated_guest&#39;] = hotel[&#39;is_repeated_guest&#39;].map(boolean_map) # re-order categories for readability hotel[category_cols + boolean_cols] = hotel[category_cols + boolean_cols].astype(&#39;category&#39;) hotel[&#39;is_canceled&#39;].cat.reorder_categories( list(boolean_map.values()), inplace=True) hotel[&#39;is_repeated_guest&#39;].cat.reorder_categories( list(boolean_map.values()), inplace=True) . . Numerical . Convert children from float to integer. . hotel[&#39;children&#39;].apply(float.is_integer).all() hotel[&#39;children&#39;] = hotel[&#39;children&#39;].astype(&#39;int&#39;) . . Datetime . Convert reservation_status_date as datetime. . hotel[&#39;reservation_status_date&#39;] = hotel[&#39;reservation_status_date&#39;].astype(&#39;datetime64&#39;) . . Feature Engineering . Feature engineering is the process of using domain knowledge to extract features from provided raw data. These features can be used to improve the performance of machine learning models. . Room Type Assignment . Instead of considering each assigned and reserved room type, we create a new column is_assigned_as_reserved to make a flag whether the customer get their expected room type or not. . hotel[&#39;reserved_room_type&#39;].cat.set_categories(hotel[&#39;assigned_room_type&#39;].cat.categories, inplace=True) hotel[&#39;is_assigned_as_reserved&#39;] = (hotel[&#39;assigned_room_type&#39;] == hotel[&#39;reserved_room_type&#39;]).astype(&#39;category&#39;) hotel[[&#39;assigned_room_type&#39;, &#39;reserved_room_type&#39;, &#39;is_assigned_as_reserved&#39;]].head() . . assigned_room_type reserved_room_type is_assigned_as_reserved . 0 C | C | True | . 1 C | C | True | . 2 C | A | False | . 3 A | A | True | . 4 A | A | True | . Arrival Date . Combine arrival_date_year, arrival_date_month, arrival_date_day_of_month into one column arrival_date so that we can extract more information from the date. . arrival_date_cols = [&#39;arrival_date_year&#39;, &#39;arrival_date_month&#39;, &#39;arrival_date_day_of_month&#39;] hotel[arrival_date_cols] = hotel[arrival_date_cols].astype(str) hotel[&#39;arrival_date&#39;] = pd.to_datetime(hotel[arrival_date_cols].apply(&#39;-&#39;.join, axis=1), format=&quot;%Y-%B-%d&quot;) hotel.drop(columns = arrival_date_cols + [&#39;arrival_date_week_number&#39;], inplace=True) hotel[[&#39;arrival_date&#39;]].head() . . arrival_date . 0 2015-07-01 | . 1 2015-07-01 | . 2 2015-07-01 | . 3 2015-07-01 | . 4 2015-07-01 | . Booking Date . Create booking_date by subtracting lead_time days from arrival_date. . hotel[&#39;booking_date&#39;] = hotel[&#39;arrival_date&#39;] - pd.to_timedelta(hotel[&#39;lead_time&#39;], unit=&#39;days&#39;) hotel[[&#39;booking_date&#39;, &#39;arrival_date&#39;, &#39;lead_time&#39;]].head() . . booking_date arrival_date lead_time . 0 2014-07-24 | 2015-07-01 | 342 | . 1 2013-06-24 | 2015-07-01 | 737 | . 2 2015-06-24 | 2015-07-01 | 7 | . 3 2015-06-18 | 2015-07-01 | 13 | . 4 2015-06-17 | 2015-07-01 | 14 | . Country and Continent Name . The column country represents code of a country in the ISO 3155–3:2013 format. By utilizing the code-to-name mapping provided in pycountry package, we can extract it into country_name and continent_name. . def convertCountryCode2Name(code): additional_code2name = {&#39;TMP&#39;: &#39;East Timor&#39;} country_name = None try: if len(code) == 2: country_name = pycountry.countries.get(alpha_2=code).name elif len(code) == 3: country_name = pycountry.countries.get(alpha_3=code).name except: if code in additional_code2name.keys(): country_name = additional_code2name[code] return country_name if country_name is not None else code def convertCountryName2Continent(country_name): additional_name2continent = { &#39;East Timor&#39;: &#39;Asia&#39;, &#39;United States Minor Outlying Islands&#39;: &#39;North America&#39;, &#39;French Southern Territories&#39;: &#39;Antarctica&#39;, &#39;Antarctica&#39;: &#39;Antarctica&#39;} continent_name = None try: alpha2 = pc.country_name_to_country_alpha2(country_name) continent_code = pc.country_alpha2_to_continent_code(alpha2) continent_name = pc.convert_continent_code_to_continent_name(continent_code) except: if country_name in additional_name2continent.keys(): continent_name = additional_name2continent[country_name] else: continent_name = &quot;UNKNOWN&quot; return continent_name if continent_name is not None else country_name hotel[&#39;country_name&#39;] = hotel[&#39;country&#39;].apply( convertCountryCode2Name).astype(&#39;category&#39;) hotel[&#39;continent_name&#39;] = hotel[&#39;country_name&#39;].apply( convertCountryName2Continent).astype(&#39;category&#39;) hotel[[&#39;country&#39;, &#39;country_name&#39;, &#39;continent_name&#39;]].head() . . country country_name continent_name . 0 PRT | Portugal | Europe | . 1 PRT | Portugal | Europe | . 2 GBR | United Kingdom | Europe | . 3 GBR | United Kingdom | Europe | . 4 GBR | United Kingdom | Europe | . Suspicious Bookings . There are some hidden anomalies present in the hotel bookings. Let&#39;s create new variables and plot their frequencies: . total_guest: Total number of adults, children, and babies | total_nights: Number of nights the guest stayed at the hotel, sum of stays_in_weekend_nights and stays_in_week_nights | . hotel[&#39;total_guest&#39;] = hotel[[&#39;adults&#39;, &#39;children&#39;, &#39;babies&#39;]].sum(axis=1) hotel[&#39;total_nights&#39;] = hotel[[&#39;stays_in_weekend_nights&#39;, &#39;stays_in_week_nights&#39;]].sum(axis=1) data2plot = [hotel[&#39;total_guest&#39;].value_counts().sort_index(ascending=False), hotel[&#39;total_nights&#39;].value_counts().sort_index(ascending=False)[-21:]] ylabs = [&quot;Total Guest per Booking (Person)&quot;, &quot;Total Nights per Booking&quot;] titles = [&quot;FREQUENCY OF TOTAL GUEST PER BOOKING n&quot;, &quot;FREQUENCY OF TOTAL NIGHTS PER BOOKING n(UP TO 20 NIGHTS ONLY)&quot;] fig, axes = plt.subplots(1, 2, figsize=(15, 5)) for ax, data, ylab, title in zip(axes, data2plot, ylabs, titles): bp = data.plot(kind=&#39;barh&#39;, rot=0, ax=ax) for rect in bp.patches: height = rect.get_height() width = rect.get_width() bp.text(rect.get_x() + width, rect.get_y() + height/2, int(width), ha=&#39;left&#39;, va=&#39;center&#39;, fontsize=8) bp.set_xlabel(&quot;Frequency&quot;) bp.set_ylabel(ylab) ax.set_title(title, fontweight=&quot;bold&quot;) . . There are 180 bookings without guest (total_guest = 0) and 715 bookings with zero nights of staying at the hotel (total_nights = 0). Ideally, such cases should not occur on our bookings data. Therefore, from this point onwards we will ignore the observations with either cases since it can affect our modeling outcome. . hotel = hotel[(hotel[&#39;total_guest&#39;] != 0) &amp; (hotel[&#39;total_nights&#39;] != 0)] hotel.shape . (118565, 33) . We end up with 118565 rows of bookings, originally it was 119390 rows. . Exploratory Data Analysis (EDA) . Before we jump into the modeling step, it is recommended to visualize the data to better understand our data. . How is the proportion of booking cancellation based on reservation status? . df_cancel_status = pd.crosstab(index=hotel.is_canceled, columns=hotel.reservation_status, margins=True) ax = df_cancel_status.iloc[:-1, :-1].plot(kind=&#39;bar&#39;, stacked=True, rot=0) for rect in ax.patches: height = rect.get_height() width = rect.get_width() if height != 0: ax.text(rect.get_x() + width, rect.get_y() + height/2, int(height), ha=&#39;left&#39;, va=&#39;center&#39;, color=&quot;black&quot;, fontsize=10) handles, labels = ax.get_legend_handles_labels() ax.legend(handles=handles, labels=labels, title=&quot;Reservation Status&quot;) percent_no = (100*df_cancel_status / df_cancel_status.iloc[-1, -1]).loc[&quot;No&quot;, &quot;All&quot;] ax.set_xticklabels([&quot;No n({:.2f} %)&quot;.format( percent_no), &quot;Yes n({:.2f} %)&quot;.format(100-percent_no)]) ax.set_xlabel(&quot;Canceled?&quot;) ax.set_ylabel(&quot;Number of Bookings&quot;) plt.title(&quot;BOOKING CANCELLATION PROPORTION&quot;, fontweight=&quot;bold&quot;) plt.show() . . The proportion of the target variable is_canceled is somewhat balanced. There is 37.26% of the bookings which are canceled, divided into two cases: . Canceled: Booking was canceled by the customer, or | No-show: Customer did not check-in and did inform the hotel of the reason why. | . . Important: In building model, we have to remove leaky variables - a predictor which would not be expected to be available at prediction time. In this case, reservation_status is a variable that is known after the target variable is_canceled is known. . Where do most of the bookings happens? . df_choropleth = hotel.copy() df_choropleth[&#39;booking_date_year&#39;] = df_choropleth[&#39;booking_date&#39;].dt.year df_country_year_count = df_choropleth.groupby([&#39;country&#39;, &#39;booking_date_year&#39;]).count()[&#39;hotel&#39;].fillna(0).reset_index() .rename(columns={&#39;country&#39;: &#39;country_code&#39;, &#39;booking_date_year&#39;: &#39;year&#39;, &#39;hotel&#39;: &#39;count&#39;}) df_country_year_count[&#39;country_name&#39;] = df_country_year_count[&#39;country_code&#39;].apply( convertCountryCode2Name) df_country_year_count[&#39;count&#39;] = df_country_year_count[&#39;count&#39;].astype(&#39;int&#39;) fig = px.choropleth(df_country_year_count[df_country_year_count[&quot;year&quot;] != 2013], locations=&quot;country_code&quot;, color=&quot;count&quot;, animation_frame=&quot;year&quot;, hover_name=&quot;country_name&quot;, range_color=(0, 5000), color_continuous_scale=px.colors.sequential.Reds, projection=&quot;natural earth&quot;) fig.update_layout(title=&#39;ANNUAL HOTEL BOOKING COUNTS&#39;, template=&quot;seaborn&quot;) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . . Note: From the choropleth map, we can see that Europe is the continent with the most hotel booking counts. The specific country with the most bookings is Portugal (PRT). . Which continent has the greatest cancellation rate? . From the previous section, we know Europe is the continent with most bookings, but how about the cancellation rate? . ax = pd.crosstab(index=hotel[&#39;continent_name&#39;], columns=hotel[&#39;is_canceled&#39;], margins=True).sort_values(&#39;All&#39;).iloc[:-1, :-1].plot(kind=&#39;barh&#39;) ax.legend(bbox_to_anchor=(1, 1), title=&quot;Canceled?&quot;) ax.set_xlabel(&quot;Number of Bookings&quot;) ax.set_ylabel(&quot;Continent Name&quot;) ax.set_title(&quot;BOOKINGS BY EACH CONTINENT&quot;, fontweight=&quot;bold&quot;) plt.show() . . ax = (pd.crosstab(index=hotel[&#39;continent_name&#39;], columns=hotel[&#39;is_canceled&#39;], normalize=&#39;index&#39;).sort_values(&#39;Yes&#39;) * 100).plot(kind=&#39;barh&#39;, stacked=True) ax.legend(bbox_to_anchor=(1, 1), title=&quot;Canceled?&quot;) ax.set_xlabel(&quot;Percentage of Bookings&quot;) ax.set_ylabel(&quot;Continent Name&quot;) ax.set_title(&quot;PERCENTAGE OF BOOKINGS CANCELLATION BY EACH CONTINENT&quot;, fontweight=&quot;bold&quot;) plt.show() . . . Note: Turns out that continent_name maybe use as a predictor since the cancellation rate differs amongst continent, with Africe with the greatest cancellation rate. . How is the cancellation rate over time? . df_cancellation = hotel.copy() df_cancellation[&#39;date_period&#39;] = df_cancellation[&#39;reservation_status_date&#39;].dt.to_period(&#39;M&#39;) df_cancellation_percent = df_cancellation.groupby([&#39;date_period&#39;, &#39;is_canceled&#39;, &#39;hotel&#39;])[&#39;hotel&#39;].count() .groupby([&#39;date_period&#39;, &#39;hotel&#39;]).apply(lambda x: 100*x/x.sum()) .unstack(level=&#39;is_canceled&#39;) .rename(columns=str).reset_index().rename_axis(None, axis=1).rename(columns={&#39;hotel&#39;: &#39;Hotel Type&#39;}) df_cancellation_percent[&#39;date_period&#39;] = df_cancellation_percent[&#39;date_period&#39;].values.astype(&#39;datetime64[M]&#39;) fig = px.line(df_cancellation_percent, x=&#39;date_period&#39;, y=&#39;Yes&#39;, color=&#39;Hotel Type&#39;) fig.update_traces(mode=&quot;markers+lines&quot;, hovertemplate=&quot;Rate: %{y:.2f}%&quot;) fig.update_layout(title=&#39;CANCELLATION RATE OVER TIME BY HOTEL TYPE&#39;, xaxis_title=&#39;Cancellation Period&#39;, yaxis_title=&#39;Cancellation Rate (%)&#39;, hovermode=&#39;x&#39;, template=&quot;seaborn&quot;, xaxis=dict(tickformat=&quot;%b %Y&quot;)) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . . Note: Most of the time, City hotel has greater cancellation rate than Resort hotel. The good news is that: both rate are decreasing towards zero during mid 2017, meaning none of the booking was cancelled. But we have to anticipate during the beginning of year 2018, since for the past three years there are peaks on the cancellation rate during January. Therefore in the next section, we can use a Machine Learning model to predict whether certain bookings will be canceled by the customer or not from predictor variables present in our data. . Predictive Power Score (PPS) . According to Florian Wetschoreck, PPS is an asymmetric and data-type-agnostic score that can detect linear or non-linear relationships between two columns. One column acts as an univariate predictor, whereas the other acts as the target variable. The score ranges from 0 (no predictive power) to 1 (perfect predictive power). It can be used as an alternative to the correlation matrix. . The PPS is calculated as follows: . $PPS = dfrac{F1_{model} - F1_{naive}}{1 - F1_{naive}}$ . where: . $F1_{naive}$ is the weighted F1 score of a naive model that always predicts the most common class of the target column. | $F1_{model}$ is the weighted F1 score of a classifier using sklearn.DecisionTreeClassifier. | . . Note: Detailed explanation of Predictive Power Score is available on GitHub. . Before we investigate the PPS, we do the following data preparation: . Consider dayofyear instead of datetime for booking_date, reservation_status_date, and arrival_date. | Ignore assigned_room_type and reserved_room_type because the levels are quite many, instead is_assigned_as_reserved will be considered. | Ignore country and country_name because the levels are too many, instead continent_name will be considered. | Convert the categorical columns into dummy variables. | . datetime_cols = [&#39;booking_date&#39;, &#39;reservation_status_date&#39;, &#39;arrival_date&#39;] for col in datetime_cols: hotel[f&quot;{col}_dayofyear&quot;] = hotel[col].dt.dayofyear ignore_cols = [&#39;assigned_room_type&#39;, &#39;reserved_room_type&#39;, &#39;country&#39;, &#39;country_name&#39;] hotel_pps_data = hotel.drop(datetime_cols + ignore_cols, axis=1) . . We calculate PPS for each categorical and numerical predictors and then present the ranking in a bar chart. . pps_score = [] target = &#39;is_canceled&#39; for col in hotel_pps_data.columns: if col == target: continue d = {} d[&#39;feature&#39;] = col d[&#39;dtypes&#39;] = hotel_pps_data[col].dtypes d[&#39;pps&#39;] = pps.score(hotel_pps_data, x=col, y=target)[&#39;ppscore&#39;] pps_score.append(d) hotel_pps = pd.DataFrame(pps_score).set_index(&#39;feature&#39;) . . ax = hotel_pps[hotel_pps[&#39;dtypes&#39;] == &#39;category&#39;].sort_values(&#39;pps&#39;)[:-1] .plot(kind=&#39;barh&#39;, legend=False) for rect in ax.patches: height = rect.get_height() width = rect.get_width() ax.text(rect.get_x() + 1.01*width, rect.get_y() + 0.75*height, round(width, 2), ha=&#39;left&#39;, va=&#39;top&#39;, fontsize=8) ax.set_xlabel(&quot;PPS&quot;) ax.set_ylabel(&quot;Predictor Variable&quot;) plt.title(&quot;CATEGORICAL PREDICTORS PREDICTIVE POWER SCORE n TARGET: is_canceled&quot;, fontweight=&quot;bold&quot;) plt.show() . . . Note: From PPS of the categorical variables, we have to ignore reservation_status for the modeling. The score is high because from the business perspective, this value is actually is_canceled but breakdown into three specific categories. So we cannot use this as a predictor. . ax = hotel_pps[hotel_pps[&#39;dtypes&#39;] != &#39;category&#39;].sort_values(&#39;pps&#39;) .plot(kind=&#39;barh&#39;, legend=False) for rect in ax.patches: height = rect.get_height() width = rect.get_width() ax.text(rect.get_x() + width, rect.get_y() + height/2, round(width, 2), ha=&#39;left&#39;, va=&#39;center&#39;, fontsize=8) ax.set_xlabel(&quot;PPS&quot;) ax.set_ylabel(&quot;Predictor Variable&quot;) plt.title(&quot;NUMERICAL PREDICTORS PREDICTIVE POWER SCORE n TARGET: is_canceled&quot;, fontweight=&quot;bold&quot;) plt.show() . . . Note: From PPS of the numerical variables, we have ignore reservation_status_date_dayofyear the reason is the same as before when we ignore reservation_status. We can ignore stay_in_week_nights and stay_in_weekend_nights because already explained by total_nights. Also, adults, children, and babies are explained by total_guest. . Modeling and Evaluation . Now, let&#39;s proceed to the modeling step using Random Forest Classifier by sklearn. . Feature Selection . PPS score from the previous section help us to identify which column to be ignored. We are going to drop four columns before we fit the data into Random Forest model. . ignore_cols_for_model = [&#39;reservation_status&#39;, &#39;reservation_status_date_dayofyear&#39;, &#39;total_nights&#39;, &#39;total_guest&#39;] hotel_model = hotel_pps_data.drop(columns=ignore_cols_for_model) print(f&quot;Dimension of data: {hotel_model.shape[0]} rows and {hotel_model.shape[1]} columns&quot;) . Dimension of data: 118565 rows and 25 columns . Train-test split . We split our dataset into 80% train and 20% test dataset for model evaluation. . hotel_model_dummy = pd.get_dummies(hotel_model, drop_first=True) X = hotel_model_dummy.drop([&#39;is_canceled_Yes&#39;], axis=1) y = hotel_model_dummy[&#39;is_canceled_Yes&#39;] X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.8, shuffle=True, random_state=333) print(f&quot;Predictors: {X_train.shape[1]}&quot;) print(f&quot;Training dataset: {y_train.shape[0]} observations&quot;) print(f&quot;Testing dataset: {y_test.shape[0]} observations&quot;) . Predictors: 45 Training dataset: 94852 observations Testing dataset: 23713 observations . Default Model . First, we fit the train dataset into RandomForestClassifier without tuning any parameter to know the starting performance of our model as a benchmark. . model_default = RandomForestClassifier( n_jobs=multiprocessing.cpu_count()-1, oob_score=True, random_state=333) model_default.fit(X_train, y_train) . RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=7, oob_score=True, random_state=333, verbose=0, warm_start=False) . Then we evaluate the model_default by printing out classification metrics such as: . Accuracy: the proportion of observations that are classified correctly. | Precision: the proportion of True Positives out of all observations predicted as positive. | Recall: the proportion of actual positives that are classified correctly. | F1-score: the harmonic mean of precision and recall. | . def clfReport(y_true, y_pred): report = classification_report( y_true, y_pred, output_dict=True, target_names=[&quot;Not Canceled&quot;, &quot;Canceled&quot;]) return pd.DataFrame(report).transpose() def clfReport_default(model, X, y): clfreport = clfReport(y, model.predict(X)) print(f&#39;Accuracy: {clfreport.loc[&quot;accuracy&quot;, &quot;support&quot;]}&#39;) return clfreport.iloc[:2, :-1] . . clfReport_default(model_default, X_test, y_test) . Accuracy: 0.8594441867330156 . precision recall f1-score . Not Canceled 0.858404 | 0.930393 | 0.892950 | . Canceled 0.861684 | 0.738600 | 0.795409 | . From the report above, we conclude that model_default bias towards &quot;Not Canceled&quot; (negative class) because the recall is much higher than &quot;Canceled&quot; (positive class). The cause of this may be because the target variable proportion is not equally balanced, the original ratio is 63:37. In the next section, we&#39;ll perform sampling to overcome this problem. . . Note: Let&#8217;s focus on recall because in this case, we want to minimize False Negative, where a canceled booking is predicted as not canceled. Suppose the marketing team of this hotel only offers follow-up promotions to customers who are predicted as canceled. If the recall is too low, then the potential customer who will cancel their booking won&#8217;t be contacted. . Balancing Data . We can do either upsampling or downsampling to balance the positive and negative class of is_canceled into a 50:50 ratio. . Upsampling is a method to randomly subsample the observation from minority class to make the dataset balanced. | Downsampling is a method to randomly sampled (with replacement) the observation from the majority class to make the dataset balanced. | . In this case, we prefer to do downsampling since we still have thousands of observations, which are sufficient for modeling. But we only do downsampling on the training dataset, whereas still retain the ratio of testing data. . def sampleData(data, target_col, method=&quot;down&quot;): freq = data[target_col].value_counts() class_majority, class_minority = freq.idxmax(), freq.idxmin() data_majority, data_minority = data[data[target_col] == class_majority], data[data[target_col] == class_minority] if method == &quot;down&quot;: data_hold = data_minority data_sample = data_majority replacement = False elif method == &quot;up&quot;: data_hold = data_majority data_sample = data_minority replacement = True data_sampled = resample(data_sample, n_samples=data_hold.shape[0], replace=replacement, random_state=333) return pd.concat([data_sampled, data_hold]) . . hotel_model_train_down = sampleData( pd.concat([X_train, y_train], axis=1), &quot;is_canceled_Yes&quot;, &quot;down&quot;) X_train_down = hotel_model_train_down.drop([&#39;is_canceled_Yes&#39;], axis=1) y_train_down = hotel_model_train_down[&#39;is_canceled_Yes&#39;] . Here is the proportion of target variable before and after downsampling. . pd.concat([ y_train.value_counts().rename(&quot;Before Sampling&quot;), y_train_down.value_counts().rename(&quot;After Sampling&quot;)], axis=1).set_index(pd.Series([&quot;Not Canceled&quot;, &quot;Canceled&quot;])) . . Before Sampling After Sampling . Not Canceled 59448 | 35404 | . Canceled 35404 | 35404 | . Default Model (with Downsampled Data) . We fit the downsampled train dataset again into RandomForestClassifier and compare it with model_default. . model_default_down = RandomForestClassifier( n_jobs=multiprocessing.cpu_count()-1, oob_score=True, random_state=333) model_default_down.fit(X_train_down, y_train_down) clfReport_default(model_default_down, X_test, y_test) . Accuracy: 0.8525703200775946 . precision recall f1-score . Not Canceled 0.886786 | 0.878121 | 0.882432 | . Canceled 0.795806 | 0.809052 | 0.802374 | . Good, now the recall gap is not too huge. Next, let&#39;s do train-test evaluation to check whether the model is a good fit or not. . def trainTestEval(model, pos_class, X_train, X_test, y_train, y_test): report_train = clfReport(y_train, model.predict(X_train)) report_test = clfReport(y_test, model.predict(X_test)) df = pd.concat([report_train.loc[pos_class, :].rename(&quot;Train&quot;), report_test.loc[pos_class, :].rename(&quot;Test&quot;)], axis=1).transpose() df.insert(loc=0, column=&quot;accuracy&quot;, value=[ report_train.loc[&quot;accuracy&quot;, &quot;support&quot;], report_test.loc[&quot;accuracy&quot;, &quot;support&quot;]]) df[&quot;observation&quot;] = np.array([X_train.shape[0], X_test.shape[0]]) df[&quot;pos_proportion&quot;] = df[&quot;support&quot;]/df[&quot;observation&quot;] return df.drop(columns=&quot;support&quot;) . . trainTestEval(model_default_down, &quot;Canceled&quot;, X_train_down, X_test, y_train_down, y_test) . accuracy precision recall f1-score observation pos_proportion . Train 0.983801 | 0.989008 | 0.978477 | 0.983715 | 70808 | 0.500000 | . Test 0.852570 | 0.795806 | 0.809052 | 0.802374 | 23713 | 0.369924 | . . Note: From the evaluation report above, we can conclude that model_default_down is overfitted the data, since the performance on the training dataset is nearly perfect but not so good on the test dataset. To overcome this, we are going to tune the hyperparameter using Random Search Cross-Validation. . Random Search Cross-Validation . This technique narrows down our search by evaluating a wide range of values for each parameter. Using the sklearn RandomizedSearchCV method, we define a grid of hyperparameter ranges random_grid and as the name suggests, it randomly samples from the grid, performing k-fold cross-validation with each combination of values. . random_grid = { &#39;n_estimators&#39;: [100, 200, 300], &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_depth&#39;: [x for x in range(5, 100, 2)], &#39;max_features&#39;: [&#39;sqrt&#39;, &#39;log2&#39;, None] } # number of possible combinations mul = 1 for key in random_grid.keys(): mul *= len(random_grid[key]) print(f&quot;Number of possible combinations: {mul}&quot;) . . Number of possible combinations: 864 . From the above number of possible combinations, we decided to build models by using only 200 combinations. Using 3-fold cross-validation means that each combination of parameters will be evaluated 3 times with a 67-33 train-test proportion. In this case, we want to maximize recall score on the testing set. Anyway, the code below takes a while to be executed, 2-3 hours to be specific, since we train them on CPU - maybe should consider using GPU. . model_base = RandomForestClassifier(warm_start=True, n_jobs=multiprocessing.cpu_count()-1, random_state=333) rf_random = RandomizedSearchCV(estimator=model_base, param_distributions=random_grid, scoring=&#39;recall&#39;, n_iter=250, cv=3, verbose=1, n_jobs=multiprocessing.cpu_count()-1, random_state=333) rf_random.fit(X_train_down, y_train_down) . Fitting 3 folds for each of 250 candidates, totalling 750 fits . [Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers. [Parallel(n_jobs=7)]: Done 36 tasks | elapsed: 7.1min [Parallel(n_jobs=7)]: Done 186 tasks | elapsed: 30.5min [Parallel(n_jobs=7)]: Done 436 tasks | elapsed: 67.4min [Parallel(n_jobs=7)]: Done 750 out of 750 | elapsed: 132.6min finished . RandomizedSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=&#39;warn&#39;, n_jobs=7, oob_score... iid=&#39;warn&#39;, n_iter=250, n_jobs=7, param_distributions={&#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_depth&#39;: [5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, ...], &#39;max_features&#39;: [&#39;sqrt&#39;, &#39;log2&#39;, None], &#39;n_estimators&#39;: [100, 200, 300]}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=333, refit=True, return_train_score=False, scoring=&#39;recall&#39;, verbose=1) . def saveModel(model, pickle_name): with open(pickle_name, &#39;wb&#39;) as f: pickle.dump(model, f) saveModel(rf_random, &quot;cache/rf_randomcv_500_down.pkl&quot;) . . Let&#39;s take a look of the result from best to worst in terms of Recall and print one combination of the parameter which yield the best recall. . with open(&quot;cache/rf_randomcv_500_down.pkl&quot;, &#39;rb&#39;) as f: rf_random = pickle.load(f) pd.DataFrame(rf_random.cv_results_).sort_values(&#39;rank_test_score&#39;).head() . . mean_fit_time std_fit_time mean_score_time std_score_time param_n_estimators param_max_features param_max_depth param_criterion params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score . 100 164.374928 | 0.922007 | 4.703054 | 0.291531 | 300 | None | 25 | entropy | {&#39;n_estimators&#39;: 300, &#39;max_features&#39;: None, &#39;m... | 0.840451 | 0.835183 | 0.833743 | 0.836459 | 0.002883 | 1 | . 130 73.878549 | 6.213109 | 5.713560 | 0.240528 | 100 | None | 25 | entropy | {&#39;n_estimators&#39;: 100, &#39;max_features&#39;: None, &#39;m... | 0.839519 | 0.834167 | 0.833828 | 0.835838 | 0.002607 | 2 | . 21 126.615590 | 2.122243 | 5.434928 | 0.828088 | 200 | None | 33 | entropy | {&#39;n_estimators&#39;: 200, &#39;max_features&#39;: None, &#39;m... | 0.838756 | 0.831201 | 0.832896 | 0.834284 | 0.003237 | 3 | . 143 76.089374 | 0.373065 | 4.953523 | 0.353314 | 100 | None | 31 | entropy | {&#39;n_estimators&#39;: 100, &#39;max_features&#39;: None, &#39;m... | 0.839095 | 0.831201 | 0.831624 | 0.833974 | 0.003626 | 4 | . 215 51.189257 | 0.704618 | 3.211668 | 0.305625 | 100 | None | 23 | gini | {&#39;n_estimators&#39;: 100, &#39;max_features&#39;: None, &#39;m... | 0.837231 | 0.833912 | 0.830523 | 0.833889 | 0.002739 | 5 | . rf_random.best_params_ . {&#39;n_estimators&#39;: 300, &#39;max_features&#39;: None, &#39;max_depth&#39;: 25, &#39;criterion&#39;: &#39;entropy&#39;} . Just like we did on model_default, let&#39;s evaluate whether the tuned model is still overfit or not: . model_tuned_metrics = trainTestEval( rf_random, &quot;Canceled&quot;, X_train_down, X_test, y_train_down, y_test) model_tuned_metrics . accuracy precision recall f1-score observation pos_proportion . Train 0.974494 | 0.967548 | 0.981923 | 0.974682 | 70808 | 0.500000 | . Test 0.856155 | 0.785129 | 0.841427 | 0.812304 | 23713 | 0.369924 | . Threshold Tuning . We can further tune model_tuned by changing the threshold when classifying the probabilities to a class. So far, the model use default threshold which is 0.5, means that if a booking is predicted to have more than 50% chance of cancellation, then it is classified as Canceled, otherwise Not Canceled. We create tuningThresholdPlot function to iteratively move the threshold from 0 to 1 and plot the classification metrics. . def tuningThresholdPlot(model, X, y): tuning_list = [] y_pred_prob = model.predict_proba(X)[:, 1] for threshold in np.linspace(0, 1, 101)[:-1]: y_pred_class = y_pred_prob &gt; threshold report = clfReport(y, y_pred_class) tuning_res = {&quot;threshold&quot;: threshold, &quot;accuracy&quot;: report.loc[&quot;accuracy&quot;][-1]} tuning_res = {**tuning_res, ** report.loc[&quot;Canceled&quot;].to_dict()} # append dicts tuning_res.pop(&quot;support&quot;, None) tuning_list.append(tuning_res) tuning_df = pd.DataFrame(tuning_list) # plotting ax = tuning_df.plot(x=&quot;threshold&quot;, color=&quot;rgyb&quot;, lw=3, figsize=(10, 6)) # vertical line for center diff = abs(tuning_df[&quot;recall&quot;] - tuning_df[&quot;precision&quot;]) thresh_center = tuning_df[diff == min(diff)][&quot;threshold&quot;].values[0] ax.axvline(x=thresh_center, ls=&#39;--&#39;, color=&quot;k&quot;) ax.text(x=thresh_center + 0.01, y=1, s=f&quot;CENTER&quot;, fontsize=12, color=&quot;k&quot;) ax.set_xticks(list(ax.get_xticks()[1:-1]) + [thresh_center]) ax.legend(loc=&quot;upper center&quot;, bbox_to_anchor=( 0.5, -0.15), shadow=True, ncol=4) ax.set_ylabel(&quot;Metric&quot;) plt.title(&quot;TUNING THRESHOLD&quot;, size=15, fontweight=&quot;bold&quot;) plt.show() return tuning_df tuning_thresh = tuningThresholdPlot(rf_random, X_test, y_test) . . The vertical line labeled &quot;CENTER&quot; is when precision equal to recall. As mentioned earlier, we do care about the recall, but on second priority, we have to take into account about the precision too so that our model can correctly classify bookings that are not canceled but predicted as canceled (False Positives). We want to minimize this case in order to reduce the risk of overbooking. In conclusion, we will move the threshold to be less than the CENTER to achieve higher recall, but still maintaining a good precision. Let&#39;s compare the center to the default threshold, which is 0.5. . DEFAULT_THRESHOLD = 0.5 CENTER_THRESHOLD = 0.55 compare_thresh = tuning_thresh[tuning_thresh[&quot;threshold&quot;].isin( [DEFAULT_THRESHOLD, CENTER_THRESHOLD])] compare_thresh . . threshold accuracy precision recall f1-score . 50 0.50 | 0.856155 | 0.785129 | 0.841427 | 0.812304 | . 55 0.55 | 0.862649 | 0.815395 | 0.812699 | 0.814045 | . Let&#39;s calculate gain/loss of the metrics if we use CENTER_THRESHOLD instead of DEFAULT_THRESHOLD . compare_thresh.iloc[1, :] - compare_thresh.iloc[0, :] . . threshold 0.050000 accuracy 0.006494 precision 0.030266 recall -0.028728 f1-score 0.001741 dtype: float64 . . Note: In conclusion, we prefer to use DEFAULT_THRESHOLD instead of CENTER_THRESHOLD in order to maintain recall value. . Receiver Operating Characteristic (ROC) Curve . It is a probability curve which plots True Positive Rate (Recall) against False Positive Rate. The area under the ROC curve, called as Area Under Curve (AUC), measures the degree of classification separability. Higher the AUC, better the model is at distinguishing between canceled and not canceled bookings. . AUC near to 1 indicates the model has good measure of separability. | AUC near to 0 means it has worst measure of separability. | When AUC is 0.5, it means model has no class separation capacity whatsoever. | . def rocauc(y_pred_list, y_test): y_pred_list = [(np.zeros(len(y_test)), &quot;BASELINE&quot;)] + y_pred_list for y_pred, label in y_pred_list: fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) plt.plot(fpr, tpr, label=f&quot;{label} n(AUC: {auc:.3f})&quot;, linestyle=&#39;--&#39; if label == &quot;BASELINE&quot; else &#39;-&#39;) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.title(&quot;RECEIVER OPERATING CHARACTERISTIC (ROC) CURVE&quot;, fontweight=&quot;bold&quot;) plt.legend(bbox_to_anchor=(1, 1)) plt.show() rocauc([(model_default.predict(X_test), &quot;DEFAULT&quot;), (rf_random.predict(X_test), &quot;THRESHOLD = 0.5&quot;), (rf_random.predict_proba(X_test)[:, 1] &gt; CENTER_THRESHOLD, f&quot;THRESHOLD = {CENTER_THRESHOLD}&quot;)], y_test) . . . Note: The AUC of using DEFAULT_THRESHOLD or CENTER_THRESHOLD is nearly the same, indicating the same measure of separability while we retain the recall performance if we use DEFAULT_THRESHOLD. . Feature Importance . This refers to a technique that assign a score to features based on how useful they are at predicting a target variable. In this case, we want to know which feature is the most important in predicting hotel booking cancellation using sklearn built-in attributes feature_importances_. . def featureImportancesPlot(features, importances, n_features=20): feature_imp_df = pd.DataFrame({&#39;Feature&#39;: features, &#39;Importance&#39;: importances}).set_index(&#39;Feature&#39;) .sort_values(&#39;Importance&#39;).tail(n_features) ax = feature_imp_df.plot(kind=&#39;barh&#39;, legend=False, title=f&quot;Top {n_features} Feature Importances&quot;) ax.set_xlabel(&quot;Relative Importance&quot;) plt.show() return feature_imp_df.sort_values(&#39;Importance&#39;, ascending=False) feature_imp = featureImportancesPlot( X.columns, rf_random.best_estimator_.feature_importances_) . . The top three features based on their importance are: . deposit_type_Non Refund: the customer made a deposit to guarantee the booking in the value of the total stay cost | lead_time: number of days that elapsed between the entering date of the booking into the PMS and the arrival date | adr: average daily rate | Conclusion . We successfully visualized the hotel booking data using plotly to gain insights about the data and modeling using Random Forest to predict the cancellation of the booking based on various predictors. Before modeling, we inspect the PPS score to do feature selection manually. Next, we tune the base model using RandomizedSearchCV to prevent overfitting, followed by threshold tuning checking. In the end, we achieve a quite good metrics: . accuracy = 85.6155% | precision = 78.5129% | recall = 84.1427% | f1-score = 81.2304% | . There are several things need to be improved: . Tuning other parameters such as ccp_alpha to cut the tree with cost complexity pruning. | SearchCV using GPU instead of CPU, explore more about cuML packages. | Use another ensemble method such as XGBoost. | .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/tree-based/scikit-learn/2020/05/29/hotel-booking-cancellation-prediction-rf.html",
            "relUrl": "/python/classification/tree-based/scikit-learn/2020/05/29/hotel-booking-cancellation-prediction-rf.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Convolutional Neural Network (CNN) for Casting Product Quality Inspection",
            "content": "Introduction . Casting is a manufacturing process in which liquid material is poured into a mold to solidify. Many types of defects or unwanted irregularities can occur during this process. The industry has its quality inspection department to remove defective products from the production line, but this is very time consuming since it is carried out manually. Furthermore, there is a chance of misclassifying due to human error, causing rejection of the whole product order. . In this post, let us build a model by training top-view images of a casted submersible pump impeller using a Convolutional Neural Network (CNN) so that it can distinguish accurately between defect from the ok one. . We will break down into several steps: . Load the images and apply the data augmentation technique . | Visualize the images . | Training with validation: define the architecture, compile the model, model fitting and evaluation . | Testing on unseen images . | Make a conclusion . | . Note: This post is extracted from Kaggle Notebook: Casting Inspection with Data Augmentation (CNN) . Import Libraries . As usual, before we begin any analysis and modeling, let&#39;s import several necessary libraries to work with the data. . import pandas as pd import numpy as np # visualization import matplotlib.pyplot as plt import seaborn as sns sns.set() # neural network model from keras.callbacks import ModelCheckpoint from keras.layers import * from keras.models import Sequential, load_model from keras.preprocessing.image import ImageDataGenerator # evaluation from sklearn.metrics import confusion_matrix, classification_report . Using TensorFlow backend. . Load the Images . Here is the structure of our folder containing image data: . casting_data ├───test │ ├───def_front │ └───ok_front └───train ├───def_front └───ok_front . The folder casting_data consists of two subfolders test and train in which each of them has another subfolder: def_front and ok_front denoting the class of our target variable. The images inside train will be used for model fitting and validation, while test will be used purely for testing the model performance on unseen images. . Data Augmentation . We apply on-the-fly data augmentation, a technique to expand the training dataset size by creating a modified version of the original image which can improve model performance and the ability to generalize. This can be achieved by using ImageDataGenerator provided by keras with the following parameters: . rotation_range: Degree range for random rotations. We choose 360 degrees since the product is a round object. | width_shift_range: Fraction range of the total width to be shifted. | height_shift_range: Fraction range of the total height to be shifted. | shear_range: Degree range for random shear in a counter-clockwise direction. | zoom_range: Fraction range for random zoom. | horizontal_flip and vertical_flip are set to True for randomly flip image horizontally and vertically. | brightness_range: Fraction range for picking a brightness shift value. | . Other parameters: . rescale: Rescale the pixel values to be in range 0 and 1. | validation_split: Reserve 20% of the training data for validation, and the rest 80% for model fitting. | . train_generator = ImageDataGenerator(rotation_range=360, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, zoom_range=0.05, horizontal_flip=True, vertical_flip=True, brightness_range=[0.75, 1.25], rescale=1./255, validation_split=0.2) . We define another set of value for the flow_from_directory parameters: . IMAGE_DIR: The directory where the image data is stored. | IMAGE_SIZE: The dimension of the image (300 px by 300 px). | BATCH_SIZE: Number of images that will be loaded and trained at one time. | SEED_NUMBER: Ensure reproducibility. . | color_mode=&quot;grayscale&quot;: Treat our image with only one channel color. . | class_mode and classes define the target class of our problem. In this case, we denote the defect class as positive (1), and ok as a negative class. | shuffle=True to make sure the model learns the defect and ok images alternately. | . IMAGE_DIR = &quot;/kaggle/input/real-life-industrial-dataset-of-casting-product/casting_data/&quot; IMAGE_SIZE = (300, 300) BATCH_SIZE = 64 SEED_NUMBER = 123 gen_args = dict(target_size=IMAGE_SIZE, color_mode=&quot;grayscale&quot;, batch_size=BATCH_SIZE, class_mode=&quot;binary&quot;, classes={&quot;ok_front&quot;: 0, &quot;def_front&quot;: 1}, shuffle=True, seed=SEED_NUMBER) train_dataset = train_generator.flow_from_directory(directory=IMAGE_DIR + &quot;train&quot;, subset=&quot;training&quot;, **gen_args) validation_dataset = train_generator.flow_from_directory(directory=IMAGE_DIR + &quot;train&quot;, subset=&quot;validation&quot;, **gen_args) . Found 5307 images belonging to 2 classes. Found 1326 images belonging to 2 classes. . . Important: We must not perform any data augmentation on the test data, since we want to test our model performance when applied on the real scenario. The rescaling is done to ensure mathematical stability when training the model. . test_generator = ImageDataGenerator(rescale=1./255) test_dataset = test_generator.flow_from_directory(directory=IMAGE_DIR + &quot;test&quot;, **gen_args) . Found 715 images belonging to 2 classes. . Image Data Proportion . We successfully load and apply on-the-fly data augmentation according to the specified parameters. Now, let&#39;s take a look on how is the proportion of the train, validation, and test image for each class. . image_data = [{&quot;data&quot;: typ, &quot;class&quot;: name.split(&#39;/&#39;)[0], &quot;filename&quot;: name.split(&#39;/&#39;)[1]} for dataset, typ in zip([train_dataset, validation_dataset, test_dataset], [&quot;train&quot;, &quot;validation&quot;, &quot;test&quot;]) for name in dataset.filenames] image_df = pd.DataFrame(image_data) data_crosstab = pd.crosstab(index=image_df[&quot;data&quot;], columns=image_df[&quot;class&quot;], margins=True, margins_name=&quot;Total&quot;) data_crosstab . . class def_front ok_front Total . data . test 453 | 262 | 715 | . train 3007 | 2300 | 5307 | . validation 751 | 575 | 1326 | . Total 4211 | 3137 | 7348 | . total_image = data_crosstab.iloc[-1, -1] ax = data_crosstab.iloc[:-1, :-1].T.plot(kind=&quot;bar&quot;, stacked=True, rot=0) percent_val = [] for rect in ax.patches: height = rect.get_height() width = rect.get_width() percent = 100*height/total_image ax.text(rect.get_x() + width - 0.25, rect.get_y() + height/2, int(height), ha=&#39;center&#39;, va=&#39;center&#39;, color=&quot;white&quot;, fontsize=10) ax.text(rect.get_x() + width + 0.01, rect.get_y() + height/2, &quot;{:.2f}%&quot;.format(percent), ha=&#39;left&#39;, va=&#39;center&#39;, color=&quot;black&quot;, fontsize=10) percent_val.append(percent) handles, labels = ax.get_legend_handles_labels() ax.legend(handles=handles, labels=labels) percent_def = sum(percent_val[::2]) ax.set_xticklabels([&quot;def_front n({:.2f} %)&quot;.format( percent_def), &quot;ok_front n({:.2f} %)&quot;.format(100-percent_def)]) plt.title(&quot;IMAGE DATA PROPORTION&quot;, fontsize=15, fontweight=&quot;bold&quot;) plt.show() . . We will proceed to the next step, since the proportion of data can be considered as balanced. . Visualize the Image . In this section, we visualize the image to make sure that it is loaded correctly. . Visualize Image in Batch . Visualize the first batch (BATCH_SIZE = 64) of the training dataset (images with data augmentation) and also the test dataset (images without data augmentation). . mapping_class = {0: &quot;ok&quot;, 1: &quot;defect&quot;} def visualizeImageBatch(dataset, title): images, labels = next(iter(dataset)) images = images.reshape(BATCH_SIZE, *IMAGE_SIZE) fig, axes = plt.subplots(8, 8, figsize=(16, 16)) for ax, img, label in zip(axes.flat, images, labels): ax.imshow(img, cmap=&quot;gray&quot;) ax.axis(&quot;off&quot;) ax.set_title(mapping_class[label], size=20) plt.tight_layout() fig.suptitle(title, size=30, y=1.05, fontweight=&quot;bold&quot;) plt.show() return images . . train_images = visualizeImageBatch( train_dataset, &quot;FIRST BATCH OF THE TRAINING IMAGES n(WITH DATA AUGMENTATION)&quot;) . . test_images = visualizeImageBatch( test_dataset, &quot;FIRST BATCH OF THE TEST IMAGES n(WITHOUT DATA AUGMENTATION)&quot;) . . Visualize Detailed Image . Let&#39;s also take a look on the detailed image by each pixel. Instead of plotting 300 pixels by 300 pixels (which computationally expensive), we take a small part of 25 pixels by 25 pixels only. . img = np.squeeze(train_images[4])[75:100, 75:100] fig = plt.figure(figsize=(15, 15)) ax = fig.add_subplot(111) ax.imshow(img, cmap=&quot;gray&quot;) ax.axis(&quot;off&quot;) w, h = img.shape for x in range(w): for y in range(h): value = img[x][y] ax.annotate(&quot;{:.2f}&quot;.format(value), xy=(y, x), horizontalalignment=&quot;center&quot;, verticalalignment=&quot;center&quot;, color=&quot;white&quot; if value &lt; 0.4 else &quot;black&quot;) . . These are the example of values that we are going to feed into our CNN architecture. . Training the Network . As mentioned earlier, we are going to train a CNN model to classify the casting product image. CNN is used as an automatic feature extractor from the images so that it can learn how to distinguish between defect and ok casted products. It effectively uses the adjacent pixel to downsample the image and then use a prediction (fully-connected) layer to solve the classification problem. This is a simple illustration by Udacity on how the layers are arranged sequentially: . . Define Architecture . Here is the detailed architecture that we are going to use: . First convolutional layer: consists of 32 filters with kernel_size matrix 3 by 3. Using 2-pixel strides at a time, reduce the image size by half. | First pooling layer: Using max-pooling matrix 2 by 2 (pool_size) and 2-pixel strides at a time further reduce the image size by half. | Second convolutional layer: Just like the first convolutional layer but with 16 filters only. | Second pooling layer: Same as the first pooling layer. | Flattening: Convert two-dimensional pixel values into one dimension, so that it is ready to be fed into the fully-connected layer. | First dense layer + Dropout: consists of 128 units and 1 bias unit. Dropout of rate 20% is used to prevent overfitting. | Second dense layer + Dropout: consists of 64 units and 1 bias unit. Dropout of rate 20% is also used to prevent overfitting. | Output layer: consists of only one unit and activation is a sigmoid function to convert the scores into a probability of an image being defect. | For every layer except output layer, we use Rectified Linear Unit (ReLU) activation function as follow: . . . Note: ReLU is commonly used as an activation function for image classification. The purpose is to introduce non-linearity to our model so that it can capture non-linear features of an image (example: pixel transition, image borders, colors, etc). . model = Sequential( [ # First convolutional layer Conv2D(filters=32, kernel_size=3, strides=2, activation=&quot;relu&quot;, input_shape=IMAGE_SIZE + (1, )), # First pooling layer MaxPooling2D(pool_size=2, strides=2), # Second convolutional layer Conv2D(filters=16, kernel_size=3, strides=2, activation=&quot;relu&quot;), # Second pooling layer MaxPooling2D(pool_size=2, strides=2), # Flattening Flatten(), # Fully-connected layer Dense(128, activation=&quot;relu&quot;), Dropout(rate=0.2), Dense(64, activation=&quot;relu&quot;), Dropout(rate=0.2), Dense(1, activation=&quot;sigmoid&quot;) ] ) model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 149, 149, 32) 320 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 36, 36, 16) 4624 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 18, 18, 16) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 5184) 0 _________________________________________________________________ dense_1 (Dense) (None, 128) 663680 _________________________________________________________________ dropout_1 (Dropout) (None, 128) 0 _________________________________________________________________ dense_2 (Dense) (None, 64) 8256 _________________________________________________________________ dropout_2 (Dropout) (None, 64) 0 _________________________________________________________________ dense_3 (Dense) (None, 1) 65 ================================================================= Total params: 676,945 Trainable params: 676,945 Non-trainable params: 0 _________________________________________________________________ . Compile the Model . Next, we specify how the model backpropagates or update the weights after each batch feed-forward. We use Adam optimizer and a loss function binary cross-entropy since we are dealing with binary classification problem. The metrics used to monitor the training progress is accuracy. . model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) . Model Fitting . For each epoch, batch_size $ times$ steps_per_epoch images will be fed into our CNN architecture. In this case, we specify the steps_per_epoch to be 150 so for each epoch 64 * 150 = 9600 augmented images from the training dataset will be fed. We let the model train for 25 epochs. . By using ModelCheckpoint, the best model will be automatically saved if the current val_loss is lower than the previous one. . STEPS = 150 checkpoint = ModelCheckpoint(&quot;cnn_casting_inspection_model.hdf5&quot;, verbose=1, save_best_only=True, monitor=&quot;val_loss&quot;) model.fit_generator(generator=train_dataset, validation_data=validation_dataset, steps_per_epoch=STEPS, epochs=25, validation_steps=STEPS, callbacks=[checkpoint], verbose=1) . Epoch 1/25 150/150 [==============================] - 166s 1s/step - loss: 0.6243 - accuracy: 0.6327 - val_loss: 0.5896 - val_accuracy: 0.7033 Epoch 00001: val_loss improved from inf to 0.58961, saving model to cnn_casting_inspection_model.hdf5 Epoch 2/25 150/150 [==============================] - 150s 1000ms/step - loss: 0.5075 - accuracy: 0.7515 - val_loss: 0.4829 - val_accuracy: 0.8062 Epoch 00002: val_loss improved from 0.58961 to 0.48288, saving model to cnn_casting_inspection_model.hdf5 Epoch 3/25 150/150 [==============================] - 151s 1s/step - loss: 0.3580 - accuracy: 0.8378 - val_loss: 0.3844 - val_accuracy: 0.8882 Epoch 00003: val_loss improved from 0.48288 to 0.38443, saving model to cnn_casting_inspection_model.hdf5 Epoch 4/25 150/150 [==============================] - 151s 1s/step - loss: 0.2687 - accuracy: 0.8818 - val_loss: 0.2590 - val_accuracy: 0.9073 Epoch 00004: val_loss improved from 0.38443 to 0.25899, saving model to cnn_casting_inspection_model.hdf5 Epoch 5/25 150/150 [==============================] - 149s 991ms/step - loss: 0.1998 - accuracy: 0.9177 - val_loss: 0.1309 - val_accuracy: 0.9443 Epoch 00005: val_loss improved from 0.25899 to 0.13093, saving model to cnn_casting_inspection_model.hdf5 Epoch 6/25 150/150 [==============================] - 148s 988ms/step - loss: 0.1671 - accuracy: 0.9350 - val_loss: 0.1323 - val_accuracy: 0.9442 Epoch 00006: val_loss did not improve from 0.13093 Epoch 7/25 150/150 [==============================] - 149s 995ms/step - loss: 0.1447 - accuracy: 0.9443 - val_loss: 0.0580 - val_accuracy: 0.9619 Epoch 00007: val_loss improved from 0.13093 to 0.05799, saving model to cnn_casting_inspection_model.hdf5 Epoch 8/25 150/150 [==============================] - 148s 988ms/step - loss: 0.1187 - accuracy: 0.9529 - val_loss: 0.0362 - val_accuracy: 0.9657 Epoch 00008: val_loss improved from 0.05799 to 0.03623, saving model to cnn_casting_inspection_model.hdf5 Epoch 9/25 150/150 [==============================] - 149s 991ms/step - loss: 0.1728 - accuracy: 0.9289 - val_loss: 0.0996 - val_accuracy: 0.9357 Epoch 00009: val_loss did not improve from 0.03623 Epoch 10/25 150/150 [==============================] - 148s 990ms/step - loss: 0.1121 - accuracy: 0.9578 - val_loss: 0.0628 - val_accuracy: 0.9673 Epoch 00010: val_loss did not improve from 0.03623 Epoch 11/25 150/150 [==============================] - 149s 995ms/step - loss: 0.0880 - accuracy: 0.9688 - val_loss: 0.0164 - val_accuracy: 0.9757 Epoch 00011: val_loss improved from 0.03623 to 0.01642, saving model to cnn_casting_inspection_model.hdf5 Epoch 12/25 150/150 [==============================] - 149s 991ms/step - loss: 0.0902 - accuracy: 0.9667 - val_loss: 0.1463 - val_accuracy: 0.9571 Epoch 00012: val_loss did not improve from 0.01642 Epoch 13/25 150/150 [==============================] - 150s 997ms/step - loss: 0.0884 - accuracy: 0.9678 - val_loss: 0.1241 - val_accuracy: 0.9726 Epoch 00013: val_loss did not improve from 0.01642 Epoch 14/25 150/150 [==============================] - 148s 986ms/step - loss: 0.0834 - accuracy: 0.9703 - val_loss: 0.1530 - val_accuracy: 0.9615 Epoch 00014: val_loss did not improve from 0.01642 Epoch 15/25 150/150 [==============================] - 149s 994ms/step - loss: 0.0906 - accuracy: 0.9661 - val_loss: 0.0405 - val_accuracy: 0.9747 Epoch 00015: val_loss did not improve from 0.01642 Epoch 16/25 150/150 [==============================] - 146s 973ms/step - loss: 0.0789 - accuracy: 0.9716 - val_loss: 0.0140 - val_accuracy: 0.9809 Epoch 00016: val_loss improved from 0.01642 to 0.01395, saving model to cnn_casting_inspection_model.hdf5 Epoch 17/25 150/150 [==============================] - 152s 1s/step - loss: 0.0623 - accuracy: 0.9795 - val_loss: 0.1122 - val_accuracy: 0.9791 Epoch 00017: val_loss did not improve from 0.01395 Epoch 18/25 150/150 [==============================] - 148s 984ms/step - loss: 0.0545 - accuracy: 0.9813 - val_loss: 0.1440 - val_accuracy: 0.9848 Epoch 00018: val_loss did not improve from 0.01395 Epoch 19/25 150/150 [==============================] - 150s 999ms/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.0300 - val_accuracy: 0.9849 Epoch 00019: val_loss did not improve from 0.01395 Epoch 20/25 150/150 [==============================] - 149s 994ms/step - loss: 0.0670 - accuracy: 0.9766 - val_loss: 0.0938 - val_accuracy: 0.9814 Epoch 00020: val_loss did not improve from 0.01395 Epoch 21/25 150/150 [==============================] - 148s 984ms/step - loss: 0.0536 - accuracy: 0.9819 - val_loss: 0.0242 - val_accuracy: 0.9854 Epoch 00021: val_loss did not improve from 0.01395 Epoch 22/25 150/150 [==============================] - 152s 1s/step - loss: 0.0507 - accuracy: 0.9843 - val_loss: 0.0155 - val_accuracy: 0.9794 Epoch 00022: val_loss did not improve from 0.01395 Epoch 23/25 150/150 [==============================] - 150s 997ms/step - loss: 0.0574 - accuracy: 0.9823 - val_loss: 0.0657 - val_accuracy: 0.9807 Epoch 00023: val_loss did not improve from 0.01395 Epoch 24/25 150/150 [==============================] - 148s 987ms/step - loss: 0.0535 - accuracy: 0.9826 - val_loss: 0.0534 - val_accuracy: 0.9861 Epoch 00024: val_loss did not improve from 0.01395 Epoch 25/25 150/150 [==============================] - 149s 994ms/step - loss: 0.0545 - accuracy: 0.9821 - val_loss: 0.0111 - val_accuracy: 0.9842 Epoch 00025: val_loss improved from 0.01395 to 0.01115, saving model to cnn_casting_inspection_model.hdf5 . &lt;keras.callbacks.callbacks.History at 0x7f55c3542160&gt; . . Note: The model achieves 98.21% accuracy on training dataset and 98.42% on validation dataset. . Training Evaluation . Let&#39;s plot both loss and accuracy metrics for train and validation data based on each epoch. . plt.subplots(figsize=(8, 6)) sns.lineplot(data=pd.DataFrame( model.history.history, index=range(1, 1+len(model.history.epoch)))) plt.title(&quot;TRAINING EVALUATION&quot;, fontweight=&quot;bold&quot;, fontsize=20) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Metrics&quot;) plt.legend(labels=[&#39;val loss&#39;, &#39;val accuracy&#39;, &#39;train loss&#39;, &#39;train accuracy&#39;]) plt.show() . . . Note: We can conclude that the model is not overfitting the data since both train loss and val loss simultaneously dropped towards zero. Also, both train accuracy and val accuracy increases towards 100%. . Testing on Unseen Images . Our model performs very well on the training and validation dataset which uses augmented images. Now, we test our model performance with unseen and unaugmented images. . best_model = load_model(&quot;/kaggle/working/cnn_casting_inspection_model.hdf5&quot;) y_pred_prob = best_model.predict_generator(generator=test_dataset, verbose=1) . 12/12 [==============================] - 2s 143ms/step . The output of the prediction is in the form of probability. We use THRESHOLD = 0.5 to separate the classes. If the probability is greater or equal to the THRESHOLD, then it will be classified as defect, otherwise ok. . THRESHOLD = 0.5 y_pred_class = (y_pred_prob &gt;= THRESHOLD).reshape(-1,) y_true_class = test_dataset.classes[test_dataset.index_array] pd.DataFrame( confusion_matrix(y_true_class, y_pred_class), index=[[&quot;Actual&quot;, &quot;Actual&quot;], [&quot;ok&quot;, &quot;defect&quot;]], columns=[[&quot;Predicted&quot;, &quot;Predicted&quot;], [&quot;ok&quot;, &quot;defect&quot;]], ) . . Predicted . ok defect . Actual ok 259 | 3 | . defect 1 | 452 | . print(classification_report(y_true_class, y_pred_class, digits=4)) . . precision recall f1-score support 0 0.9962 0.9885 0.9923 262 1 0.9934 0.9978 0.9956 453 accuracy 0.9944 715 macro avg 0.9948 0.9932 0.9940 715 weighted avg 0.9944 0.9944 0.9944 715 . On test dataset, the model achieves a very good result as follow: . Accuracy: 99.44% | Recall: 99.78% | Precision: 99.34% | F1 score: 99.56% | . . Important: According to the problem statement, we want to minimize the case of False Negative, where the defect product is misclassified as ok. This can cause the whole order to be rejected and create a big loss for the company. Therefore, in this case, we prioritize Recall over Precision. But if we take into account the cost of re-casting a product, we have to minimize the case of False Positive also, where the ok product is misclassified as defect. Therefore we can prioritize the F1 score which combines both Recall and Precision. . Visualize the Results . Lastly, we visualize the results by comparing its true label with the predicted label and also provide the probability of each image being on the predicted class. A blue color on the text indicates that our model correctly classify the image, otherwise red color is used. . images, labels = next(iter(test_dataset)) images = images.reshape(BATCH_SIZE, *IMAGE_SIZE) fig, axes = plt.subplots(4, 4, figsize=(16, 16)) for ax, img, label in zip(axes.flat, images, labels): ax.imshow(img, cmap=&quot;gray&quot;) true_label = mapping_class[label] [[pred_prob]] = best_model.predict(img.reshape(1, *IMAGE_SIZE, -1)) pred_label = mapping_class[int(pred_prob &gt;= THRESHOLD)] prob_class = 100*pred_prob if pred_label == &quot;defect&quot; else 100*(1-pred_prob) ax.set_title(f&quot;TRUE LABEL: {true_label}&quot;, fontweight=&quot;bold&quot;, fontsize=18) ax.set_xlabel(f&quot;PREDICTED LABEL: {pred_label} nProb({pred_label}) = {(prob_class):.2f}%&quot;, fontweight=&quot;bold&quot;, fontsize=15, color=&quot;blue&quot; if true_label == pred_label else &quot;red&quot;) ax.set_xticks([]) ax.set_yticks([]) plt.tight_layout() fig.suptitle(&quot;TRUE VS PREDICTED LABEL FOR 16 RANDOM TEST IMAGES&quot;, size=30, y=1.03, fontweight=&quot;bold&quot;) plt.show() . . Since the proportion of correctly classified images is very large, let&#39;s also visualize the misclassified only. . misclassify_pred = np.nonzero(y_pred_class != y_true_class)[0] fig, axes = plt.subplots(2, 2, figsize=(8, 8)) for ax, batch_num, image_num in zip(axes.flat, misclassify_pred // BATCH_SIZE, misclassify_pred % BATCH_SIZE): images, labels = test_dataset[batch_num] img = images[image_num] ax.imshow(img.reshape(*IMAGE_SIZE), cmap=&quot;gray&quot;) true_label = mapping_class[labels[image_num]] [[pred_prob]] = best_model.predict(img.reshape(1, *IMAGE_SIZE, -1)) pred_label = mapping_class[int(pred_prob &gt;= THRESHOLD)] prob_class = 100*pred_prob if pred_label == &quot;defect&quot; else 100*(1-pred_prob) ax.set_title(f&quot;TRUE LABEL: {true_label}&quot;, fontweight=&quot;bold&quot;, fontsize=18) ax.set_xlabel(f&quot;PREDICTED LABEL: {pred_label} nProb({pred_label}) = {(prob_class):.2f}%&quot;, fontweight=&quot;bold&quot;, fontsize=15, color=&quot;blue&quot; if true_label == pred_label else &quot;red&quot;) ax.set_xticks([]) ax.set_yticks([]) plt.tight_layout() fig.suptitle(f&quot;MISCLASSIFIED TEST IMAGES ({len(misclassify_pred)} out of {len(y_true_class)})&quot;, size=20, y=1.03, fontweight=&quot;bold&quot;) plt.show() . . Out of 715 test images, only 4 images are being misclassified. . Conclusion . By using CNN and on-the-fly data augmentation, the performance of our model in training, validation, and test images is almost perfect, reaching 98-99% accuracy and F1 score. We can utilize this model by embedding it into a surveillance camera where the system can automatically separate defective product from the production line. This method surely can reduce human error and human resources on manual inspection, but it still needs supervision from human since the model is not 100% correct at all times. .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/computer-vision/keras/2020/05/02/casting-inspection-cnn.html",
            "relUrl": "/python/classification/computer-vision/keras/2020/05/02/casting-inspection-cnn.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Understanding Word2Vec with Gensim and Elang",
            "content": ". . Note: This post is an extension of my first internal training at Algoritma. Visit my GitHub Repository for full content. . Packages . First, you need to install additional packages into your environment: . pip install elang pip install contractions pip install PyDrive . Next, we create a connection between Google Colab and Drive. Please go through the authentication step then download the file using the Google File ID. . from pydrive.auth import GoogleAuth from pydrive.drive import GoogleDrive from google.colab import auth from oauth2client.client import GoogleCredentials auth.authenticate_user() gauth = GoogleAuth() gauth.credentials = GoogleCredentials.get_application_default() drive = GoogleDrive(gauth) . . # you can replace the id with id of file you want to access model_id = &quot;1vXsf0DI8HKGuqYFIGIrvG-73KzZPgz5h&quot; downloaded = drive.CreateFile({&#39;id&#39;:model_id}) downloaded.GetContentFile(&#39;wiki_Animal.model&#39;) . Import necessary Python packages for the content. . import pandas as pd import numpy as np # text processing import nltk nltk.download([&quot;brown&quot;, &quot;stopwords&quot;]) from nltk.corpus import brown from nltk.probability import FreqDist # visualization import matplotlib.pyplot as plt # other packages from itertools import combinations from tqdm import tqdm import warnings warnings.filterwarnings(&#39;ignore&#39;) . [nltk_data] Downloading package brown to /root/nltk_data... [nltk_data] Package brown is already up-to-date! [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! . Introduction to Word Embedding . What does it mean? . Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. It is just a fancy way of saying numerical representation of words. A good analogy would be how we use the RGB representation for colors. . Why do we need them? . As a human, it doesn’t make much sense in wanting to represent words using numbers because numbers are used for quantification and why would one need to quantify words? . The answer to that is, we want to quantify the semantics. We want to represent words in such a manner that it captures its meaning in a way humans do. Not the exact meaning of the word but a contextual one. For example, when I say the word &quot;see&quot;, we know exactly what action — the context — I’m talking about, even though we might not be able to quote its meaning, the kind we would find in a dictionary, of the top of our head. . One-hot Vectors . The simplest word embedding you can have is using one-hot vectors. If you have $n$ words in your vocabulary, then you can represent each word as a $1 times n$ vector. . For a simple example, if we have 3 words — dog, cat, hat — in our vocabulary then we can represent them as following: . dog [1, 0, 0] cat [0, 1, 0] hat [0, 0, 1] . Problems: . Curse of dimensionality: Size of vectors depends on the size of our vocabulary (which can be huge). This is a wastage of space and increases algorithm complexity exponentially. | Transfer learning would be impossible if we add/remove words from the vocabulary, as it would require to re-train the whole model again. | This representation fails to capture the contextual meaning of the words. There is no correlation between words that have similar meaning or usage. | Using one-hot vectors, Similarity(dog, cat) == Similarity(dog, hat) == 0. But in an ideal situation, Similarity(dog, cat) &gt;&gt; Similarity(dog, hat) . Count-based Embedding . This is one type of word embeddings, based on the count of each words. One of them is a count vector. . Count Vector . Count vector model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider we have $D$ documents and $T$ is the number of different words in our vocabulary then the size of count vector matrix will be given by $D times T$. Let’s take the following two sentences: . Document 1: &quot;The cat sat on the hat&quot; Document 2: &quot;The dog ate the cat and the hat&quot; . then the count vector matrix is: . . The Drawback . Count-based language modeling is easy to comprehend — related words are observed (counted) together more often than unrelated words. Many attempts were made to improve the performance of the model to the state-of-art, using SVD, ramped window, and non-negative matrix factorization, but the model did not do well in capturing complex relationships among words. . Then, the paradigm started to change in 2013, when Thomas Mikolov proposed the prediction-based modeling technique, called Word2Vec. Unlike counting word co-occurrences, the model uses neural networks to learn intelligent representation of words in a vector space. Then, the paper Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors, quantified &amp; compared the performances of count-based vs prediction-based models. . . The blue bars represent the count-based models, and the red bars are for prediction-based models. Long story short, prediction-based models outperformed count-based models by a large margin on various language tasks. . Word Vectors Intuition . Consider a small subset of English: words for animals. Our task is to be able to write computer programs to find similarities among these words and the creatures they designate. To do this, we might start by making a spreadsheet of some animals and their characteristics. For example: . animal = pd.DataFrame({ &quot;animal&quot;: [&quot;kitten&quot;, &quot;hamster&quot;, &quot;tarantula&quot;, &quot;puppy&quot;, &quot;crocodile&quot;, &quot;dolphin&quot;, &quot;panda bear&quot;, &quot;lobster&quot;, &quot;capybara&quot;, &quot;elephant&quot;, &quot;mosquito&quot;, &quot;goldfish&quot;, &quot;horse&quot;, &quot;chicken&quot;], &quot;cuteness&quot;: [95, 80, 8, 90, 5, 60, 75, 2, 70, 65, 1, 25, 50, 25], &quot;size&quot;: [15, 8, 3, 20, 40, 45, 40, 15, 30, 90, 1, 2, 50, 15] }).set_index(&quot;animal&quot;) animal . . cuteness size . animal . kitten 95 | 15 | . hamster 80 | 8 | . tarantula 8 | 3 | . puppy 90 | 20 | . crocodile 5 | 40 | . dolphin 60 | 45 | . panda bear 75 | 40 | . lobster 2 | 15 | . capybara 70 | 30 | . elephant 65 | 90 | . mosquito 1 | 1 | . goldfish 25 | 2 | . horse 50 | 50 | . chicken 25 | 15 | . Euclidean Distance vs Cosine Similarity . DataFrame animals give us information we need to make determinations about which animals are similar (at least, similar in the properties that we&#39;ve included in the data). Try to answer the following question: Which animal is most similar to a capybara? . . You could go through the values one by one and do the math to make that evaluation, but visualizing the data as points in 2-dimensional space makes finding the answer very intuitive: . . The plot shows us that the closest animal to the capybara is the panda bear (in terms of its subjective size and cuteness). One way of calculating how &quot;far apart&quot; two points are is to find their Euclidean distance. . Using Euclidean distance might be fine for a lower dimension, how about we contrast it with this case: . Let&#39;s say you are in an e-commerce setting and you want to compare users for product recommendations. . User 1 bought 1x eggs, 1x flour, and 1x sugar User 2 bought 100x eggs, 100x flour, and 100x sugar User 3 bought 1x sugar, 1x Vodka, and 1x Red Bull . purchase = pd.DataFrame({ &quot;user&quot;: [&quot;user 1&quot;, &quot;user 2&quot;, &quot;user 3&quot;], &quot;eggs&quot;: [1, 100, 0], &quot;flour&quot;: [1, 100, 0], &quot;sugar&quot;: [1, 100, 1], &quot;vodka&quot;: [0, 0, 1], &quot;red bull&quot;: [0, 0, 1] }).set_index(&quot;user&quot;) purchase . . eggs flour sugar vodka red bull . user . user 1 1 | 1 | 1 | 0 | 0 | . user 2 100 | 100 | 100 | 0 | 0 | . user 3 0 | 0 | 1 | 1 | 1 | . for a, b in combinations(purchase.index, 2): dist = np.linalg.norm(purchase.loc[a]-purchase.loc[b]) print(f&quot;Euclidean Distance ({a}, {b}): {dist}&quot;) . . Euclidean Distance (user 1, user 2): 171.47302994931886 Euclidean Distance (user 1, user 3): 2.0 Euclidean Distance (user 2, user 3): 172.63545406433755 . Problem occurs when we use Euclidean distance, user 3 is more similar to user 1. In fact, user 2 is more similar to user 1 in terms of purchase behavior. The solution is to use Cosine Similarity. . . Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. Word vectors with similar context occupy close spatial positions; the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0. The smaller the angle, higher the cosine similarity. . def cosine_similarity(x, y): dot_products = np.dot(x, y.T) norm_products = np.linalg.norm(x) * np.linalg.norm(y) return dot_products / norm_products for a, b in combinations(purchase.index, 2): sim = cosine_similarity(purchase.loc[a], purchase.loc[b]) print(f&quot;Cosine Similarity ({a}, {b}): {sim}&quot;) . . Cosine Similarity (user 1, user 2): 1.0 Cosine Similarity (user 1, user 3): 0.33333333333333337 Cosine Similarity (user 2, user 3): 0.3333333333333333 . Interesting Properties . Back to our original illustration: . . Modeling animals in this way has a few other interesting properties: . Most Similar Point | We can pick an arbitrary point in &quot;animal space&quot; and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken. . Average Point | Reasoning visually, you can also answer questions like &quot;what&#39;s halfway between a chicken and an elephant?&quot; Simply draw a line from &quot;elephant&quot; to &quot;chicken,&quot; mark off the midpoint and find the closest animal. According to our chart, halfway between an elephant and a chicken is a horse. . Analogous relationship | You can also ask: what&#39;s the difference between a hamster and a tarantula? According to our plot, it&#39;s about seventy five units of cute (and a few units of size). The relationship of &quot;difference&quot; is an interesting one, because it allows us to reason about analogous relationships. In the chart below, I&#39;ve drawn an arrow from &quot;tarantula&quot; to &quot;hamster&quot; (in blue): . . You can understand this arrow as being the relationship between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I&#39;ve also transposed this same arrow (this time in red) so that its origin point is &quot;chicken.&quot; The arrow ends closest to &quot;kitten.&quot; What we&#39;ve discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy: . Tarantulas are to hamsters as chickens are to kittens. . Word2Vec . General Architecture . Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google. . A prerequisite for any neural network or any supervised training technique is to have labeled training data. How do you a train a neural network to predict word embedding when you don’t have any labeled data i.e words and their corresponding word embedding? . We’ll do that by creating a so-called “fake” task for the neural network to train. We won’t be interested in the inputs and outputs of this network, rather the goal is actually just to learn the weights of the hidden layer that are actually the “word vectors” that we’re trying to learn. . Let us look deeper into the Word2Vec architecture: . . Details: . The input layer is a one hot encoded vector of size $V$ (vocabulary size). | $W_{V times N}$ is the weight matrix that projects the input $x$ to the hidden layer. These values are the word vectors. | The hidden layer contains $N$ neurons (hyperparameter), it just copy the weighted sum of inputs to the next layer. There is no activation function like sigmoid, tanh or ReLU. | $W&#39;_{N times V}$ is the weight matrix that maps the hidden layer outputs to the final output layer. | The output layer is again a $V$ length vector, with softmax activation function which is a function that turn numbers, aka logits, into probabilities that sum to one. | . There are two flavors of Word2Vec in which both are using the same architecture: Skip-Gram and Continuous Bag-Of-Words (CBOW). For each of them, we will be considering this example: . Let&#39;s say we have a sentence: . I love to drink orange juice . and we want to generate the training samples for the input and output words with window size is equal to 2. The illustration of sliding window is given by the picture below. . . The word highlighted in yellow is the target/center word and the words highlighted in green are its context/neighboring words. . Skip-Gram . Skip-Gram Training Samples . The fake task for Skip-gram model would be: Given a target word, we’ll try to predict its context words. Here are the list of training samples generated from source text if we use skip-gram: . (&#39;I&#39;, &#39;love&#39;), (&#39;I&#39;, &#39;to&#39;), (&#39;love&#39;, &#39;I&#39;), (&#39;love&#39;, &#39;to&#39;), (&#39;love&#39;, &#39;drink&#39;), (&#39;to&#39;, &#39;I&#39;), (&#39;to&#39;, &#39;love&#39;), (&#39;to&#39;, &#39;drink&#39;), (&#39;to&#39;, &#39;orange&#39;), (&#39;drink&#39;, &#39;love&#39;), (&#39;drink&#39;, &#39;to&#39;), (&#39;drink&#39;, &#39;orange&#39;), (&#39;drink&#39;, &#39;juice&#39;), (&#39;orange&#39;, &#39;to&#39;), (&#39;orange&#39;, &#39;drink&#39;), (&#39;orange&#39;, &#39;juice&#39;), (&#39;juice&#39;, &#39;drink&#39;), (&#39;juice&#39;, &#39;orange&#39;) . Skip-Gram Architecture . We breakdown the way this model works in these steps: . Convert the generated training samples into one hot vectors $x$ (center word) for the input layer and $y_1, y_2, ..., y_C$ (context words) for the output, where each is a vector of $V$-dimension. | Multiply input layer with $W_{V times N}$ to get the embedded vectors of size $N$. | The embedded vectors is then multiplied with $W&#39;_{N times V}$ to get the logit scores of size $V$. | Apply softmax function to turn the scores into probabilities, we get $ hat{y}$. | Error between output and each context word is calculated as follows: $ sum limits_{i=1}^C {( hat{y} - y_i)}$ | Backpropagation to re-adjust the weights, by using Gradient Descent or other optimizer. a. All weights in output matrix will be updated. b. Only corresponding word vector in the input matrix that will be updated. | . Note: Detail of the backpropagation process is provided here. . Continuous Bag-Of-Words (CBOW) . CBOW Training Samples . The fake task in CBOW is somewhat similar to Skip-gram, in the sense that we still take a pair of words and teach the model that they co-occur but instead it is learning to predict the target word by the context words. . (&#39;love&#39;, &#39;I&#39;), (&#39;to&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;love&#39;), (&#39;to&#39;, &#39;love&#39;), (&#39;drink&#39;, &#39;love&#39;), (&#39;I&#39;, &#39;to&#39;), (&#39;love&#39;, &#39;to&#39;), (&#39;drink&#39;, &#39;to&#39;), (&#39;orange&#39;, &#39;to&#39;), (&#39;love&#39;, &#39;drink&#39;), (&#39;to&#39;, &#39;drink&#39;), (&#39;orange&#39;, &#39;drink&#39;), (&#39;juice&#39;, &#39;drink&#39;), (&#39;to&#39;, &#39;orange&#39;), (&#39;drink&#39;, &#39;orange&#39;), (&#39;juice&#39;, &#39;orange&#39;), (&#39;drink&#39;, &#39;juice&#39;), (&#39;orange&#39;, &#39;juice&#39;) . CBOW Architecture . We breakdown the way this model works in these steps: . Convert the generated training samples into one hot vectors $x_1, x_2, ..., x_C$ (context word) for the input layer. So, the size is $C times V$ | Multiply all vector $x$ with $W_{V times N}$ and then take the sum or mean of embedded vectors. | The hidden layer is then multiplied with $W&#39;_{N times V}$ to get the logit scores of size $V$. | Apply softmax function to turn the scores into probabilities, we get $ hat{y}$. | Error between output and each context word is calculated as follows: ${( hat{y} - y)}$ | Back-propagation to re-adjust the weights, by using Gradient Descent or other optimizer. Just like Skip-gram: a. All weights in output matrix will be updated. b. Only corresponding word vector in the input matrix that will be updated. | . Note: Visit Word Embedding Visual Inspector (wevi) to get an intuitive interpretation on how a neural word embedding is trained. Hereby, I provide a function to convert sentences into syntax that wevi accepts as input: . def wevi_input(sentence, window=2, sg=1): sentence_list = sentence.split() result = [] for idx, target in enumerate(sentence_list): context = [] for w in range(idx-window, idx+window+1): if w &lt; 0 or w == idx: continue try: context.append(sentence_list[w]) except: continue if sg: # Skip-gram result.append(target + &#39;|&#39; + &#39;^&#39;.join(context)) else: # CBOW result.append(&#39;^&#39;.join(context) + &#39;|&#39; + target) return &#39;,&#39;.join(result) # specify input sentences_list = [&quot;I love the color blue&quot;, &quot;I love to eat oranges&quot;] window = 3 sg = 1 # skip-gram (1) or CBOW (other than 1) # print input text for wevi wevi_input_txt = &#39;,&#39;.join([wevi_input(s, window, sg) for s in sentences_list]) print(wevi_input_txt) . I|love^the^color,love|I^the^color^blue,the|I^love^color^blue,color|I^love^the^blue,blue|love^the^color,I|love^to^eat,love|I^to^eat^oranges,to|I^love^eat^oranges,eat|I^love^to^oranges,oranges|love^to^eat . Training Optimization . Up until this point, you should have understand how the Word2Vec works. But, there is an issue with the softmax function — it is computationally very expensive, as it requires scanning through the entire output embedding matrix to compute the probability distribution of all V words, where V can be millions or more. If we look the softmax function below: . $softmax(y_i) = dfrac{e^{y_i}}{ sum limits_{y=1}^V e^{y_j}}$ . The normalization factor in the denominator also requires $V$ iterations. When implemented in codes, the normalization factor is computed only once and cached as a Python variable, making the algorithm complexity $O(V)$. . Let&#39;s assume that the training corpus has $V = 10,000$ and the hidden layer is 300-dimensional. This means that there are $3,000,000$ neurons in the output weight matrix that need to be updated for each training batch. . Negative Sampling . Due to this computational inefficiency, softmax function is preferably not used in most implementations of Word2Vec. Instead we use an alternative called negative sampling with sigmoid function, which rephrases the problem into a set of independent binary logistic classification task of algorithm complexity = $O(K+1)$, where $K$ is the number of negative samples and $1$ is the positive sample. . . Tip: Mikolov suggest to use $K$ in range $[5, 20]$ for small vocabulary and $[2, 5]$ for a larger vocabulary. . How does it works? . The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned Let us take the previous sentence: . I love to drink orange juice . Assume we are training on a Skip-Gram model and the training pairs is (juice, drink). With $K = 3$, we will have a set of sample like below:| CENTER | CONTEXT | LABEL || | | | | juice | drink | positive (1) | | juice | regression | negative (0) | | juice | orange | negative (0) | | juice | school | negative (0) | . Now, we only update the output matrix for these context word during training and ignore the other weights. . Notice that (juice, orange) is labeled as negative sample when the model is training on (juice, drink). It is okay if by chance this case happens, because on the next iteration (juice, orange) will be considered as positive sample. . How the negative samples are drawn? . Negative samples are drawn randomly from a noise distribution. . $P(Wi) = dfrac{f(W_i)^{p}}{ sum limits_{j=1}^V {f(W_j)^{p}}}$ . Notice: . $p = 1$ means the negative samples are drawn from the word frequency distribution. But the problem is that frequent word will be more likely to be drawn as negative samples. | $p = 0$ means the negative samples are drawn from a uniform distribution. But this doesn&#39;t represent the exact word distribution of an English word. | . Instead, we use $p = 0.75$ so that: . Frequent words have smaller probability to be sampled. | Rare words have larger probability to be sampled. | . Let&#39;s use The Brown Corpus from nltk to illustrate the noise distribution. It was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. . brown_reviews = brown.words(categories=&quot;reviews&quot;) # lower case and remove punctuation brown_reviews_lower = [w.lower() for w in brown_reviews if w.isalnum()] brown_reviews_lower[:10] . [&#39;it&#39;, &#39;is&#39;, &#39;not&#39;, &#39;news&#39;, &#39;that&#39;, &#39;nathan&#39;, &#39;milstein&#39;, &#39;is&#39;, &#39;a&#39;, &#39;wizard&#39;] . Comparison of the distribution using different values of $ alpha$ is given below: . def generateWordFreqProbTable(text_list, p=1, ntop=25): fd = FreqDist(text_list) word_freq = pd.DataFrame(list(fd.items()), columns=[&quot;Word&quot;, &quot;Frequency&quot;]).sort_values(by=&#39;Frequency&#39;, ascending=False).head(ntop) word_freq[&quot;Probability&quot;] = (word_freq[&quot;Frequency&quot;]**p)/(word_freq[&quot;Frequency&quot;]**p).sum() return word_freq # visualization fig, axes = plt.subplots(1, 3, figsize=(15, 5)) compare_p = [1, 0.75, 0] compare_df = [generateWordFreqProbTable(brown_reviews_lower, alpha) for alpha in compare_p] for idx, (ax, df) in enumerate(zip(axes, compare_df)): df.plot(kind=&quot;bar&quot;, x=&quot;Word&quot;, y=&quot;Probability&quot;, ax=ax) ax.get_legend().remove() ax.set_ylim((0, 0.01 + compare_df[0].iloc[0][&quot;Probability&quot;])) ax.set_ylabel(&quot;Probability&quot;) ax.set_title(f&quot;p = {compare_p[idx]}&quot;) plt.tight_layout() fig.suptitle(&quot;NOISE DISTRIBUTION FOR NEGATIVE SAMPLING&quot;, size=25, y=1.1, fontweight=&quot;bold&quot;) plt.show() . . Hierarchical Softmax . Okay, with Negative Sampling we only updating certain weights in the output matrix independently, which loses the softmax behaviour. But how if we still want the model to learn the softmax behaviour? . Hierarchical softmax is a technique to approximate the value of softmax and preserve its behaviour. Output weights are now organized in a structure of Huffman Coding Tree. . . It is a full binary tree with the following characteristics: . Every internal node has exactly two branches of child node. The internal nodes are depicted with blue circles. | The leaf node is the vocabulary word, depicted with orange circles. | The number of internal node is always one less than the number of leaf node. | The leaf nodes are ordered such that frequent words are located near the root node. | . The approximated value will be less accurate, but the computational cost is more efficient. It reduces the complexity from $O(V)$ to $O(log_2{V})$. . . Note: To understand the construction of Huffman Coding Tree, see Huffman Coding Visualization. Note that the leaf node is a character instead of a word. Try input the text aaaabbbbcccdddefgh to get a similar structure as the picture above. . How does the training works? . The input matrix is still the same as a regular Word2Vec architecture. The main difference is on the output matrix. We treat each internal node as a column in the output matrix. Hence, the size of output matrix will be $N times (V-1)$. . Suppose using the above huffman tree, we train a Skip-gram using the sample (chupacabra, active). . . The steps are: . Take the $N$-dimensional vector of hidden layer of the word chupacabra. | Travel along the huffman tree to locate the word active. | For each traversed node: Compute the logit score, which is the dot product of the word vector with the corresponding column of output matrix. | Apply sigmoid activation function to get a probability. We treat this as $ hat{y}$. | The traversed branch/edge is treated as $y$. | . | Compute error and backpropagate, just like the original architecture. | Hyperparameter Tuning . The hyperparameter choice is crucial for model performance, both speed and accuracy. However it varies for different applications. According to this Google Code Archive, the main choices to make are: . Architecture: . Skip-gram: better for rare words and small training set but slower | CBOW: faster but bias toward frequent words | | Window size: . Skip-gram usually around 10 | CBOW around 5 | | Word vectors dimensionality: usually more is better, but not always . | Training optimization: . Hierarchical softmax: better for infrequent words | Negative sampling: better for frequent words, better with low dimensional vectors | | Sub-sampling frequent words: improve both accuracy and speed for large data sets (useful values are in range 0.1% to 0.001%) . | . Practical Word2Vec using Gensim and Elang on Wikipedia Articles . Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community. . . Note: For more information: Gensim Word2Vec Documentation . Developed by Samuel Chan and me, Elang is an acronym that combines the phrases Embedding (E) and Language (Lang) Models. Its goal is to help NLP (natural language processing) researchers, Word2Vec practitioners, educators and data scientists be more productive in training language models and explaining key concepts in word embeddings. . . Note: For more information: Elang Github Documentation . Step 1. Gather Text Data . Scrape Wikipedia articles by using build_from_wikipedia function, saved it to a .txt file. | Load the .txt file as list of sentences. One sentence represent the content of one article. | from elang.word2vec.builder import build_from_wikipedia build_from_wikipedia( slug=&quot;Animal&quot;, # scrape with query levels=1, # levels of articles lang=&quot;en&quot;, # english language model=False, # don&#39;t create model save=True) # save to txt file . Level 1 Queried so far: [&#39;Choanoflagellata&#39;, &#39;Gastropod&#39;, &#39;Yondelis&#39;, &#39;Spicule_(sponge)&#39;, &#39;Predator&#39;, &#39;Big_bug_movie&#39;, &#39;Aerobic_respiration&#39;, &#39;Bivalve&#39;, &#39;Working_animals&#39;, &#39;Crustacea&#39;, &#39;Helminths&#39;, &#39;Porifera&#39;, &#39;Placozoan&#39;, &#39;Late_Cambrian&#39;, &#39;Biological_tissue&#39;, &#39;Burgess_shale&#39;, &#39;Tardigrada&#39;, &#39;Mollusc&#39;, &#39;Hemichordata&#39;, &#39;Gastrula&#39;, &#39;Protostomes&#39;, &#39;Fossil_record&#39;, &#39;Homeodomain&#39;, &#39;Hierarchical&#39;, &#39;Multicellular&#39;, &#39;Eukaryotic&#39;, &#39;Signs_of_the_zodiac&#39;, &#39;Jean-Baptiste_de_Lamarck&#39;, &#39;Amphibia&#39;, &#39;Land_plant&#39;, &#39;Classical_era&#39;, &#39;Scala_naturae&#39;, &#39;Prokaryotic&#39;, &#39;Nematodes&#39;, &#39;Vertebrates&#39;, &#39;Unicellular&#39;, &#39;Alga&#39;, &#39;Animal&#39;, &#39;Zoophytes&#39;, &#39;Haploid&#39;, &#39;Nerve_tissue&#39;, &#39;Praying_mantis&#39;, &#39;Protista&#39;, &#39;TGF-beta&#39;, &#39;Philosophie_Zoologique&#39;, &#39;Sinew&#39;, &#39;Extant_taxon&#39;, &#39;Canary_(bird)&#39;, &#39;Animal_attacks&#39;, &#39;Feeding_behaviour&#39;, &#39;Cirripede&#39;, &#39;Consumer-resource_systems&#39;, &#39;Embryogenesis&#39;, &#39;Rouphozoa&#39;, &#39;Deuterostomes&#39;, &#39;Diploblastic&#39;, &#39;Parasite&#39;, &#39;Triploblastic&#39;, &#39;Blowdart&#39;, &#39;Cilia&#39;, &#39;Cephalisation&#39;, &#39;Titanosaur&#39;, &#39;Insecta&#39;, &#39;Sense_organ&#39;] Article content successfully saved to wikipedia_branch_Animal_1.txt . wiki_file = open(&quot;/content/corpus/txt/wikipedia_branch_Animal_1.txt&quot;, &quot;r&quot;) wiki_text = wiki_file.readlines() . Step 2. Preprocessing . contractions.fix(): Replace contractions by expanding it. Example: I&#39;m become I am. | simple_preprocess(): Remove numbers, punctuation, whitespace then return as lowercase tokens in list. | Remove stopwords, this is optional depends on the case. | Demonstrate what each function does with a sample sentence: . example_sentence = &quot;I&#39;ll arrive at the café around 9 PM. See you there! :)&quot; print(example_sentence) # contractions.fix() import contractions sent = contractions.fix(example_sentence) print(sent) # simple_preprocess() from gensim.utils import simple_preprocess sent_list = simple_preprocess(sent, deacc = True) # list of tokens print(&#39; &#39;.join(sent_list)) # optional: Remove stopwords (with the help of nltk) from nltk.corpus import stopwords stopword_list = stopwords.words(&quot;english&quot;) clean_sent_list = [word for word in sent_list if word not in stopword_list] print(&#39; &#39;.join(clean_sent_list)) . I&#39;ll arrive at the café around 9 PM. See you there! :) I will arrive at the café around 9 PM. See you there! :) will arrive at the cafe around pm see you there arrive cafe around pm see . Let&#39;s clean our wiki_text (list of sentences) without removing the stopwords. . clean_wiki_text = list(map(contractions.fix, tqdm(wiki_text))) # clean_wiki_text = list(map(simple_preprocess, clean_wiki_text)) # with no additional parameter clean_wiki_text = [simple_preprocess(sentence, deacc = True) for sentence in tqdm(clean_wiki_text)] # with additional parameter . Step 3. Model Training . Don&#39;t worry about the code below. We are setting up the logging settings to monitor the training process. . from gensim.models.callbacks import CallbackAny2Vec from time import time import logging logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO) class Callback(CallbackAny2Vec): def __init__(self): self.epoch = 1 def on_epoch_end(self, model): loss = model.get_latest_training_loss() if self.epoch == 1: print(&#39;Loss after epoch {}: {}&#39;.format(self.epoch, loss)) else: print(&#39;Loss after epoch {}: {}&#39;.format( self.epoch, loss - self.loss_previous_step)) self.epoch += 1 self.loss_previous_step = loss . Separate the training into three distinctive steps: . Word2Vec(): Set up the parameters of the model one-by-one and leave the model uninitialized. . | .build_vocab(): Builds the vocabulary from a sequence of sentences and thus initialized the model. . | .train(): Finally, trains the model. The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously. . | from gensim.models import Word2Vec model = Word2Vec( size=300, # dimensionality of the word vectors window=5, # max distance between context and target word min_count=10, # frequency cut-off sg=1, # skip-gram = 1, CBOW = 0 cbow_mean=0, # only applies to CBOW. Use 0 for sum, or 1 for mean of the word vectors hs=1, # using hierarchical softmax negative=20, # negative sampling will not be used, since hs is activated ns_exponent=0.75, # reshape the noise distribution of negative sampling (p) alpha=0.005, # backpropagation learning rate seed=123, # reproducibility workers=1) print(model) . Word2Vec(vocab=0, size=300, alpha=0.005) . logging.disable(logging.NOTSET) # enable logging t = time() model.build_vocab(clean_wiki_text, progress_per=100) print(&#39;Time to build vocab: {} seconds&#39;.format(round((time() - t), 2))) . 2020-04-21 08:42:38,065 : INFO : collecting all words and their counts 2020-04-21 08:42:38,066 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types 2020-04-21 08:42:38,107 : INFO : collected 17525 word types from a corpus of 187489 raw words and 64 sentences 2020-04-21 08:42:38,110 : INFO : Loading a fresh vocabulary 2020-04-21 08:42:38,207 : INFO : effective_min_count=10 retains 2450 unique words (13% of original 17525, drops 15075) 2020-04-21 08:42:38,208 : INFO : effective_min_count=10 leaves 153644 word corpus (81% of original 187489, drops 33845) 2020-04-21 08:42:38,221 : INFO : deleting the raw counts dictionary of 17525 items 2020-04-21 08:42:38,222 : INFO : sample=0.001 downsamples 46 most-common words 2020-04-21 08:42:38,222 : INFO : downsampling leaves estimated 109736 word corpus (71.4% of prior 153644) 2020-04-21 08:42:38,228 : INFO : constructing a huffman tree from 2450 words 2020-04-21 08:42:38,281 : INFO : built huffman tree with maximum node depth 14 2020-04-21 08:42:38,286 : INFO : estimated required memory for 2450 words and 300 dimensions: 10535000 bytes 2020-04-21 08:42:38,286 : INFO : resetting layer weights . Time to build vocab: 0.75 seconds . logging.disable(logging.INFO) # disable logging callback = Callback() # instead, print out loss for each epoch t = time() model.train(clean_wiki_text, total_examples=model.corpus_count, # count of sentences epochs=10, # number of iterations over the corpus, compute_loss=True, # to track model loss callbacks=[callback]) print(&#39;Time to train the model: {} seconds&#39;.format(round((time() - t), 2))) . Loss after epoch 1: 8712558.0 Loss after epoch 2: 8204748.0 Loss after epoch 3: 4327134.0 Loss after epoch 4: 4215556.0 Loss after epoch 5: 4094404.0 Loss after epoch 6: 3943968.0 Loss after epoch 7: 2950096.0 Loss after epoch 8: 2920060.0 Loss after epoch 9: 2927080.0 Loss after epoch 10: 2943032.0 Time to train the model: 50.64 seconds . print(model) . Word2Vec(vocab=2450, size=300, alpha=0.005) . We can save the trained model using .save() method . . Load the pre-trained model using .load() method. I trained the model using wikipedia_branch_Animal_3.txt for a good 1.5 - 2 hours. . from gensim.models import Word2Vec model = Word2Vec.load(&quot;wiki_Animal.model&quot;) print(model) . 2020-04-24 07:18:47,535 : INFO : loading Word2Vec object from wiki_Animal.model 2020-04-24 07:18:48,261 : INFO : loading wv recursively from wiki_Animal.model.wv.* with mmap=None 2020-04-24 07:18:48,262 : INFO : setting ignored attribute vectors_norm to None 2020-04-24 07:18:48,263 : INFO : loading vocabulary recursively from wiki_Animal.model.vocabulary.* with mmap=None 2020-04-24 07:18:48,264 : INFO : loading trainables recursively from wiki_Animal.model.trainables.* with mmap=None 2020-04-24 07:18:48,267 : INFO : setting ignored attribute cum_table to None 2020-04-24 07:18:48,268 : INFO : loaded wiki_Animal.model . Word2Vec(vocab=23190, size=300, alpha=0.005) . Step 4. Visualize . The visualization can be useful to understand how Word2Vec works and how to interpret relations between vectors captured from your texts before using them in other machine learning algorithms. By using elang, dimensionality reduction is performed on each word vectors in order to create the two-dimensional plot. . from elang.plot.utils import plot2d, plotNeighbours plot2d(model, method=&quot;TSNE&quot;, random_state=123) . similar_w = [w[0] for w in model.wv.most_similar(&quot;mouth&quot;, topn=20)] plot2d(model, targets=similar_w, method=&quot;TSNE&quot;, random_state=123) . Words that have similar meaning (by cosine similarity) tend to plotted next to each other. Therefore, creating a word clusters. . plotNeighbours(model, words=[&quot;cat&quot;, &quot;strawberry&quot;, &quot;indonesia&quot;, &quot;blue&quot;, &quot;mathematics&quot;, &quot;school&quot;], method=&quot;TSNE&quot;, k=10, random_state=123, draggable=True) . From the word embedding visualization above, we can conclude 6 clusters as below: . WORD COLOR CATEGORY . cat | red | animal | . strawberry | blue | fruit/plant | . indonesia | violet | country | . blue | yellow | color | . mathematics | pink | field of study | . school | light blue | academic-related | . Step 5. Using the Word2Vec model . VOCABULARY LIST . The list of vocabulary is saved on model.wv.vocab . list(model.wv.vocab)[:10] . [&#39;barnacle&#39;, &#39;is&#39;, &#39;type&#39;, &#39;of&#39;, &#39;arthropod&#39;, &#39;constituting&#39;, &#39;the&#39;, &#39;infraclass&#39;, &#39;in&#39;, &#39;subphylum&#39;] . WORD VECTORS . Check the dimensionality and content of a word vector. . vec = model.wv[&quot;dog&quot;] len(vec) . 300 . MOST SIMILAR WORDS . List out topn similar words based on Cosine Similarity. . model.wv.most_similar(&quot;dog&quot;, topn=5) . 2020-04-24 07:27:57,335 : INFO : precomputing L2-norms of word weight vectors . [(&#39;bites&#39;, 0.8471914529800415), (&#39;bite&#39;, 0.8361319303512573), (&#39;cat&#39;, 0.8241820335388184), (&#39;dogs&#39;, 0.7486913204193115), (&#39;pet&#39;, 0.7360792756080627)] . OUT-OF-LIST WORD . From a list of words, word vectors can choose one word that has different context among the rest. . model.wv.doesnt_match([&#39;dog&#39;, &#39;cat&#39;, &#39;wolf&#39;, &#39;human&#39;, &#39;eagle&#39;]) . &#39;human&#39; . model.wv.doesnt_match([&#39;orange&#39;, &#39;red&#39;, &#39;banana&#39;, &#39;blue&#39;, &#39;white&#39;]) . &#39;banana&#39; . WORD RELATIONSHIP . The word vectors can capture word relationship for example: . apple - red = ... - yellow . A proper word for this is a fruit with a yellow color. . model.wv.most_similar(positive=[&#39;apple&#39;, &#39;yellow&#39;], negative=[&#39;red&#39;]) . [(&#39;pepper&#39;, 0.7986029982566833), (&#39;cabbage&#39;, 0.7931523323059082), (&#39;bean&#39;, 0.7775619029998779), (&#39;pink&#39;, 0.776929497718811), (&#39;rot&#39;, 0.7747037410736084), (&#39;brassica&#39;, 0.7720001935958862), (&#39;mustard&#39;, 0.7547014951705933), (&#39;citrus&#39;, 0.7532116770744324), (&#39;blister&#39;, 0.7486450672149658), (&#39;oak&#39;, 0.7483341693878174)] . Potential Uses . NLP Applications . Word2Vec is used as an input to other neural network architecture for: . Article Classification | Machine Translation | Auto Image Captioning | Chatbot | Text Summarization | and many more... | . Other Non-NLP Use Cases . 1. Music Recommender at Spotify and Anghami . A user’s listening queue can be used to learn song vectors in the same way that a string of words in text is used to learn word vectors. The assumption is that users will tend to listen to similar tracks in sequence. How these song vectors are then used? One use is to create a kind of music taste vector for a user by averaging together the vectors for songs that a user likes to listen to. This taste vector can then become the query for a similarity search to find songs which are similar to the user’s taste vector. . . 2. Listing Recommendations at Airbnb . Imagine you are looking for an apartment to rent for your vacation to Paris. As you browse through the available homes, it’s likely that you will investigate a number of listings which fit your preferences and are comparable in features like amenities and design taste. Here the user activity data takes the form of click data, specifically the sequence of listings that a user viewed. Airbnb is able to learn vector representations of their listings by applying the word2vec approach to this data. . . An important piece of the word2vec training algorithm is that for each word that we train on, we select a random handful of words (which are not in the context of that word) to use as negative samples. Airbnb found that in order to learn vectors which could distinguish listings within Paris (and not just distinguish Paris listings from New York listings), it was important that the negative samples be drawn from within Paris. . Their solution to the cold start problem is to simply averaged the vectors of the geographically closest three listings to the new listing to create an initial vector. . 3. Product Recommendations in Yahoo Mail . Yahoo use purchase receipts in a user’s email inbox to form a purchase activity, allowing them in turn to learn product feature vectors which can be used to make product recommendations. . Since online shoppers receive e-mail receipts for their purchases, mail clients are in a unique position to see user purchasing activity across many different e-commerce sites. Even without this advantage, though, Yahoo’s approach seems applicable and potentially valuable to online retailers as well. . . Yahoo augmented the word2vec approach with a few notable innovations. The most interesting to me was their use of clustering to promote diversity in their recommendations. After learning vectors for all the products in their database, they clustered these vectors into groups. When making recommendations for a user based on a product the user just purchased, they don’t recommend products from within the same cluster. Instead, they identify which other clusters users most often purchase from after purchasing from the current cluster, and they recommend products from those other clusters instead. . 4. Matching Ads to Search Queries on Yahoo Search . The goal is to learn vector representations for search queries and for advertisements in the same embedding space, so that a given search query can be matched against available advertisements in order to find the most relevant ads to show the user. . The training data consists of user search sessions which consist of search queries entered, advertisements clicked, and search result links clicked. The sequence of these user actions is treated like the words in a sentence, and vectors are learned for each of them based on their context–the actions that tend to occur around them. If users often click a particular ad after entering a particular query, then we’ll learn similar vectors for the ad and the query. All three types of actions (searches entered, ads clicked, links clicked) are part of a single “vocabulary” for the model, such that the model doesn’t really distinguish these from one another. . . Extension to Word2Vec . Doc2Vec . Doc2Vec is an extension of Word2vec that encodes entire documents as opposed to individual words. Doc2Vec vectors represent the theme or overall meaning of a document. In this case, a document is a sentence, a paragraph, an article or an essay etc. Similar to Word2vec, there are two primary training methods: Distributed Memory Model Of Paragraph Vectors (PV-DM) and Paragraph Vector With A Distributed Bag Of Words (PVDBOW). . Dna2Vec . Dna2Vec is a consistent vector representations of variable-length k-mers. The analogies between natural language and DNA are as follow: . K-mer as the Words | DNA fragments as the Sentences | Part/whole of genome as the Corpus | . FastText . FastText is proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple. The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. . Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words. Even though a word does not exist in the training dataset, the model still capable of figuring out this word is closely related to similar terms. . Test Your Understanding! . General Word2Vec Concept . This section will assess your understanding of general Word2Vec and Training Optimization concept. . The following three statements are about Word2Vec, choose the CORRECT statement(s): . (i) Word2Vec is an unsupervised learning, even though we use a neural network to train the data. . (ii) In Skip-Gram architecture, the values that are projected onto the hidden layer is the word vector for a certain word in vocabulary. . (iii) In CBOW architecture, the values that are projected onto the hidden layer is the word vector for a certain word in vocabulary. . a. (i) and (ii) | b. (i) and (iii) | c. (ii) and (iii) | d. all of the above | e. none of the above | . | Regardless of the Word2Vec architecture, how many training examples will be generated from the sentence &quot;roses are red violets are blue&quot; if the window size is 3? . a. 18 | b. 21 | c. 24 | d. 27 | . | Suppose we have a vocabulary of 5,000 words and learning 100-dimensional Word2Vec embeddings. There will be 500,000 weights of the output matrix to be updated during backpropagation if we don&#39;t use a training optimization. Which one of the following statements is CORRECT? . a. There are also 500,000 weights of the input matrix to be updated during backpropagation. | b. Using negative = 20 or K = 20 means that only 20 weights (instead of 500,000) of the output matrix will be updated on each batch. | c. Using ns_exponent = 0 or p = 0 means that every single word has a probability of 0.0002 to be choosen as negative sample. | d. Using hierarchical softmax, the size of our output matrix is still 100 x 5,000. | . | Training Word2Vec . This section will assess your understanding of training Word2Vec using gensim package. . Choose one parameter that will speed up the training process if we increase its value, assuming the other parameters remain unchanged. . a. min_count | b. negative | c. size | d. window | . | The following statements are all correct in the process of building a Word2Vec model using gensim, EXCEPT ... . a. Stemming is rarely performed because a word can lose its semantic meaning. | b. The training process is separated into three steps for clarity and monitoring. | c. We doesn&#39;t have to set up anything for printing out a training report. | d. The training sentence is in the form of a &quot;list of lists of tokens&quot;. | . | . Note: Comment down below your answer to the five questions above 😊 . References . Word Embedding and Word2Vec . Towards Data Science: Introduction to Word Embedding and Word2Vec | Python Notebook: Understanding Word Vectors | Machine Learning Plus: Cosine Similarity | Towards Data Science: NLP 101 Word2Vec Skip-gram and CBOW | Medium: Word Embedding | mc.ai: Understand the Softmax Function in Minutes | Youtube: Hierarchical Softmax in Word2vec | . | . Recommender System . Towards Data Science: Introduction to Recommender Systems | Analytics Vidhya: Building a Recommendation System using Word2vec | . | . Potential Uses . Chris McCormick: Applying Word2vec to Recommenders and Advertising | mc.ai: An Intuitive Introduction Of Document Vector (Doc2Vec) | arXiv Paper: dna2vec | Data Science Hongkong: Application of Word2Vec to Represent Biological Sequences | Towards Data Science: Word2Vec and FastText Word Embedding with Gensim | . | .",
            "url": "https://tomytjandra.github.io/blogs/python/natural-language-processing/gensim/2020/04/24/understanding-word2vec-with-gensim-and-elang.html",
            "relUrl": "/python/natural-language-processing/gensim/2020/04/24/understanding-word2vec-with-gensim-and-elang.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Product2Vec: Product Recommender System using Word2Vec",
            "content": ". Important: If you haven&#8217;t understand how Word2Vec works, please visit my blog on Understanding Word2Vec with Gensim and Elang. . What is Product2Vec? . Can you guess the fundamental property of a natural language that Word2Vec exploits to create vector representations of text? It is the sequential nature of the text. Every sentence or phrase has a sequence of words. In the absence of this sequence, we would have a hard time understanding the text. Just try to interpret the sentence below: . prediction-based developed word a by Word2Vec is embeddings Tomas Mikolov. . There is no sequence in this sentence. It becomes difficult for us to grasp it and that’s why the sequence of words is so important in any natural language. This very property can be applied to data other than text that has a sequential nature as well. . One such data is the purchases made by the consumers on E-commerce websites. Most of the time there is a pattern in the buying behavior of the consumers. For example, a person involved in sports-related activities might have an online buying pattern similar to this: . . If we can represent each of these products by a vector, then we can easily find similar products. So, if a user is checking out a product online, then we can easily recommend him/her similar products by using the vector similarity score between the products. . But how do we get a vector representation of these products? Can we use the word2vec model to get these vectors? . We surely can! Just imagine the buying history of a consumer as a sentence and the products as its words: . . Now, let’s set up and understand our problem statement: . We are asked to create a system that automatically recommends a certain number of products to the consumers on an E-commerce website based on the past purchase behavior of the consumers. . Recommender Systems . Collaborative: . (+) Require no information about users or items and, so can be used in many situations. . (+) More accurate recommendations, since new interactions recorded over time bring new information and make the system more effective. . (-) Cold start problem: Impossible to recommend items to new users or to recommend a new item to any users and many users or items have too few interactions to be efficiently handled. . Content-based: . (+) Suffer less from the cold start problem than collaborative approaches: new users or items can be described by their characteristics (content) and so relevant suggestions can be done for these new entities. . (-) Tend to over-specialize: it will recommend items similar to those already consumed, with a tendency of creating a &quot;filter bubble&quot;. . We&#39;ll be using collaborative, memory-based method for our case study. Let&#39;s go: . Import Libraries and Model . Let’s fire up our notebook and quickly import the required libraries. . We create a connection between Google Colab and Drive. Please go through the authentication step then download the file using the Google File ID. . from pydrive.auth import GoogleAuth from pydrive.drive import GoogleDrive from google.colab import auth from oauth2client.client import GoogleCredentials auth.authenticate_user() gauth = GoogleAuth() gauth.credentials = GoogleCredentials.get_application_default() drive = GoogleDrive(gauth) . . # you can replace the id with id of file you want to access model_id = &quot;1-rFzYA2oNPm2F680L4FsjnkIpDzCDlwK&quot; downloaded = drive.CreateFile({&#39;id&#39;: model_id}) downloaded.GetContentFile(&#39;recommender.model&#39;) . import pandas as pd import numpy as np # modeling from sklearn.model_selection import train_test_split from gensim.models import Word2Vec # visualization import matplotlib.pyplot as plt %matplotlib inline # other from tqdm import tqdm import warnings warnings.filterwarnings(&#39;ignore&#39;) pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_colwidth&#39;, -1) . Explore the Data . We are going to use an Online Retail Dataset that you can download from UCI Machine Learning. . Import Data . url = &#39;https://github.com/tomytjandra/word2vec-embeddings/blob/master/dataset/Online%20Retail.xlsx?raw=true&#39; retail = pd.read_excel(url) retail.head(10) . InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 0 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850.0 | United Kingdom | . 1 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 2 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850.0 | United Kingdom | . 3 536365 | 84029G | KNITTED UNION FLAG HOT WATER BOTTLE | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 4 536365 | 84029E | RED WOOLLY HOTTIE WHITE HEART. | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 5 536365 | 22752 | SET 7 BABUSHKA NESTING BOXES | 2 | 2010-12-01 08:26:00 | 7.65 | 17850.0 | United Kingdom | . 6 536365 | 21730 | GLASS STAR FROSTED T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 4.25 | 17850.0 | United Kingdom | . 7 536366 | 22633 | HAND WARMER UNION JACK | 6 | 2010-12-01 08:28:00 | 1.85 | 17850.0 | United Kingdom | . 8 536366 | 22632 | HAND WARMER RED POLKA DOT | 6 | 2010-12-01 08:28:00 | 1.85 | 17850.0 | United Kingdom | . 9 536367 | 84879 | ASSORTED COLOUR BIRD ORNAMENT | 32 | 2010-12-01 08:34:00 | 1.69 | 13047.0 | United Kingdom | . Column Description . retail.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 541909 entries, 0 to 541908 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 InvoiceNo 541909 non-null object 1 StockCode 541909 non-null object 2 Description 540455 non-null object 3 Quantity 541909 non-null int64 4 InvoiceDate 541909 non-null datetime64[ns] 5 UnitPrice 541909 non-null float64 6 CustomerID 406829 non-null float64 7 Country 541909 non-null object dtypes: datetime64[ns](1), float64(2), int64(1), object(4) memory usage: 33.1+ MB . The dataset contains 541909 transactions. That is a pretty good number for us to build our model. Here is the description of the fields in this dataset: . InvoiceNo: Invoice number. a unique number assigned to each transaction | StockCode: Product/item code. a unique number assigned to each distinct product | Description: Product name | Quantity: The quantities of each product per transaction | InvoiceDate: Invoice Date and time. The day and time when each transaction was generated | UnitPrice: Price of each unit product | CustomerID: Customer number. a unique number assigned to each customer | Country: The country where each transaction was generated | Check Date . Check whether InvoiceDate has been sorted increasingly or not. . retail[&quot;InvoiceDate&quot;].is_monotonic_increasing . True . Data Preparation . Missing Values . Check for missing values . retail.isnull().sum() . InvoiceNo 0 StockCode 0 Description 1454 Quantity 0 InvoiceDate 0 UnitPrice 0 CustomerID 135080 Country 0 dtype: int64 . Since we have sufficient data, we will drop all the rows with missing values. . retail.dropna(inplace=True) . Data Type Conversion . retail.dtypes . InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate datetime64[ns] UnitPrice float64 CustomerID float64 Country object dtype: object . retail[&quot;CustomerID&quot;] = retail[&quot;CustomerID&quot;].astype(int) retail[[&quot;InvoiceNo&quot;, &quot;StockCode&quot;, &quot;Description&quot;, &quot;CustomerID&quot;]] = retail[[&quot;InvoiceNo&quot;, &quot;StockCode&quot;, &quot;Description&quot;, &quot;CustomerID&quot;]].astype(str) retail[&quot;Country&quot;] = retail[&quot;Country&quot;].astype(&quot;category&quot;) . retail.dtypes . InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate datetime64[ns] UnitPrice float64 CustomerID object Country category dtype: object . Remove Leading and Trailing Whitespace . Some of the product description have leading and trailing whitespace as shown below: . retail[&quot;Description&quot;].values . array([&#39;WHITE HANGING HEART T-LIGHT HOLDER&#39;, &#39;WHITE METAL LANTERN&#39;, &#39;CREAM CUPID HEARTS COAT HANGER&#39;, ..., &#39;CHILDRENS CUTLERY DOLLY GIRL &#39;, &#39;CHILDRENS CUTLERY CIRCUS PARADE&#39;, &#39;BAKING SET 9 PIECE RETROSPOT &#39;], dtype=object) . Let&#39;s remove it: . retail[&quot;Description&quot;] = retail[&quot;Description&quot;].str.strip() retail[&quot;Description&quot;].values . array([&#39;WHITE HANGING HEART T-LIGHT HOLDER&#39;, &#39;WHITE METAL LANTERN&#39;, &#39;CREAM CUPID HEARTS COAT HANGER&#39;, ..., &#39;CHILDRENS CUTLERY DOLLY GIRL&#39;, &#39;CHILDRENS CUTLERY CIRCUS PARADE&#39;, &#39;BAKING SET 9 PIECE RETROSPOT&#39;], dtype=object) . Splitting Data . There are 4372 unique customers in our dataset. For each of these customers, we will extract their buying history. In other words, we can have 4372 sequences of purchases. . customers = retail[&quot;CustomerID&quot;].unique().tolist() len(customers) . 4372 . It is a good practice to set aside a small part of the dataset for validation purposes. Therefore, we will use the data of 95% of the customers to create word2vec embeddings. Let’s split the data. . customers_train, customers_val = train_test_split(customers, train_size=0.95, shuffle=True, random_state=123) retail_train = retail[retail[&quot;CustomerID&quot;].isin(customers_train)] retail_val = retail[retail[&quot;CustomerID&quot;].isin(customers_val)] print(&quot;No. of Customers in Training Data:&quot;, len(customers_train)) print(&quot;Dim. of Purchases in Training Data:&quot;, retail_train.shape) print(&quot;No. of Customers in Validation Data:&quot;, len(customers_val)) print(&quot;Dim. of Purchases in Validation Data:&quot;, retail_val.shape) . . No. of Customers in Training Data: 4153 Dim. of Purchases in Training Data: (390857, 8) No. of Customers in Validation Data: 219 Dim. of Purchases in Validation Data: (15972, 8) . List of Purchases . We will create sequences of purchases made by the customers in the dataset for both the train and validation set. . def purchasesList(df): purchases_list = [] customers_list = df[&#39;CustomerID&#39;].unique() for customer in tqdm(customers_list): customer_purchases_list = df[df[&quot;CustomerID&quot;] == customer][&quot;StockCode&quot;].tolist() purchases_list.append(customer_purchases_list) return purchases_list . . purchases_train = purchasesList(retail_train) purchases_val = purchasesList(retail_val) . 100%|██████████| 4153/4153 [01:31&lt;00:00, 45.22it/s] 100%|██████████| 219/219 [00:00&lt;00:00, 472.04it/s] . Word2Vec Embeddings . Let&#39;s prepare a Callback to print loss after each epoch. . from gensim.models.callbacks import CallbackAny2Vec class Callback(CallbackAny2Vec): def __init__(self): self.epoch = 1 def on_epoch_end(self, model): loss = model.get_latest_training_loss() if self.epoch == 1: print(&#39;Loss after epoch {}: {}&#39;.format(self.epoch, loss)) else: print(&#39;Loss after epoch {}: {}&#39;.format( self.epoch, loss - self.loss_previous_step)) self.epoch += 1 self.loss_previous_step = loss . Model Training . Step to train the model: . Initialize the Word2Vec settings | Build vocabulary from a sequence of sentences | Update the model’s neural weights from a sequence of sentences | model = Word2Vec( size=100, # dimensionality of the word vectors window=10, # max distance between context and target word min_count=5, # frequency cut-off sg=1, # using skip-gram hs=0, # no hierarchical softmax (default) negative=15, # negative sampling data alpha=0.005, # learning rate seed=123, # reproducibility workers=1) # STEP 2 model.build_vocab(purchases_train) # STEP 3 callback = Callback() model.train(purchases_train, total_examples=model.corpus_count, # count of sentences epochs=10, # number of iterations over the corpus compute_loss=True, # to track model loss callbacks=[callback]) . Loss after epoch 1: 13739012.0 Loss after epoch 2: 9969096.0 Loss after epoch 3: 9610266.0 Loss after epoch 4: 11308194.0 Loss after epoch 5: 11223612.0 Loss after epoch 6: 11192972.0 Loss after epoch 7: 963048.0 Loss after epoch 8: 878904.0 Loss after epoch 9: 845232.0 Loss after epoch 10: 814056.0 . (3869966, 3908570) . Let’s check out the summary of our model: . model = Word2Vec.load(&quot;recommender.model&quot;) print(model) . Word2Vec(vocab=3196, size=100, alpha=0.005) . Our model has a vocabulary of 3196 unique products and their vectors of size 100 each. Next, we will extract the vectors of all the words in our vocabulary and store it in one place for easy access. . X = model[model.wv.vocab] X.shape . (3196, 100) . Visualize Word2Vec . It is always quite helpful to visualize the embeddings that you have created. Over here, we have 100-dimensional embeddings. We can’t even visualize 4 dimensions let alone 100. What in the world can we do? . We are going to reduce the dimensions of the product embeddings from 100 to 2 by using various dimensionality reduction algorithm: . Principal Component Analysis (PCA) | t-Distributed Stochastic Neighbor Embedding (t-SNE) | Uniform Manifold Approximation and Projection (UMAP) | from sklearn.decomposition import PCA from sklearn.manifold import TSNE from umap import UMAP X_pca = PCA(n_components=2).fit_transform(X) X_tsne = TSNE(n_components=2, random_state=123).fit_transform(X) X_umap = UMAP(n_components=2, random_state=123).fit_transform(X) fig, axes = plt.subplots(1, 3, figsize=(15, 5)) fig.patch.set_facecolor(&quot;white&quot;) for ax, X_reduced, title in zip(axes, [X_pca, X_tsne, X_umap], [&quot;PCA&quot;, &quot;t-SNE&quot;, &quot;UMAP&quot;]): ax.scatter(X_reduced[:, 0], X_reduced[:, 1], s=1) ax.axis(&quot;off&quot;) ax.set_title(title, fontsize=15) plt.tight_layout() fig.suptitle(&quot;WORD2VEC REDUCED DIMENSIONALITY&quot;, size=25, y=1.075, fontweight=&quot;bold&quot;) plt.show() . . Start Recommending Products . Congratulations! We are finally ready with the word2vec embeddings for every product in our online retail dataset. Now, our next step is to suggest similar products for a certain product or a product’s vector. . Let’s first create a dictionary to easily map a product’s Description to its StockCode and vice versa. . products = retail[[&quot;StockCode&quot;, &quot;Description&quot;]] # example of duplicates products[products[&quot;StockCode&quot;] == &quot;22632&quot;].drop_duplicates() . StockCode Description . 8 22632 | HAND WARMER RED POLKA DOT | . 257 22632 | HAND WARMER RED RETROSPOT | . products.drop_duplicates(subset=&quot;StockCode&quot;, keep=&quot;last&quot;, inplace=True) # create a dictionary for mapping code_to_name = pd.Series(products[&quot;Description&quot;].values, index=products[&quot;StockCode&quot;]).to_dict() name_to_code = pd.Series(products[&quot;StockCode&quot;].values, index=products[&quot;Description&quot;]).to_dict() dict(list(code_to_name.items())[:20]) . {&#39;16161M&#39;: &#39;WRAP PINK FLOCK&#39;, &#39;20878&#39;: &#39;SET/9 CHRISTMAS T-LIGHTS SCENTED&#39;, &#39;20957&#39;: &#39;PORCELAIN HANGING BELL SMALL&#39;, &#39;21268&#39;: &#39;VINTAGE BLUE TINSEL REEL&#39;, &#39;21486&#39;: &#39;PINK HEART DOTS HOT WATER BOTTLE&#39;, &#39;21488&#39;: &#39;RED WHITE SCARF HOT WATER BOTTLE&#39;, &#39;21895&#39;: &#34;POTTING SHED SOW &#39;N&#39; GROW SET&#34;, &#39;22275&#39;: &#39;WEEKEND BAG VINTAGE ROSE PAISLEY&#39;, &#39;35271S&#39;: &#39;GOLD PRINT PAPER BAG&#39;, &#39;37461&#39;: &#39;FUNKY MONKEY MUG&#39;, &#39;47503J&#39;: &#39;SET/3 FLORAL GARDEN TOOLS IN BAG&#39;, &#39;47579&#39;: &#39;TEA TIME BREAKFAST BASKET&#39;, &#39;82615&#39;: &#39;PINK MARSHMALLOW SCARF KNITTING KIT&#39;, &#39;84614A&#39;: &#39;PINK BAROQUE FLOCK CANDLE HOLDER&#39;, &#39;84773&#39;: &#39;RED ROSE AND LACE C/COVER&#39;, &#39;84854&#39;: &#39;GIRLY PINK TOOL SET&#39;, &#39;84963A&#39;: &#39;PINK PAINTED KASHMIRI CHAIR&#39;, &#39;90128B&#39;: &#39;BLUE LEAVES AND BEADS PHONE CHARM&#39;, &#39;90152B&#39;: &#39;BLUE/GREEN SHELL NECKLACE W PENDANT&#39;, &#39;90167&#39;: &#39;BEADED LOVE HEART JEWELLERY SET&#39;} . Case 1: Recommendation based on last bought product . I have defined the function below. It will take a product&#39;s StockCode as input and return top n similar products. . def similarProductsByVector(vec, n=10): # extract most similar products for the input vector similar_products = model.wv.similar_by_vector(vec, topn=n) # extract name and similarity score of the similar products product_list = [] for prod, sim in similar_products: product_list.append((code_to_name[prod], sim)) return product_list . Let&#39;s try out our function by passing the StockCode &quot;90019A&quot; (&#39;SILVER M.O.P ORBIT BRACELET&#39;) . code_to_name[&quot;90019A&quot;] . &#39;SILVER M.O.P ORBIT BRACELET&#39; . similarProductsByVector(model[&quot;90019A&quot;]) . [(&#39;SILVER M.O.P ORBIT BRACELET&#39;, 1.0), (&#39;PINK ROSEBUD PEARL BRACELET&#39;, 0.988038182258606), (&#39;SILVER ROCCOCO CHANDELIER&#39;, 0.986005425453186), (&#39;MIDNIGHT BLUE GLASS/SILVER BRACELET&#39;, 0.9858831167221069), (&#39;ANT COPPER RED BOUDICCA BRACELET&#39;, 0.9858506321907043), (&#39;SOFT PINK ROSE TOWEL&#39;, 0.983485221862793), (&#39;WHITE VINT ART DECO CRYSTAL NECKLAC&#39;, 0.9819661378860474), (&#39;MIRROR MOSAIC CANDLE PLATE&#39;, 0.9817532300949097), (&#39;WOVEN BUBBLE GUM CUSHION COVER&#39;, 0.981330156326294), (&#39;CRYSTAL PAIR HEART HAIR SLIDES&#39;, 0.9808048009872437)] . Identify last bought products for each CustomerID and recommend them 3 other similar products. . last_bought = retail_val.drop_duplicates(subset=&quot;CustomerID&quot;, keep=&quot;last&quot;) last_bought.head() . InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 6352 536884 | 48185 | DOORMAT FAIRY CAKE | 1 | 2010-12-03 11:42:00 | 7.95 | 14589 | United Kingdom | . 19025 537827 | 22145 | CHRISTMAS CRAFT HEART STOCKING | 30 | 2010-12-08 14:43:00 | 2.10 | 15332 | Lithuania | . 21696 538108 | 20676 | RED RETROSPOT BOWL | 8 | 2010-12-09 15:23:00 | 1.25 | 17456 | United Kingdom | . 23212 538184 | 22904 | CALENDAR PAPER CUT DESIGN | 6 | 2010-12-10 10:21:00 | 2.95 | 17880 | United Kingdom | . 25079 538352 | 51008 | AFGHAN SLIPPER SOCK PAIR | 40 | 2010-12-10 15:21:00 | 3.45 | 16565 | United Kingdom | . recommendation_df = [] for idx, s in last_bought.iterrows(): temp_dict = {} temp_dict[&quot;CustomerID&quot;] = s[&quot;CustomerID&quot;] temp_dict[&quot;Last Bought Product&quot;] = code_to_name[s[&quot;StockCode&quot;]] try: temp_dict[&quot;Recommended Products&quot;] = np.transpose( similarProductsByVector(model[s[&quot;StockCode&quot;]], n=4))[0][1:] except KeyError: continue recommendation_df.append(temp_dict) pd.DataFrame(recommendation_df).sample(5, random_state=123) . . CustomerID Last Bought Product Recommended Products . 204 14282 | LUNCH BAG PAISLEY PARK | [LUNCH BAG RED VINTAGE DOILY, JUMBO BAG PAISLEY PARK, VINTAGE DOILY JUMBO BAG RED] | . 203 17290 | DOORMAT SPOTTY HOME SWEET HOME | [DOORMAT FANCY FONT HOME SWEET HOME, DOORMAT NEW ENGLAND, DOORMAT AIRMAIL] | . 31 13723 | PICNIC BASKET WICKER SMALL | [PICNIC BASKET WICKER LARGE, CHERRY BLOSSOM DECORATIVE FLASK, ENCHANTED BIRD PLANT CAGE] | . 186 16496 | YOU&#39;RE CONFUSING ME METAL SIGN | [METAL SIGN NEIGHBOURHOOD WITCH, CHOCOLATE THIS WAY METAL SIGN, COOK WITH WINE METAL SIGN] | . 169 15838 | HAND WARMER OWL DESIGN | [HAND WARMER SCOTTY DOG DESIGN, HAND WARMER BIRD DESIGN, HAND WARMER RED LOVE HEART] | . Cool! The results are pretty relevant and match well with the input product. However, this output is based on the vector of a single product only. What if we want recommend a user products based on the multiple purchases he or she has made in the past? . Case 2: Recommendation based on multiple purchases . One simple solution is to take average of all the vectors of the products he has bought so far and use this resultant vector to find similar products. For that we will use the function below that takes in a list of product ID&#39;s and gives out a 100 dimensional vector which is mean of vectors of the products in the input list. . def aggregateVectors(products): product_vec = [] for i in products: try: product_vec.append(model[i]) except KeyError: continue return np.mean(product_vec, axis=0) . . If you can recall, we have already created a separate list of purchase sequences for validation purpose. Now let&#39;s make use of that. . len(purchases_val[-1]) . 18 . The length of the first list of products purchased by a user is 240. We will pass this products&#39; sequence of the validation set to the function aggregate_vectors. . aggregateVectors(purchases_val[-1]).shape . (100,) . Well, the function has returned an array of 100 dimensions. It means the function is working fine. Now we can use this result to get the most similar products. Let&#39;s do it. . similarProductsByVector(aggregateVectors(purchases_val[-1])) . [(&#39;METAL SIGN TAKE IT OR LEAVE IT&#39;, 0.9587694406509399), (&#39;BEWARE OF THE CAT METAL SIGN&#39;, 0.9575548768043518), (&#39;CHOCOLATE THIS WAY METAL SIGN&#39;, 0.952998161315918), (&#39;AREA PATROLLED METAL SIGN&#39;, 0.9523403644561768), (&#39;OPEN CLOSED METAL SIGN&#39;, 0.9495117664337158), (&#39;HOT BATHS METAL SIGN&#39;, 0.9472224712371826), (&#39;N0 SINGING METAL SIGN&#39;, 0.9446829557418823), (&#34;YOU&#39;RE CONFUSING ME METAL SIGN&#34;, 0.9446626901626587), (&#39;METAL SIGN NEIGHBOURHOOD WITCH&#39;, 0.9415547251701355), (&#34;I&#39;M ON HOLIDAY METAL SIGN&#34;, 0.9393553137779236)] . As it turns out, our system has recommended products based on the entire purchase history of a user. Feel free to play the code, try to get product recommendation for more sequences from the validation set. . Test Your Understanding! . In this section, we will build a system to recommend similar products based on online transaction history which provided in Online Retail.xlsx. This dataset contains 541909 transactions and 8 columns as follow: . InvoiceNo: Invoice number. a unique number assigned to each transaction | StockCode: Product/item code. a unique number assigned to each distinct product | Description: Product name | Quantity: The quantities of each product per transaction | InvoiceDate: Invoice Date and time. The day and time when each transaction was generated | UnitPrice: Price of each unit product | CustomerID: Customer number. a unique number assigned to each customer | Country: The country where each transaction was generated | . After loading the dataset, make sure you have performed the following preprocessing step: . Drop all the rows with missing value | Convert each columns to its proper data type | Remove leading and trailing whitespace on column Description | Prepare a code_to_name and/or name_to_code dictionary mapping, assuming the product name used is the latest name by date | . In order to get the same result, pre-trained model recommender.model is provided to you inside the folder models. The model has a vocabulary of 3196 unique products and size of the word vector is 100 dimensions. The parameters used to train the model are as follow: . size = 100 | window = 10 | min_count = 5 | sg = 1 | hs = 0 | negative = 15 | alpha = 0.005 | seed = 123 | workers = 1 | epochs = 10 | . Please make sure to include the Callback class before loading the pre-trained model: . from gensim.models.callbacks import CallbackAny2Vec class Callback(CallbackAny2Vec): def __init__(self): self.epoch = 1 def on_epoch_end(self, model): loss = model.get_latest_training_loss() if self.epoch == 1: print(&#39;Loss after epoch {}: {}&#39;.format(self.epoch, loss)) else: print(&#39;Loss after epoch {}: {}&#39;.format(self.epoch, loss - self.loss_previous_step)) self.epoch += 1 self.loss_previous_step = loss . Let&#39;s say we build a recommender system so that items that have at least a similarity score of 0.90 with previous transactions will appear on the &quot;Product Recommendation&quot; section of a website. Suppose there is a customer who only purchased one item, namely &quot;BLUE PAISLEY TISSUE BOX&quot;. How many new products will appear on their &quot;Product Recommendation&quot; section? a. 9 | b. 10 | c. 11 | d. 12 | . | If we use the system stated on previous question, the number of recommended products will be different for each customer. Suppose we change how our recommender system works. Now, for each customer, we want the model to recommend exactly 10 most similar products based on entire purchase history of a user. . First, you may need to find a list of purchased products for each customer: . purchased_product_for_each_customer = retail.groupby(&quot;...&quot;)[&quot;...&quot;].apply(list) . Now, let&#39;s analyze customer with CustomerID &quot;13160&quot;. Is there any product that they have purchased before, but the model recommends again? If yes, what is the product description? a. 15CM CHRISTMAS GLASS BALL 20 LIGHTS | b. BLUE SPOT CERAMIC DRAWER KNOB | c. DRAWER KNOB CERAMIC IVORY | d. HEART WREATH DECORATION WITH BELL | e. None of the top 10 recommended products have been purchased by the CustomerID &quot;13160&quot; | . | . Note: Comment down below your answers 😊 .",
            "url": "https://tomytjandra.github.io/blogs/python/recommender-system/gensim/2020/04/24/product2vec-recommender-system.html",
            "relUrl": "/python/recommender-system/gensim/2020/04/24/product2vec-recommender-system.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Liver Disease Classification using Logistic Regression and k-Nearest Neighbors",
            "content": "Introduction . Patients with liver disease have been continuously increasing because of excessive consumption of alcohol, inhale of harmful gases, intake of contaminated food, pickles, and drugs. This use case is provided on Kaggle using the dataset from UCI ML Repository). The dataset contains 416 liver patient records and 167 non-liver patient records collected from North East of Andhra Pradesh, India. The collected data is used for evaluating prediction algorithms to reduce the burden on doctors. . Import Libraries . First of all, let us import all necessary libraries, mainly for data analysis, visualization, and modeling. . import pandas as pd import numpy as np # visualization import seaborn as sns import matplotlib.pyplot as plt from matplotlib.cbook import boxplot_stats # preprocessing (pre-modeling) from sklearn.utils import resample from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler, StandardScaler # modeling import statsmodels.api as sm from sklearn.neighbors import KNeighborsClassifier # evaluation (post-modeling) from sklearn.metrics import * from statsmodels.stats.outliers_influence import variance_inflation_factor from scipy import stats from scipy.special import logit . Set Notebook Options . Suppress package warning | Set color of the plot to be more contrast | Change float format to five decimal places | Display all content in a pandas column | . import warnings warnings.filterwarnings(&quot;ignore&quot;) sns.set(style=&quot;ticks&quot;, color_codes=True) pd.options.display.float_format = &#39;{:.5f}&#39;.format pd.options.display.max_colwidth = None . . Data Wrangling . Before we jump into any visualization or modeling step, we have to make sure our data is ready. . Import Data . Let&#39;s import indian_liver_patient.csv and analyze the data structure. . liver = pd.read_csv(&quot;data_input/indian_liver_patient.csv&quot;) liver.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 583 entries, 0 to 582 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 Age 583 non-null int64 1 Gender 583 non-null object 2 Total_Bilirubin 583 non-null float64 3 Direct_Bilirubin 583 non-null float64 4 Alkaline_Phosphotase 583 non-null int64 5 Alamine_Aminotransferase 583 non-null int64 6 Aspartate_Aminotransferase 583 non-null int64 7 Total_Protiens 583 non-null float64 8 Albumin 583 non-null float64 9 Albumin_and_Globulin_Ratio 579 non-null float64 10 Dataset 583 non-null int64 dtypes: float64(5), int64(5), object(1) memory usage: 50.2+ KB . The dataset contains 583 observations of patient records with 11 columns as follows (Reference): . Age: Age of the patient. Any patient whose age exceeded 89 is listed as being age 90. | Gender: Gender of the patient, either Male or Female. | Total_Bilirubin: Bilirubin is an orange-yellow pigment, a waste product primarily produced by the normal breakdown of heme. This test measures the amount of bilirubin in the blood to evaluate a person&#39;s liver function or to help diagnose anemias caused by red blood cell destruction (hemolytic anemia). Measured in mg/dL. | Direct_Bilirubin: Water-soluble forms of bilirubin. Measured in mg/dL. | Alkaline_Phosphotase (ALP): Enzyme found in several tissues throughout the body. The highest concentrations of ALP are present in the cells that comprise bone and the liver. Elevated levels of ALP in the blood are most commonly caused by liver disease or bone disorders. Measured in U/L. | Alamine_Aminotransferase (ALT): Enzyme found mostly in the cells of the liver and kidney. Normally, ALT levels in the blood are low, but when the liver is damaged, ALT is released into the blood and the level increases. This test measures the level of ALT in the blood and is useful for early detection of liver disease. Measured in U/L. | Aspartate_Aminotransferase: Enzyme found in cells throughout the body but mostly in the heart and liver and, to a lesser extent, in the kidneys and muscles. In healthy individuals, levels of AST in the blood are low. When liver or muscle cells are injured, they release AST into the blood. This makes AST a useful test for detecting or monitoring liver damage. Measured in U/L. | Total_Protiens: Measures the amount of protein in g/dL. | Albumin: Made by the liver and makes up about 60% of the total protein. Albumin keeps fluid from leaking out of blood vessels, nourishes tissues, and transports hormones, vitamins, drugs, and substances like calcium throughout the body. Measured in g/dL. | Albumin_and_Globulin_Ratio: Compares the amount of Albumin with Globulin. Globulin made up the remaining 40% of proteins in the blood which is a varied group of proteins, some produced by the liver and some by the immune system. They help fight infection and transport nutrients. Measured in g/dL. | Dataset: A class label used to divide the patient into two groups. Value 1 indicates patient with liver disease and 2 otherwise. | . Rename Columns . Rename the column to its abbreviation in order to make it shorter. . column_names = { &quot;Age&quot;: &quot;Age&quot;, &quot;Gender&quot;: &quot;Gender&quot;, &quot;TB&quot;: &quot;Total Bilirubin&quot;, &quot;DB&quot;: &quot;Direct Bilirubin&quot;, &quot;ALP&quot;: &quot;Alkaline Phosphotase&quot;, &quot;ALT&quot;: &quot;Alamine Aminotransferase&quot;, &quot;AST&quot;: &quot;Aspartate Aminotransferase&quot;, &quot;TP&quot;: &quot;Total Proteins&quot;, &quot;Albumin&quot;: &quot;Albumin&quot;, &quot;A/G Ratio&quot;: &quot;Albumin and Globulin Ratio&quot;, &quot;Disease&quot;: &quot;Dataset&quot; } liver.columns = column_names.keys() liver.head() . . Age Gender TB DB ALP ALT AST TP Albumin A/G Ratio Disease . 0 65 | Female | 0.70000 | 0.10000 | 187 | 16 | 18 | 6.80000 | 3.30000 | 0.90000 | 1 | . 1 62 | Male | 10.90000 | 5.50000 | 699 | 64 | 100 | 7.50000 | 3.20000 | 0.74000 | 1 | . 2 62 | Male | 7.30000 | 4.10000 | 490 | 60 | 68 | 7.00000 | 3.30000 | 0.89000 | 1 | . 3 58 | Male | 1.00000 | 0.40000 | 182 | 14 | 20 | 6.80000 | 3.40000 | 1.00000 | 1 | . 4 72 | Male | 3.90000 | 2.00000 | 195 | 27 | 59 | 7.30000 | 2.40000 | 0.40000 | 1 | . Rename Target Variable . Rename the values in the target variable Disease: . 1 indicates patient with liver disease, we subtitute this with &quot;Yes&quot; | 2 indicates a non-liver disease patient, we subtitute this with &quot;No&quot; | . liver.loc[liver[&quot;Disease&quot;] == 1, &quot;Disease&quot;] = &quot;Yes&quot; liver.loc[liver[&quot;Disease&quot;] == 2, &quot;Disease&quot;] = &quot;No&quot; liver[&quot;Disease&quot;].unique() . . array([&#39;Yes&#39;, &#39;No&#39;], dtype=object) . Data Type Conversion . Convert these two categorical columns, from object to category type: . Gender with two levels: &quot;Female&quot; and &quot;Male&quot; | Disease with two levels: &quot;Yes&quot; and &quot;No&quot; | . liver[[&quot;Gender&quot;, &quot;Disease&quot;]] = liver[[&quot;Gender&quot;, &quot;Disease&quot;]].astype(&quot;category&quot;) liver.dtypes . . Age int64 Gender category TB float64 DB float64 ALP int64 ALT int64 AST int64 TP float64 Albumin float64 A/G Ratio float64 Disease category dtype: object . Identify Missing Values . We have to make sure our data is complete, means that there are no missing or NA values. . liver.isna().sum() . . Age 0 Gender 0 TB 0 DB 0 ALP 0 ALT 0 AST 0 TP 0 Albumin 0 A/G Ratio 4 Disease 0 dtype: int64 . Four observations of A/G Ratio are missing, this will be handled later on after the feature engineering step. . Feature Engineering . Feature engineering is the process of using domain knowledge to extract features from provided raw data. These features can be used to improve the performance of machine learning models. In this section, we&#39;ll be extracting features to reduce the correlation between the predictors and reduce the risk of multicollinearity on the model. So let&#39;s plot the Pearson correlation heatmap to find out which predictors have a strong correlation. . def plotCorrelationHeatmap(data, figsize=(12, 6)): plt.figure(figsize=figsize) corr_val = data.corr(method=&quot;pearson&quot;) mask = np.zeros_like(corr_val, dtype=np.bool) mask[np.triu_indices_from(mask, k=1)] = True corr_heatmap = sns.heatmap(corr_val, mask=mask, annot=True, fmt=&#39;.3f&#39;, linewidths=3, cmap=&quot;Reds&quot;) corr_heatmap.set_title(&quot;PEARSON CORRELATION HEATMAP&quot;, fontsize=15, fontweight=&quot;bold&quot;) corr_heatmap plotCorrelationHeatmap(liver) . . Consider correlation above 0.6 as strong, we want to minimize the risk of multicollinearity on our model by considering the correlation of these features: . TB and DB (0.875): Merge into DB/TB Percentage (Reference) | AST and ALT (0.792): We can merge them into AST/ALT Ratio (Reference), but then we ended up with an insignificant predictor later on modelling. Thus, we&#39;ll keep them as separate predictors. | Albumin and A/G Ratio (0.69): Calculate Globulin level, given the A/G ratio | Albumin and TP (0.784): Albumin is a group of protein, it certainly have a strong correlation with TP (Total Protein). These two variables will be discussed later on. | . liver[&quot;DB/TB Percentage&quot;] = liver[&quot;DB&quot;] / liver[&quot;TB&quot;] * 100 # liver[&quot;AST/ALT Ratio&quot;] = liver[&quot;AST&quot;]/liver[&quot;ALT&quot;] liver[&quot;Globulin&quot;] = liver[&quot;Albumin&quot;] / liver[&quot;A/G Ratio&quot;] liver = liver.drop([&quot;DB&quot;, &quot;TB&quot;, &quot;A/G Ratio&quot;], axis=1) liver.head() . . Age Gender ALP ALT AST TP Albumin Disease DB/TB Percentage Globulin . 0 65 | Female | 187 | 16 | 18 | 6.80000 | 3.30000 | Yes | 14.28571 | 3.66667 | . 1 62 | Male | 699 | 64 | 100 | 7.50000 | 3.20000 | Yes | 50.45872 | 4.32432 | . 2 62 | Male | 490 | 60 | 68 | 7.00000 | 3.30000 | Yes | 56.16438 | 3.70787 | . 3 58 | Male | 182 | 14 | 20 | 6.80000 | 3.40000 | Yes | 40.00000 | 3.40000 | . 4 72 | Male | 195 | 27 | 59 | 7.30000 | 2.40000 | Yes | 51.28205 | 6.00000 | . Here is the new correlation heatmap after feature engineering: . plotCorrelationHeatmap(liver, (14, 6)) . . Interestingly, TP is now strongly correlated with both Albumin and Globulin. Let&#39;s manually add the value of Albumin and Globulin as Albumin+Globulin then further investigate the correlation with TP: . AG_df = liver[[&quot;Albumin&quot;, &quot;Globulin&quot;, &quot;TP&quot;]] AG_df[&quot;Albumin+Globulin&quot;] = AG_df[&quot;Albumin&quot;] + AG_df[&quot;Globulin&quot;] plotCorrelationHeatmap(AG_df, (6, 5)) . . Such a strong (almost perfect) correlation between TP and Albumin+Globulin, indicating that TP can be explained by both Albumin and Globulin. We will use this information to handle missing value. . Handle Missing Value . Remember about the missing value in A/G Ratio? Now they are moved to Globulin since we calculate it using A/G Ratio. . missing = liver[&quot;Globulin&quot;].isna() liver[missing] . . Age Gender ALP ALT AST TP Albumin Disease DB/TB Percentage Globulin . 209 45 | Female | 189 | 23 | 33 | 6.60000 | 3.90000 | Yes | 33.33333 | nan | . 241 51 | Male | 230 | 24 | 46 | 6.50000 | 3.10000 | Yes | 25.00000 | nan | . 253 35 | Female | 180 | 12 | 15 | 5.20000 | 2.70000 | No | 33.33333 | nan | . 312 27 | Male | 106 | 25 | 54 | 8.50000 | 4.80000 | No | 46.15385 | nan | . Let&#39;s impute missing Globulin value by fitting a linear regression line $TP = Albumin + Globulin + constant$ using the non-missing data. . . Important: Make sure to fit the regression line to the training data only, since we don&#8217;t want data leakage to occur. Statistics between training and testing data should not be shared. Use the same random_state to ensure the split in this section is the same as the split in the modeling section. . liver_train, liver_test = train_test_split(liver, test_size=0.25, random_state=888) X_impute = liver_train[-missing][[&quot;Albumin&quot;, &quot;Globulin&quot;]] y_impute = liver_train[-missing][&quot;TP&quot;].values lin_reg = sm.OLS(y_impute, sm.add_constant(X_impute)).fit() beta = lin_reg.params.values print(&quot;Adjusted R-squared: {:.3f}%&quot;.format(100*lin_reg.rsquared_adj)) print(&quot;Estimate:&quot;, beta) . . Adjusted R-squared: 90.619% Estimate: [0.4775957 1.06058543 0.77073515] . The Adjusted R-squared indicating that the model is a good fit. Now, we have to change the subject of the formula from TP to Globulin as follows: . $TP = beta_0 + beta_1*Albumin + beta_2*Globulin$ . $Globulin = dfrac{TP - beta_0 - beta_1*Albumin}{ beta_2}$ . where: . $ beta_0$ is the intercept of regression line. $TP$ is equal to $ beta_0$ when $Albumin = 0$ and $Globulin = 0$ | $ beta_1$ is the coefficient of $Albumin$. One unit increase in $Albumin$ will increase $TP$ as much as $ beta_1$ | $ beta_2$ is the coefficient of $Globulin$. One unit increase in $Globulin$ will increase $TP$ as much as $ beta_2$ | . We will use this formula to impute the missing value of Globulin based on TP and Albumin. . liver[&quot;Globulin&quot;] = liver.apply( lambda row: (row[&quot;TP&quot;] - beta[0] - beta[1] * row[&quot;Albumin&quot;]) / beta[2] if np.isnan(row[&quot;Globulin&quot;]) else row.Globulin, axis=1 ) liver[missing] . . Age Gender ALP ALT AST TP Albumin Disease DB/TB Percentage Globulin . 209 45 | Female | 189 | 23 | 33 | 6.60000 | 3.90000 | Yes | 33.33333 | 2.57692 | . 241 51 | Male | 230 | 24 | 46 | 6.50000 | 3.10000 | Yes | 25.00000 | 3.54803 | . 253 35 | Female | 180 | 12 | 15 | 5.20000 | 2.70000 | No | 33.33333 | 2.41175 | . 312 27 | Male | 106 | 25 | 54 | 8.50000 | 4.80000 | No | 46.15385 | 3.80363 | . We drop TP as it strongly correlated to Albumin and Globulin. . liver = liver.drop([&quot;TP&quot;], axis=1) liver.head() . . Age Gender ALP ALT AST Albumin Disease DB/TB Percentage Globulin . 0 65 | Female | 187 | 16 | 18 | 3.30000 | Yes | 14.28571 | 3.66667 | . 1 62 | Male | 699 | 64 | 100 | 3.20000 | Yes | 50.45872 | 4.32432 | . 2 62 | Male | 490 | 60 | 68 | 3.30000 | Yes | 56.16438 | 3.70787 | . 3 58 | Male | 182 | 14 | 20 | 3.40000 | Yes | 40.00000 | 3.40000 | . 4 72 | Male | 195 | 27 | 59 | 2.40000 | Yes | 51.28205 | 6.00000 | . Exploratory Data Analysis . Our data is ready, now we visualize the distribution and proportion of the variables. . Pair Plot . pair_plot = sns.pairplot(liver, hue=&quot;Disease&quot;, diag_kind=&quot;kde&quot;, corner=True, markers=&#39;+&#39;) pair_plot.fig.suptitle(&quot;PAIR PLOT OF NUMERICAL VARIABLES&quot;, size=25, fontweight=&quot;bold&quot;) pair_plot; . . . Note: From the pair plot of numerical variables, we expect little to no correlation between the independent variables, since we have done the feature engineering step except for ALT and AST. Variables ALP, ALT, AST, and DB/TB Percentage have a very positive skewed distribution. We&#8217;ll analyze them further using box plot. . Box Plot . fig, axes = plt.subplots(2, 4, figsize=(15, 8)) for ax, col in zip(axes.flat, liver.select_dtypes(&#39;number&#39;).columns): sns.boxplot(x=&quot;Disease&quot;, y=col, data=liver, ax=ax) # Outlier Count outlier_count = 0 for disease in liver[&quot;Disease&quot;].cat.categories: liver_disease = liver.loc[liver[&quot;Disease&quot;] == disease, col] outlier_list = boxplot_stats(liver_disease).pop(0)[&#39;fliers&#39;] outlier_count += len(outlier_list) ax.set_title(&quot;Outlier Count: {} ({:.2f}%)&quot;.format( outlier_count, 100*outlier_count/liver.shape[0])) axes[-1, -1].axis(&quot;off&quot;) plt.tight_layout() fig.suptitle(&quot;BOX PLOT OF NUMERICAL VARIABLES&quot;, size=28, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . . Note: Using median, patient who has liver disease have higher Age, lower Albumin, and slightly higher Globulin compared to those who doesn&#8217;t. This will be further explained in the modeling step. More than 10% of the observations in ALP, ALT, and AST are considered as outliers. We&#8217;ll apply some transformation during the modeling to handle the outlier. . Modeling: Logistic Regression . Logistic regression is a classification algorithm used to fit a regression curve. $ log{ frac{p}{1-p}} = beta_0 + beta_1 X_1 + beta_2 X_2 + ... + beta_n X_n$ where: . $p$ is the probability of an observation to be in the positive class | $ frac{p}{1-p}$ is called as odds | $ log{ frac{p}{1-p}}$ is called as log-odds or logit | $X_1, X_2, ..., X_n$ are the predictors | $ beta_0$ is a constant value | $ beta_1, beta_2, ..., beta_n$ are the coefficient of $X_1, X_2, ..., X_n$ respectively | $n$ is the number of predictors | . The formula above can be re-written as a sigmoid curve: . $p = dfrac{1}{1 + e^{-z}}$ where $z = beta_0 + beta_1 X_1 + beta_2 X_2 + ... + beta_n X_n$ . z_plot = np.linspace(-10, 10) plt.plot(z_plot, 1/(1 + np.exp(-z_plot))) plt.axvline(0, color=&quot;k&quot;, ls=&quot;--&quot;, alpha=0.25) plt.axhline(0.5, color=&quot;k&quot;, ls=&quot;--&quot;, alpha=0.25) plt.xlabel(&quot;z&quot;) plt.ylabel(&quot;p&quot;) plt.title(&quot;ILLUSTRATION: SIGMOID CURVE&quot;, fontweight=&quot;bold&quot;) plt.show() . . Data Preparation . We have to do some data preparation specifically for modeling: . Create dummy variables for the categorical variables Gender and Disease | Separate the target variable from the predictors | Train test split in order to evaluate our model, with 75% train and 25% test | . liver_dummy = pd.get_dummies( liver, columns=liver.select_dtypes(&#39;category&#39;).columns, drop_first=True) X = liver_dummy.drop([&quot;Disease_Yes&quot;], axis=1) y = liver_dummy[&quot;Disease_Yes&quot;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=888) print(&quot;X Train:&quot;, X_train.shape) print(&quot;X Test:&quot;, X_test.shape) print(&quot;y Train:&quot;, y_train.shape) print(&quot;y Test:&quot;, y_test.shape) . . X Train: (437, 8) X Test: (146, 8) y Train: (437,) y Test: (146,) . Imbalance Data . Next we check the frequency of each levels in the target variable Disease by Gender . def plotProportion(data): data[&#39;Disease&#39;] = data[&#39;Disease&#39;].replace({0: &#39;No&#39;, 1: &#39;Yes&#39;}) ax = data.groupby([&#39;Disease&#39;, &#39;Set&#39;]).size().unstack().plot(kind=&#39;bar&#39;, stacked=True) for rect in ax.patches: height = rect.get_height() width = rect.get_width() padding = 0.25 ax.text(rect.get_x() + width - padding, rect.get_y() + height / 2, int(height), ha=&#39;center&#39;, va=&#39;center&#39;, color=&quot;white&quot;) data = pd.concat([ pd.DataFrame({&#39;Disease&#39;: y_train, &#39;Set&#39;: &#39;Training&#39;}), pd.DataFrame({&#39;Disease&#39;: y_test, &#39;Set&#39;: &#39;Testing&#39;}) ]) plotProportion(data) plt.title(&quot;PROPORTION OF TARGET VARIABLE&quot;, fontsize=14, fontweight=&quot;bold&quot;) plt.show() . . data_prop = pd.crosstab( index=data.Set, columns=data.Disease, margins=True, normalize=&quot;index&quot;) * 100 data_prop.round(2).astype(str) + &#39; %&#39; . . Disease No Yes . Set . Testing 28.77 % | 71.23 % | . Training 28.6 % | 71.4 % | . All 28.64 % | 71.36 % | . We consider our data as imbalanced with 71:29 ratio. Therefore, we can do either upsampling or downsampling to balance the positive and negative class of Disease. . Upsampling is a method to randomly subsample the observation from minority class to make the dataset balanced. | Downsampling is a method to randomly sample (with replacement) the observation from majority class to make the dataset balanced. | . If we choose to downsample the data, we only end up with a small number of observations and will lose some information from the data. Therefore, in this case we prefer to do upsampling using Synthetic Minority Over-sampling Technique (known as SMOTE). . from imblearn.over_sampling import SMOTE sampler = SMOTE(random_state=888) X_train_smote, y_train_smote = sampler.fit_resample(X_train, y_train) . data_smote = pd.concat([ pd.DataFrame({&#39;Disease&#39;: y_train_smote, &#39;Set&#39;: &#39;Training&#39;}), pd.DataFrame({&#39;Disease&#39;: y_test, &#39;Set&#39;: &#39;Testing&#39;}) ]) plotProportion(data_smote) plt.title(&quot;PROPORTION OF TARGET VARIABLE (UPSAMPLED)&quot;, fontsize=14, fontweight=&quot;bold&quot;) plt.show() . . data_smote_prop = pd.crosstab( index=data_smote.Set, columns=data_smote.Disease, margins=True, normalize=&quot;index&quot;) * 100 data_smote_prop.round(2).astype(str) + &#39; %&#39; . . Disease No Yes . Set . Testing 28.77 % | 71.23 % | . Training 50.0 % | 50.0 % | . All 45.97 % | 54.03 % | . . Important: Notice how the proportion of testing set is left untouched. The common practice is to do balancing data only on training set, since we want to put the testing set aside as an unseen data. . Base Model . As a starting point, let&#39;s us fit a logistic regression model with all existing predictors and see how it goes. . def fitLogisticRegression(X, y): model = sm.Logit(y, sm.add_constant(X)) result = model.fit() return result model_all = fitLogisticRegression(X_train_smote, y_train_smote) model_all.summary() . . Optimization terminated successfully. Current function value: 88.807037 Iterations 8 . Logit Regression Results Dep. Variable: y | No. Observations: 624 | . Model: Logit | Df Residuals: 615 | . Method: MLE | Df Model: 8 | . Date: Tue, 26 Oct 2021 | Pseudo R-squ.: inf | . Time: 21:03:57 | Log-Likelihood: -55416. | . converged: True | LL-Null: 0.0000 | . Covariance Type: nonrobust | LLR p-value: 1.000 | . | coef std err z P&gt;|z| [0.025 0.975] . const -3.7837 | 0.790 | -4.790 | 0.000 | -5.332 | -2.236 | . Age 0.0244 | 0.006 | 4.066 | 0.000 | 0.013 | 0.036 | . ALP 0.0013 | 0.001 | 1.936 | 0.053 | -1.6e-05 | 0.003 | . ALT 0.0112 | 0.005 | 2.458 | 0.014 | 0.002 | 0.020 | . AST 0.0069 | 0.003 | 2.249 | 0.025 | 0.001 | 0.013 | . Albumin -0.2874 | 0.135 | -2.133 | 0.033 | -0.551 | -0.023 | . DB/TB Percentage 0.0201 | 0.008 | 2.505 | 0.012 | 0.004 | 0.036 | . Globulin 0.3830 | 0.151 | 2.538 | 0.011 | 0.087 | 0.679 | . Gender_Male 0.7252 | 0.208 | 3.480 | 0.001 | 0.317 | 1.134 | . Can we further improve our logistic regression model? One way is to handle the outlier present in our predictors. . Handle Outliers . In this section, we&#39;ll be transforming the outliers present in these predictors: ALP, ALT, and AST. We will compare the following: . Log transformation | Square root transformation | fig, axes = plt.subplots(1, 2, figsize=(10, 5)) x_axis = np.linspace(1, 5000, 1000) y_axis = [np.log(x_axis), np.sqrt(x_axis)] title_list = [&quot;LOG: $y = log(x)$&quot;, &quot;SQUARE ROOT: $y = sqrt{x}$&quot;] for y, title, ax in zip(y_axis, title_list, axes): ax.plot(x_axis, y) ax.set_xlabel(&quot;Original Value&quot;) ax.set_ylabel(&quot;Transformed Value&quot;) ax.set_title(title) plt.tight_layout() fig.suptitle(&quot;TRANSFORMATION FUNCTION&quot;, size=28, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . transform_col = [&quot;ALP&quot;, &quot;ALT&quot;, &quot;AST&quot;] X_train_untransformed = X_train_smote.drop(transform_col, axis=1) X_test_untransformed = X_test.drop(transform_col, axis=1) . . Note: Logarithm and square root function can be used to handle outlier since it condensed the range of the original value (horizontal axis) to its transformed value (vertical axis). . Log Transformation . Transform the predictors ALP, ALT, and AST using the natural logarithm function: $y = log(x)$ . def transform(data, transform_col, func): data = data[transform_col].transform(func) data.columns = [f&quot;{func.__name__}_{col}&quot; for col in transform_col] return data X_train_smote_log = transform(X_train_smote, transform_col, np.log) X_test_log = transform(X_test, transform_col, np.log) X_train_smote_log.head() . . log_ALP log_ALT log_AST . 0 5.20949 | 4.51086 | 3.97029 | . 1 5.14166 | 3.09104 | 2.77259 | . 2 6.19441 | 4.74493 | 4.51086 | . 3 5.55296 | 4.38203 | 4.72739 | . 4 5.37064 | 5.24702 | 6.85646 | . . Note: In the case of using transformation function, we have to apply it to both training and testing set since the model expect the predictor value in the same range. . def boxPlotTransformedData(data, figsize=(10, 4)): fig, axes = plt.subplots(1, data.shape[1]-1, figsize=figsize) for ax, col in zip(axes, data.columns[:-1]): sns.boxplot(x=&quot;Disease&quot;, y=col, data=data, ax=ax) # Outlier Count outlier_count = 0 for flag in data[&quot;Disease&quot;].cat.categories: flag_disease = data.loc[data[&quot;Disease&quot;] == flag, col] outlier_list = boxplot_stats(flag_disease).pop(0)[&#39;fliers&#39;] outlier_count += len(outlier_list) ax.set_title(&quot;Outlier Count: {} ({:.2f}%)&quot;.format( outlier_count, 100*outlier_count/liver.shape[0])) plt.tight_layout() boxPlotTransformedData( pd.concat([ X_train_smote_log, pd.Series(y_train_smote, name=&quot;Disease&quot;).replace({0: &#39;No&#39;, 1: &#39;Yes&#39;}).astype(&#39;category&#39;) ], axis=1) ) plt.suptitle(&quot;BOX PLOT OF LOG-TRANSFORMED VARIABLES&quot;, size=25, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . The outlier count has been reduced, now let&#39;s fit the log-transformed data into the logistic regression model. . X_train_smote_log = pd.concat([X_train_untransformed, X_train_smote_log], axis=1) X_test_log = pd.concat([X_test_untransformed, X_test_log], axis=1) model_log = fitLogisticRegression(X_train_smote_log, y_train_smote) model_log.summary() . . Optimization terminated successfully. Current function value: 90.611754 Iterations 6 . Logit Regression Results Dep. Variable: y | No. Observations: 624 | . Model: Logit | Df Residuals: 615 | . Method: MLE | Df Model: 8 | . Date: Tue, 26 Oct 2021 | Pseudo R-squ.: inf | . Time: 21:03:58 | Log-Likelihood: -56542. | . converged: True | LL-Null: 0.0000 | . Covariance Type: nonrobust | LLR p-value: 1.000 | . | coef std err z P&gt;|z| [0.025 0.975] . const -8.8601 | 1.409 | -6.290 | 0.000 | -11.621 | -6.099 | . Age 0.0253 | 0.006 | 4.196 | 0.000 | 0.013 | 0.037 | . Albumin -0.2775 | 0.135 | -2.055 | 0.040 | -0.542 | -0.013 | . DB/TB Percentage 0.0217 | 0.008 | 2.712 | 0.007 | 0.006 | 0.037 | . Globulin 0.3583 | 0.151 | 2.371 | 0.018 | 0.062 | 0.655 | . Gender_Male 0.7102 | 0.209 | 3.396 | 0.001 | 0.300 | 1.120 | . log_ALP 0.5216 | 0.236 | 2.206 | 0.027 | 0.058 | 0.985 | . log_ALT 0.6592 | 0.230 | 2.863 | 0.004 | 0.208 | 1.111 | . log_AST 0.2873 | 0.200 | 1.439 | 0.150 | -0.104 | 0.679 | . Square Root Transformation . Transform the predictors ALP, ALT, and AST using the square root function: $y = sqrt{x}$ . X_train_smote_sqrt = transform(X_train_smote, transform_col, np.sqrt) X_test_sqrt = transform(X_test, transform_col, np.sqrt) X_train_smote_sqrt.head() . . sqrt_ALP sqrt_ALT sqrt_AST . 0 13.52775 | 9.53939 | 7.28011 | . 1 13.07670 | 4.69042 | 4.00000 | . 2 22.13594 | 10.72381 | 9.53939 | . 3 16.06238 | 8.94427 | 10.63015 | . 4 14.66288 | 13.78405 | 30.82207 | . boxPlotTransformedData( pd.concat([ X_train_smote_sqrt, pd.Series(y_train_smote, name=&quot;Disease&quot;).replace({0: &#39;No&#39;, 1: &#39;Yes&#39;}).astype(&#39;category&#39;) ], axis=1) ) plt.suptitle(&quot;BOX PLOT OF SQRT-TRANSFORMED VARIABLES&quot;, size=25, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . The outlier count has been reduced. Now, let&#39;s fit the square root transformed data into the logistic regression model. . X_train_smote_sqrt = pd.concat([X_train_untransformed, X_train_smote_sqrt], axis=1) X_test_sqrt = pd.concat([X_test_untransformed, X_test_sqrt], axis=1) model_sqrt = fitLogisticRegression(X_train_smote_sqrt, y_train_smote) model_sqrt.summary() . . Optimization terminated successfully. Current function value: 90.255749 Iterations 7 . Logit Regression Results Dep. Variable: y | No. Observations: 624 | . Model: Logit | Df Residuals: 615 | . Method: MLE | Df Model: 8 | . Date: Tue, 26 Oct 2021 | Pseudo R-squ.: inf | . Time: 21:03:59 | Log-Likelihood: -56320. | . converged: True | LL-Null: 0.0000 | . Covariance Type: nonrobust | LLR p-value: 1.000 | . | coef std err z P&gt;|z| [0.025 0.975] . const -5.2668 | 0.868 | -6.067 | 0.000 | -6.968 | -3.565 | . Age 0.0251 | 0.006 | 4.155 | 0.000 | 0.013 | 0.037 | . Albumin -0.2765 | 0.135 | -2.048 | 0.041 | -0.541 | -0.012 | . DB/TB Percentage 0.0201 | 0.008 | 2.512 | 0.012 | 0.004 | 0.036 | . Globulin 0.3677 | 0.151 | 2.431 | 0.015 | 0.071 | 0.664 | . Gender_Male 0.7173 | 0.209 | 3.430 | 0.001 | 0.307 | 1.127 | . sqrt_ALP 0.0568 | 0.026 | 2.149 | 0.032 | 0.005 | 0.109 | . sqrt_ALT 0.1758 | 0.068 | 2.577 | 0.010 | 0.042 | 0.310 | . sqrt_AST 0.1010 | 0.052 | 1.953 | 0.051 | -0.000 | 0.202 | . Evaluation . Next, we evaluate our binary logistic regression classifier by using a confusion matrix as follows: . pd.DataFrame( [[&quot;True Positive (TP)&quot;, &quot;False Negative (FN)&quot;], [&quot;False Positive (FP)&quot;, &quot;True Negative (TN)&quot;]], index = [[&quot;Actual&quot;, &quot;Actual&quot;], [&quot;Positive&quot;, &quot;Negative&quot;]], columns = [[&quot;Predicted&quot;, &quot;Predicted&quot;], [&quot;Positive&quot;, &quot;Negative&quot;]], ) . . Predicted . Positive Negative . Actual Positive True Positive (TP) | False Negative (FN) | . Negative False Positive (FP) | True Negative (TN) | . There are several metrics considered in this case: . Recall is the proportion of actual positives that are classified correctly. | . $Recall = dfrac{TP}{TP+FN}$ Precision is the proportion of TP out of all observations predicted as positive. | . $Precision = dfrac{TP}{TP+FP}$ F1-score is the harmonic mean of precision and recall. | . $F1 = dfrac{2 times Precision times Recall}{Precision+Recall}$But Precision, Recall, F1-score do not account for the TN. We&#39;ll introduce another metrics that measure the overall performance of the model which includes all values in the confusion matrix: . Accuracy is the proportion of observations that are classified correctly. This metric is not robust when the class is imbalance. | . $Accuracy = dfrac{TP+TN}{TP+TN+FP+FN}$ MCC, stands for Matthew&#39;s Correlation Coefficient, ranges between -1 and 1. . MCC = 1 indicates a perfect positive correlation, the classifier is perfect $(FP = FN = 0)$ | MCC = -1 indicates a perfect negative correlation, the classifier always misclassifies $(TP = TN = 0)$ . | MCC = 0 indicates no correlation, the classifier randomly classify observations . | . | . $MCC = dfrac{TP times TN - FP times FN}{ sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$ def evaluateLogReg(result, X_true, y_true): eval_list = [] y_pred_prob = result.predict(sm.add_constant(X_true)) for threshold in np.linspace(0, 0.99, 100): y_pred_cl = (y_pred_prob &gt; threshold).astype(int) eval_res = { &quot;Threshold&quot;: threshold, &quot;Recall&quot;: recall_score(y_true, y_pred_cl), &quot;Precision&quot;: precision_score(y_true, y_pred_cl), &quot;F1&quot;: f1_score(y_true, y_pred_cl), &quot;Accuracy&quot;: accuracy_score(y_true, y_pred_cl), &quot;MCC&quot;: matthews_corrcoef(y_true, y_pred_cl) } eval_list.append(eval_res) eval_df = pd.DataFrame(eval_list) return eval_df eval_logreg_all = evaluateLogReg(model_all, X_test, y_test) eval_logreg_log = evaluateLogReg(model_log, X_test_log, y_test) eval_logreg_sqrt = evaluateLogReg(model_sqrt, X_test_sqrt, y_test) eval_logreg_list = [eval_logreg_all, eval_logreg_log, eval_logreg_sqrt] . . title_list = [&quot;ALL PREDICTORS&quot;, &quot;LOG-TRANSFORMED&quot;, &quot;SQRT-TRANSFORMED&quot;] fig, axes = plt.subplots(3, 1, figsize=(15, 10)) thresh_list = [] for ax, eval_df, title in zip(axes.flat, eval_logreg_list, title_list): # LINE PLOT eval_df = eval_df.drop([&quot;Accuracy&quot;], axis=1) lineplot = eval_df.plot(x=&quot;Threshold&quot;, color=&quot;rgbk&quot;, legend=False, ax=ax) # IDENTIFY CENTER diff = abs(eval_df[&quot;Recall&quot;] - eval_df[&quot;Precision&quot;]) thresh_eq = eval_df[diff == min(diff)][&quot;Threshold&quot;].values[0] ax.axvline(x=thresh_eq, ls=&#39;--&#39;, color=&quot;y&quot;) ax.text(x=thresh_eq + 0.01, y=0.05, s=&quot;CENTER&quot;, fontsize=12, color=&quot;y&quot;) # F1 MEASURE row_max_F1 = eval_df[eval_df[&quot;F1&quot;] == max(eval_df[&quot;F1&quot;])] thresh_max_F1 = row_max_F1[&quot;Threshold&quot;].values[0] ax.axvline(x=thresh_max_F1, ls=&#39;--&#39;, color=&quot;b&quot;) ax.text(x=thresh_max_F1 - 0.01, y=0.7, s=&quot;MAX F1&quot;, horizontalalignment=&#39;right&#39;, fontsize=12, color=&quot;b&quot;) # LOCATE MCC mcc = row_max_F1[&quot;MCC&quot;].values[0] ax.plot(thresh_max_F1, mcc, marker=&#39;x&#39;, markersize=10, color=&quot;k&quot;) ax.text(x=thresh_max_F1 - 0.025, y=mcc, s=&quot;MCC = {:.3f}&quot;.format(mcc), horizontalalignment=&#39;right&#39;, fontsize=12, fontweight=&quot;bold&quot;, color=&quot;k&quot;) ax.set_xticks([0, 1] + [thresh_eq, thresh_max_F1]) ax.set_title(title, fontweight=&quot;bold&quot;) handles, labels = ax.get_legend_handles_labels() thresh_list.append(thresh_max_F1) plt.tight_layout() plt.legend(handles=handles, loc=&quot;center&quot;, bbox_to_anchor=(0.5, -0.5), shadow=True, ncol=4) fig.suptitle(&quot;LOGISTIC REGRESSION MODEL EVALUATION&quot;, size=28, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . The output of our logistic regression is $p$, the probability of a particular patient to be classified as having a liver disease. For each of the model, we iterate the threshold from 0 to 1. If $p &gt; threshold$, then the patient is classified as having a liver disease. Our goal is to find the optimum threshold by comparing metrics. Here&#39;s the thought process: . First, identify the center of threshold, where Recall is equal to Precision. | In this case, we want to minimize the case of False Negative, where patient with liver disease is predicted to be healthy. This can be a threat to one&#39;s life because they will not get the treatment. Therefore, we prioritize Recall over Precision, where the threshold is smaller than the center threshold. | In reality, we also do care about minimizing the case of False Positive too, where healthy patient is predicted to have liver disease. This can triggers panic, but not as severe as False Negative case. Therefore we have to choose the threshold with maximum value of F1 score which take account the Precision value. | Lastly, out of the four models, we pick the best overall model by the highest MCC value. | . eval_logreg_df = pd.concat([eval_df[eval_df[&quot;Threshold&quot;] == thresh_list[idx]] for idx, eval_df in enumerate(eval_logreg_list)]) eval_logreg_df.index = title_list eval_logreg_df . . Threshold Recall Precision F1 Accuracy MCC . ALL PREDICTORS 0.19000 | 0.98077 | 0.73913 | 0.84298 | 0.73973 | 0.24591 | . LOG-TRANSFORMED 0.20000 | 0.98077 | 0.75556 | 0.85356 | 0.76027 | 0.33453 | . SQRT-TRANSFORMED 0.19000 | 0.98077 | 0.74453 | 0.84647 | 0.74658 | 0.27750 | . In conclusion, we choose logistic regression model with log-transformed data at threshold = 0.2. Next, we interpret and check the assumptions of this model. . Model Interpretation . One of the advantages of logistic regression model is its interpretability. We can interpret the coefficient and its significancy to the target variable. . final_model = model_log final_model.summary() . Logit Regression Results Dep. Variable: y | No. Observations: 624 | . Model: Logit | Df Residuals: 615 | . Method: MLE | Df Model: 8 | . Date: Tue, 26 Oct 2021 | Pseudo R-squ.: inf | . Time: 21:04:01 | Log-Likelihood: -56542. | . converged: True | LL-Null: 0.0000 | . Covariance Type: nonrobust | LLR p-value: 1.000 | . | coef std err z P&gt;|z| [0.025 0.975] . const -8.8601 | 1.409 | -6.290 | 0.000 | -11.621 | -6.099 | . Age 0.0253 | 0.006 | 4.196 | 0.000 | 0.013 | 0.037 | . Albumin -0.2775 | 0.135 | -2.055 | 0.040 | -0.542 | -0.013 | . DB/TB Percentage 0.0217 | 0.008 | 2.712 | 0.007 | 0.006 | 0.037 | . Globulin 0.3583 | 0.151 | 2.371 | 0.018 | 0.062 | 0.655 | . Gender_Male 0.7102 | 0.209 | 3.396 | 0.001 | 0.300 | 1.120 | . log_ALP 0.5216 | 0.236 | 2.206 | 0.027 | 0.058 | 0.985 | . log_ALT 0.6592 | 0.230 | 2.863 | 0.004 | 0.208 | 1.111 | . log_AST 0.2873 | 0.200 | 1.439 | 0.150 | -0.104 | 0.679 | . Recall that the formula of logistic regression is $ log{ frac{p}{1-p}} = beta_0 + beta_1 X_1 + beta_2 X_2 + ... + beta_n X_n$ where the coefficients are exactly the $ beta$ s. It means that coefficients represent the change in logit value for one unit increase in the predictor. For example, one unit increase of Age will increase the logit value by 0.0196 whereas one unit increase of Albumin will decrease the logit value by 0.2576. . We can apply exponent to the coefficient to get the ratio of odds. This represent how many percent increase in the odds for one unit increase in the predictor. Let&#39;s take a look on the table below. . model_interpret = pd.DataFrame(final_model.params, columns=[&quot;Logit Difference&quot;]) model_interpret[&quot;Ratio of Odds&quot;] = model_interpret[&quot;Logit Difference&quot;].transform(np.exp) model_interpret . . Logit Difference Ratio of Odds . const -8.86010 | 0.00014 | . Age 0.02533 | 1.02565 | . Albumin -0.27747 | 0.75770 | . DB/TB Percentage 0.02174 | 1.02198 | . Globulin 0.35831 | 1.43090 | . Gender_Male 0.71016 | 2.03432 | . log_ALP 0.52158 | 1.68469 | . log_ALT 0.65923 | 1.93330 | . log_AST 0.28731 | 1.33284 | . The ratio of odds of Age is 1.02565 means that for one unit increase in Age, we expect to see 2.565% increase in the odds of a patient having liver disease. On the other hand, the ratio of odds of Albumin is 0.75770 means that for one unit increase in Albumin, we expect to see 24.23% decrease in the odds of a patient having liver disease. . . Note: The table above shows the increase/decrease in the odds not probability. The ratio of probability changes depending on the predictor value as follows: . for age in range(20, 23): print(f&quot; nComparison of Age {age} with Age {age+1}&quot;) print(&quot;=================================&quot;) interpret_df = pd.DataFrame([[1, age, 5, 50, 5, 1, 5, 5, 5], [1, age+1, 5, 50, 5, 1, 5, 5, 5]], columns=[&quot;const&quot;] + list(X_train_smote_log.columns)) prob_interpret = final_model.predict(interpret_df) logit_interpret = prob_interpret.transform(logit) odds_20 = np.exp(logit_interpret[0]) odds_21 = np.exp(logit_interpret[1]) print(&quot;Logit Difference: {:.5f}&quot;.format( logit_interpret[1] - logit_interpret[0])) print(&quot;Ratio of Odds: {:.5f}&quot;.format(odds_21/odds_20)) print(&quot;Ratio of Probability: {:.5f}&quot;.format( prob_interpret[1]/prob_interpret[0])) . . Comparison of Age 20 with Age 21 ================================= Logit Difference: 0.02533 Ratio of Odds: 1.02565 Ratio of Probability: 1.00587 Comparison of Age 21 with Age 22 ================================= Logit Difference: 0.02533 Ratio of Odds: 1.02565 Ratio of Probability: 1.00576 Comparison of Age 22 with Age 23 ================================= Logit Difference: 0.02533 Ratio of Odds: 1.02565 Ratio of Probability: 1.00565 . How to interpret the coefficient of log_ALP, log_ALT, and log_AST? The original value must be log-transformed first, then interpreted just like above. Here&#39;s the illustration of how one unit increase of log-transformed value looks like: . x_range = (1, 4.25) x_axis = np.linspace(*x_range, 100) x_ticks = np.arange(*x_range) y_ticks = np.exp(x_ticks) plt.plot(x_axis, np.exp(x_axis)) plt.scatter(x_ticks, y_ticks, marker=&quot;x&quot;) for x, y in zip(x_ticks, y_ticks): plt.axvline(x, color=&quot;k&quot;, ls=&quot;--&quot;, alpha=0.25) plt.axhline(y, color=&quot;k&quot;, ls=&quot;--&quot;, alpha=0.25) plt.xticks(x_ticks) plt.yticks(y_ticks) plt.xlabel(&quot;Log-Transformed Value&quot;) plt.ylabel(&quot;Original Value&quot;) plt.title(&quot;ILLUSTRATION: ONE UNIT INCREASE OF LOG&quot;, fontweight=&quot;bold&quot;) plt.show() . . Model Assumptions . There are three assumptions of logistic regression model: . Linearity . We assume that the independent variables (predictors) are linearly related to the log odds of the target variable Disease, meaning that the data is assumed to be linearly separable. . No Multicollinearity . We expect the model to have little to no multicollinearity. It is a condition where at least two predictors have a strong linear relationship. Multicollinearity exists if the Variance Inflation Factor (VIF) value is greater than 10. . vif_list = [] for idx, col in enumerate(final_model.model.exog_names[1:]): vif_dict = {&quot;Variable&quot;: col, &quot;VIF&quot;: variance_inflation_factor(final_model.model.exog, idx+1)} vif_list.append(vif_dict) pd.DataFrame(vif_list) . . Variable VIF . 0 Age | 1.12372 | . 1 Albumin | 1.21778 | . 2 DB/TB Percentage | 1.07372 | . 3 Globulin | 1.12135 | . 4 Gender_Male | 1.11552 | . 5 log_ALP | 1.29119 | . 6 log_ALT | 3.48703 | . 7 log_AST | 3.47282 | . . Note: We can conclude that there is only little multicollinearity exist in our model, since the VIFs &lt; 10 . Independence of Observation . We assume that the observations are independent and are not a repeated measurement of the same patient. . Modeling: K-Nearest Neighbor . K-Nearest Neighbor (KNN) is a non-parametric, lazy learning classification algorithm. No model are being learned, it uses the training examples to classify new data points. Here&#39;s the algorithm: . Specify a positive integer $k$ | Compute the similarity/distance from the unlabeled data point to every points on the training examples. The most commonly used is Euclidean distance. | Assign the class by majority voting based on $k$-nearest training examples. If tie, then assign randomly. To avoid this, choose $k$ to be an odd positive number. | Repeat step 2-3 for each unlabeled data points in the test set. | . Note: Classical KNN is not suitable for categorical data, since the distance metric used is Euclidean distance. So, for this case we drop the dummy variable. . X_train_knn = X_train_smote.drop(columns=[&#39;Gender_Male&#39;]) X_test_knn = X_test.drop(columns=[&#39;Gender_Male&#39;]) . Feature Scaling . This is a necessary step before fitting the data into KNN. The range of each predictor is not the same, they should be treated equally so that each feature’s contribution to the distance formula is equally weighted. We will perform two types of scaling to shrink/expand the range: . Min-Max Normalization, scale a variable to have a range between 0 and 1 | Standardization, transforms data to have a mean of 0 and a standard deviation of 1 | X_train_knn.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . Age ALP ALT AST Albumin DB/TB Percentage Globulin . min 4.00000 | 63.00000 | 10.00000 | 10.00000 | 0.90000 | 4.80000 | 1.00000 | . max 90.00000 | 2110.00000 | 2000.00000 | 4929.00000 | 5.50000 | 500.00000 | 6.25000 | . def scaling(train, test, scaler): s = scaler().fit(train) train_scaled = pd.DataFrame(s.transform(train), columns=train.columns) test_scaled = pd.DataFrame(s.transform(test), columns=test.columns) return train_scaled, test_scaled . . . Important: We only call .fit() method on training data, since we treat testing data as unseen data, which the statistics is unknown to the model. . # min-max scaler X_train_knn_minmax, X_test_knn_minmax = scaling(X_train_knn, X_test_knn, MinMaxScaler) X_train_knn_minmax.describe().loc[[&#39;min&#39;, &#39;max&#39;]] . . Age ALP ALT AST Albumin DB/TB Percentage Globulin . min 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | . max 1.00000 | 1.00000 | 1.00000 | 1.00000 | 1.00000 | 1.00000 | 1.00000 | . Using min-max scaler, the min and max of all variables are 0 and 1 respectively. . # standard scaler X_train_knn_std, X_test_knn_std = scaling(X_train_knn, X_test_knn, StandardScaler) X_train_knn_std.describe().loc[[&#39;mean&#39;, &#39;std&#39;]] . . Age ALP ALT AST Albumin DB/TB Percentage Globulin . mean 0.00000 | -0.00000 | 0.00000 | 0.00000 | 0.00000 | 0.00000 | -0.00000 | . std 1.00080 | 1.00080 | 1.00080 | 1.00080 | 1.00080 | 1.00080 | 1.00080 | . Using standard scaler, the mean and standard deviation of all variables are 0 and 1 respectively. . Evaluation . Since there are no model to be learned, we directly evaluate our binary KNN classifier by using a confusion matrix as follows: . def evaluateKNN(X_train, y_train, X_test, y_test, range_k): eval_list = [] for k in range(*range_k, 2): knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) y_pred_cl = knn.predict(X_test) eval_res = { &quot;k&quot;: k, &quot;Recall&quot;: recall_score(y_test, y_pred_cl), &quot;Precision&quot;: precision_score(y_test, y_pred_cl), &quot;F1&quot;: f1_score(y_test, y_pred_cl), &quot;Accuracy&quot;: accuracy_score(y_test, y_pred_cl), &quot;MCC&quot;: matthews_corrcoef(y_test, y_pred_cl) } eval_list.append(eval_res) eval_df = pd.DataFrame(eval_list) return eval_df . . range_k = (1, 50) # min-max scaler eval_knn_minmax = evaluateKNN( X_train_knn_minmax, y_train_smote, X_test_knn_minmax, y_test, range_k) # standard scaler eval_knn_std = evaluateKNN( X_train_knn_std, y_train_smote, X_test_knn_std, y_test, range_k) . . Next, we compare the performance of the two scaling types based on Recall, F1-score, and MCC. . metric_list = [&quot;Recall&quot;, &quot;F1&quot;, &quot;MCC&quot;] fig, axes = plt.subplots(1, 3, figsize=(15, 5)) eval_minmax = pd.melt(eval_knn_minmax, id_vars=&quot;k&quot;, var_name=&quot;Metric&quot;, value_name=&quot;Value&quot;) eval_minmax[&quot;Scaling Type&quot;] = &quot;Min-Max&quot; eval_standard = pd.melt(eval_knn_std, id_vars=&quot;k&quot;, var_name=&quot;Metric&quot;, value_name=&quot;Value&quot;) eval_standard[&quot;Scaling Type&quot;] = &quot;Standard&quot; eval_df = pd.concat([eval_minmax, eval_standard]) for ax, metric in zip(axes.flat, metric_list): df = eval_df[eval_df[&quot;Metric&quot;] == metric] line = sns.lineplot(data=df, x=&quot;k&quot;, y=&quot;Value&quot;, hue=&quot;Scaling Type&quot;, palette=&quot;gray&quot;, ax=ax) line.legend_.remove() ax.set_title(metric.upper(), size=15, fontweight=&quot;bold&quot;) plt.tight_layout() plt.legend(handles=axes[0].get_legend_handles_labels()[0], loc=&quot;center&quot;, bbox_to_anchor=(-0.7, -0.2), shadow=True, ncol=3) fig.suptitle(&quot;K-NEAREST NEIGHBOR MODEL EVALUATION&quot;, size=28, y=1.05, fontweight=&quot;bold&quot;) plt.show() . . As explained on the model evaluation of logistic regression, we prioritize Recall over Precision and taking maximum value of F1 score. So, we choose the model with Standard Scaling data because the Recall and F1 is relatively higher than the min-max one. Next, we choose the optimal number of neighbors $k$. . fig, ax = plt.subplots(figsize=(8, 5)) eval_df = eval_knn_std.drop([&quot;Accuracy&quot;], axis=1) # LINE PLOT eval_df.plot(x=&quot;k&quot;, color=&quot;rgbk&quot;, ax=ax) # F1 SCORE # row_max_F1 = eval_df[eval_df[&quot;F1&quot;] == max(eval_df[&quot;F1&quot;])] # k_max_F1 = row_max_F1[&quot;k&quot;].values[0] # ax.axvline(x=k_max_F1, ls=&#39;--&#39;, color=&quot;b&quot;) # ax.text(x=k_max_F1 + 5, y=0.2, # s=&quot;MAX F1&quot;, # fontsize=12, color=&quot;b&quot;) # ax.set_xticks(list(ax.get_xticks())[1:-1] + [k_max_F1]) # SQRT NROWS sqrt_n = int(X_train_smote.shape[0] ** 0.5) ax.axvline(x=sqrt_n, ls=&#39;--&#39;, color=&#39;b&#39;) ax.text(x=sqrt_n + 1, y=0.2, s=&quot;$ sqrt{nrows}$&quot;, fontsize=12, color=&quot;b&quot;) # CHOOSE K MANUALLY choose_k = 5 ax.axvline(x=choose_k, ls=&#39;--&#39;, color=&#39;y&#39;) ax.set_xticks(list(ax.get_xticks())[1:-1] + [sqrt_n, choose_k]) plt.legend(loc=&quot;center&quot;, bbox_to_anchor=(0.5, -0.2), shadow=True, ncol=4) fig.suptitle(&quot;K-NEAREST NEIGHBOR METRICS (STANDARDIZATION)&quot;, size=16, fontweight=&quot;bold&quot;) plt.show() . . Most common value of $k$ to be used is $ sqrt{nrows}$, which is 24 in this case but it doesn&#39;t result in good metrics. From the plot, we can choose $k$ manually as k=5 to sacrifice Precision for a better Recall score. . eval_knn_list = [eval_knn_minmax, eval_knn_std] eval_knn_df = pd.concat([eval_df[eval_df[&quot;k&quot;] == choose_k] for eval_df in eval_knn_list]) eval_knn_df.index = [&quot;MIN-MAX NORMALIZATION&quot;, &quot;STANDARDIZATION&quot;] eval_knn_df . . k Recall Precision F1 Accuracy MCC . MIN-MAX NORMALIZATION 5 | 0.50962 | 0.76812 | 0.61272 | 0.54110 | 0.11666 | . STANDARDIZATION 5 | 0.55769 | 0.84058 | 0.67052 | 0.60959 | 0.26820 | . In this case, using the same number of neighbors $k$, standardization is overall outperforming the min-max normalization. . Conclusion . Finally, let&#39;s compare the perfomance of logistic regression model and k-nearest neighbor side by side. . pd.DataFrame( [eval_logreg_df.loc[&quot;LOG-TRANSFORMED&quot;][1:], eval_knn_df.loc[&quot;STANDARDIZATION&quot;][1:]] ) . . Recall Precision F1 Accuracy MCC . LOG-TRANSFORMED 0.98077 | 0.75556 | 0.85356 | 0.76027 | 0.33453 | . STANDARDIZATION 0.55769 | 0.84058 | 0.67052 | 0.60959 | 0.26820 | . The logistic regression with log-transformed data is better in recall but worse in precision. . To further evaluate our model, let&#39;s introduce another performance measurement which is the Receiver Operating Characteristic (ROC) curve. It is a probability curve which plots True Positive Rate (Recall) against False Positive Rate. Then the area under the ROC curve, called as Area Under Curve (AUC), measures the degree of classification separability. Higher the AUC, better the model is at distinguishing between patients with liver disease and no disease. . AUC near to 1 indicates the model has good measure of separability. | AUC near to 0 means it has worst measure of separability. | When AUC is 0.5, it means model has no class separation capacity whatsoever. | . # BASELINE base_probs = np.zeros(len(y_test)) base_fpr, base_tpr, _ = roc_curve(y_test, base_probs) base_auc = roc_auc_score(y_test, base_probs) # LOGISTIC REGRESSION logreg_probs = model_log.predict(sm.add_constant(X_test_log)) logreg_fpr, logreg_tpr, _ = roc_curve(y_test, logreg_probs) logreg_auc = roc_auc_score(y_test, logreg_probs) # KNN knn_opt_model = KNeighborsClassifier(n_neighbors=choose_k).fit(X_train_knn_std, y_train_smote) knn_probs = knn_opt_model.predict_proba(X_test_knn_minmax)[:, 0] knn_fpr, knn_tpr, _ = roc_curve(y_test, knn_probs) knn_auc = roc_auc_score(y_test, knn_probs) # PLOT ROC plt.plot(base_fpr, base_tpr, linestyle=&#39;--&#39;, label=&quot;Baseline (AUC = {:.3f})&quot;.format(base_auc)) plt.plot(logreg_fpr, logreg_tpr, linestyle=&#39;-&#39;, label=&quot;Logistic Regression (AUC = {:.3f})&quot;.format(logreg_auc), color=&quot;r&quot;) plt.plot(knn_fpr, knn_tpr, linestyle=&#39;-&#39;, label=&quot;KNN (AUC = {:.3f})&quot;.format(knn_auc), color=&quot;g&quot;) plt.xlabel(&quot;False Positive Rate&quot;) plt.ylabel(&quot;True Positive Rate (Recall)&quot;) plt.title(&quot;RECEIVER OPERATING CHARACTERISTIC (ROC) CURVE&quot;, fontweight=&quot;bold&quot;) plt.legend() plt.show() . . In conclusion, we choose the logistic regression model because the AUC score is greater than KNN, indicating that the model have a good capability in separating patients with liver disease and no disease no matter the threshold is. Secondly, the model&#39;s interpretability gives us insight on which predictors to be used in classifying whether patient have liver disease or not, whereas we couldn&#39;t interpret KNN. From the model summary, we can conclude: . Older patient have significantly higher probability of having a liver disease. | Higher Albumin present in the blood significantly decrease the probability of patient having a liver disease. | Being a male (Gender_Male = 1) significantly increase the probability of patient having a liver disease than female (Gender_Male = 0). | Other test results such as higher DB/TB Percentage, Globulin, ALP, ALT, AST will significantly increase the probability of patient having a liver disease. | . final_model.summary() . Logit Regression Results Dep. Variable: y | No. Observations: 624 | . Model: Logit | Df Residuals: 615 | . Method: MLE | Df Model: 8 | . Date: Tue, 26 Oct 2021 | Pseudo R-squ.: inf | . Time: 21:04:03 | Log-Likelihood: -56542. | . converged: True | LL-Null: 0.0000 | . Covariance Type: nonrobust | LLR p-value: 1.000 | . | coef std err z P&gt;|z| [0.025 0.975] . const -8.8601 | 1.409 | -6.290 | 0.000 | -11.621 | -6.099 | . Age 0.0253 | 0.006 | 4.196 | 0.000 | 0.013 | 0.037 | . Albumin -0.2775 | 0.135 | -2.055 | 0.040 | -0.542 | -0.013 | . DB/TB Percentage 0.0217 | 0.008 | 2.712 | 0.007 | 0.006 | 0.037 | . Globulin 0.3583 | 0.151 | 2.371 | 0.018 | 0.062 | 0.655 | . Gender_Male 0.7102 | 0.209 | 3.396 | 0.001 | 0.300 | 1.120 | . log_ALP 0.5216 | 0.236 | 2.206 | 0.027 | 0.058 | 0.985 | . log_ALT 0.6592 | 0.230 | 2.863 | 0.004 | 0.208 | 1.111 | . log_AST 0.2873 | 0.200 | 1.439 | 0.150 | -0.104 | 0.679 | . By using log-transformed data and threshold = 0.2, here&#39;s our final logistic regression model performance: . pd.DataFrame(eval_logreg_df.loc[&quot;LOG-TRANSFORMED&quot;][1:]) . . LOG-TRANSFORMED . Recall 0.98077 | . Precision 0.75556 | . F1 0.85356 | . Accuracy 0.76027 | . MCC 0.33453 | .",
            "url": "https://tomytjandra.github.io/blogs/python/classification/scikit-learn/statsmodels/2020/04/07/liver-disease-classification-logreg-knn.html",
            "relUrl": "/python/classification/scikit-learn/statsmodels/2020/04/07/liver-disease-classification-logreg-knn.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Medical Cost Analysis: Smoking Is Bad for You (and Your Insurance Charges)",
            "content": "Introduction . Business Problem . In this post, we will try to analyze the factor of medical cost for personal insurance (charges) using Multiple Linear Regression. This use case is provided on Kaggle: Medical Cost Personal Datasets and the dataset is available on GitHub: Data for Machine Learning with R. . Import Packages . First of all, let&#39;s us import packages used in this analysis, mainly for data analysis, visualization, and modeling. . import numpy as np import pandas as pd # visualization import seaborn as sns import matplotlib.pyplot as plt from IPython.display import display, HTML # modeling from scipy import stats from sklearn.model_selection import train_test_split import statsmodels.api as sm import sklearn.metrics as metrics from statsmodels.stats.outliers_influence import variance_inflation_factor . Set Notebook Options . Set color of plot to be contrast | Suppress warning of pandas chaining assignment | Change float format to three decimal places | Display all content in a pandas column | . sns.set(style=&quot;ticks&quot;, color_codes=True) pd.options.mode.chained_assignment = None pd.options.display.float_format = &#39;{:.3f}&#39;.format pd.options.display.max_colwidth = None . . Exploratory Data Analysis (EDA) . Let&#39;s us explore to better understand the provided data, by doing data preparation and visualization of the data. . Data Preparation . Before we jump into further step, we have to make sure our data is ready to be analyze. We import insurance.csv and analyze the data structure. . insurance = pd.read_csv(&quot;data_input/insurance.csv&quot;) insurance.head() . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.924 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.552 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.462 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.471 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.855 | . insurance.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1338 entries, 0 to 1337 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 age 1338 non-null int64 1 sex 1338 non-null object 2 bmi 1338 non-null float64 3 children 1338 non-null int64 4 smoker 1338 non-null object 5 region 1338 non-null object 6 charges 1338 non-null float64 dtypes: float64(2), int64(2), object(3) memory usage: 73.3+ KB . insurance is a DataFrame object with 1338 rows and 7 columns. There is no missing value present in our data. Here are the explanation for each columns: . age: age of primary beneficiary (in years) | sex: gender of insurance contractor, either female or male | bmi: Body Mass Index which provides an understanding of a body by using a number expressing the ratio of body weight (in kilograms) to height squared (in meters). The value of bmi is ideally between 18.5 and 24.9 | children: number of children/dependents covered by health insurance | smoker: whether the primary beneficiary smoking or not | region: the beneficiary&#39;s residential area in the US, either northeast, southeast, southwest, or northwest | charges: Individual medical costs billed by health insurance | . . Note: We will use charges as our target variable and the rest as the candidate predictors. . Based on the explanation above, we have to convert the data type of column sex, smoker, and region into categorical data. . insurance[[&#39;sex&#39;, &#39;smoker&#39;, &#39;region&#39;]] = insurance[[&#39;sex&#39;, &#39;smoker&#39;, &#39;region&#39;]].astype(&#39;category&#39;) insurance.dtypes . age int64 sex category bmi float64 children int64 smoker category region category charges float64 dtype: object . We investigate the levels for each categorical data . for col in insurance.select_dtypes(&#39;category&#39;).columns: print(f&quot;{col}: {insurance[col].cat.categories.values}&quot;) . . sex: [&#39;female&#39; &#39;male&#39;] smoker: [&#39;no&#39; &#39;yes&#39;] region: [&#39;northeast&#39; &#39;northwest&#39; &#39;southeast&#39; &#39;southwest&#39;] . Visualization . We visualize the numerical variables using pair plot. Plots on the diagonal represent the distribution of each variables, and the rest will be plotted as scatter plot. . pair_plot = sns.pairplot(insurance, diag_kind=&quot;kde&quot;, corner=True, markers=&#39;+&#39;, kind=&quot;reg&quot;) pair_plot.fig.suptitle(&quot;Pair Plot of Numerical Variables&quot;, size=25, y=1.05) pair_plot . . From the pair plot above, we couldn&#39;t get any interesting insight. How about the categorical variables? Let&#39;s us plot them with violin plot, which tell us the distribution of charges for categorical variable in each levels. . fig, axes = plt.subplots(1, 3, figsize=(15, 5)) for ax, col in zip(axes, insurance.select_dtypes(&#39;category&#39;).columns): sns.violinplot(x=col, y=&quot;charges&quot;, data=insurance, ax=ax) plt.tight_layout() fig.suptitle(&quot;Violin Plot of Categorical Variables&quot;, size=28, y=1.05) . . . Note: From the violin plot above, we can clearly see the difference of charges distribution based on smoker. The hypothesis is that beneficiary who is a smoker will have relatively high charges than those who aren&#8217;t. Is this statistically significant? We will try to analyze this on the modeling section. . To explore the data more, we create scatter plot which represent the distribution of numerical variables based on charges, distinguished by each categorical variables. . fig, axes = plt.subplots(3, 3, figsize=(15, 15)) for row, cat in enumerate(insurance.select_dtypes(&#39;category&#39;).columns): for col, num in enumerate(insurance.select_dtypes(np.number).columns[:-1]): sns.scatterplot(x=num, y=&quot;charges&quot;, hue=cat, data=insurance, alpha=0.6, ax=axes[row][col]) plt.tight_layout() fig.suptitle( &quot;Scatter Plot of Each Numerical and Categorical Variables&quot;, size=28, y=1.025) . . . Note: Three plots on the second row show us an interesting pattern. Just like the violin plot before, beneficiary who is a smoker will relatively have high charges no matter the values of age, bmi, or children. . So far, we only focus on the candidate predictors, but how about the distribution of the target variables itself? . y_boxplot = sns.boxplot(x=insurance[&#39;charges&#39;]) y_boxplot.set_title(&quot;Boxplot of Charges&quot;) y_boxplot; . . . Note: The dots which lie outside the whiskers are called as an outlier. They are data points which significantly differs from the other observations. . Next, we iteratively remove these outliers from the data and calculate exactly how many data points are considered as an outlier. . insurance_wo_outlier = insurance.copy() while True: y_boxplot = plt.boxplot(insurance_wo_outlier[&#39;charges&#39;]) lower_whisker, upper_whisker = [ item.get_ydata()[1] for item in y_boxplot[&#39;whiskers&#39;]] outlier_flag = (insurance_wo_outlier[&#39;charges&#39;] &lt; lower_whisker) | ( insurance_wo_outlier[&#39;charges&#39;] &gt; upper_whisker) num_outlier = sum(outlier_flag) if num_outlier == 0: before = insurance.shape[0] after = insurance_wo_outlier.shape[0] print(&quot;Total Outlier Removed: {}/{} ({}%)&quot;.format(before-after, before, round(100*(before-after)/before, 3))) print(&quot;Final Range: ({}, {})&quot;.format(lower_whisker, upper_whisker)) break print(&quot;Remove Outlier: {}/{} ({}%)&quot;.format(num_outlier, insurance_wo_outlier.shape[0], round(100*num_outlier/insurance_wo_outlier.shape[0], 3))) plt.show() insurance_wo_outlier = insurance_wo_outlier[-outlier_flag] . . Remove Outlier: 139/1338 (10.389%) . Remove Outlier: 55/1199 (4.587%) . Remove Outlier: 20/1144 (1.748%) . Remove Outlier: 6/1124 (0.534%) . Remove Outlier: 2/1118 (0.179%) . Total Outlier Removed: 222/1338 (16.592%) Final Range: (1121.8739, 23401.30575) . . Warning: Outliers are not meant to always be removed because they may be informative. Thus, we have to try handle it case-by-case. In the next modelling section, we will compare models with and without outliers. Besides removing outliers, one other way is to perform the transformation with the Box-Cox function, but this is not the focus of this post. . Correlation . In order to apply Linear Regression to our data, we have to check the correlation between each candidate predictors and charges as the target variable. Variable will not be a good predictor if it doesn&#39;t significantly correlated with the target variable. . Correlation is a statistical measure on how strong a linear relationship between two variables. The values ranged between -1.0 and 1.0, where: . Positive value = positive correlation, means one variable increases as the other variable increases, or vice versa. The closer to 1, the stronger the positive relationship. | Negative value = negative correlation, means one variable decreases as the other variable increases, or vice versa. The closer to -1, the stronger the negative relationship. | Zero = no correlation, means that a variable has nothing to do with the other variable. | . Pearson&#39;s Correlation Coefficient is one type of correlation which measures the linear relationship between two quantitative variables. . corr_heatmap = sns.heatmap(insurance.corr(method=&quot;pearson&quot;), annot=True, fmt=&#39;.3f&#39;, linewidths=5, cmap=&quot;Reds&quot;) corr_heatmap.set_title(&quot;Pearson Correlation&quot;, size=25) corr_heatmap . . . Note: The Pearson correlation between charges and the other numerical variables is positively correlated but not very high, means they have a weak linear relationship in the same direction. . Variable Assumption: Linearity . We also have to perform statistical significancy test to check the linearity of all candidate predictors to our target variable, including the categorical variables. Spearman&#39;s rank correlation is a non-parametric test which can be used to measure the degree of association between categorical and numerical variables. . Here is the hypothesis of the test: . Null Hypothesis ($H_0$): Correlation is not significant | Alternative Hypothesis ($H_1$): Correlation is significant | . def check_linearity(data, target_var, SL=0.05): cor_test_list = [] for col in data.drop(target_var, axis=1).columns: if col in data.select_dtypes(&#39;category&#39;).columns: cor_test = stats.spearmanr(data[col], data[target_var]) cor_type = &quot;Spearman&quot; else: cor_test = stats.pearsonr(data[col], data[target_var]) cor_type = &quot;Pearson&quot; cor_dict = {&quot;Predictor&quot;: col, &quot;Type&quot;: cor_type, &quot;Correlation&quot;: cor_test[0], &quot;P-Value&quot;: cor_test[1], &quot;Conclusion&quot;: &quot;significant&quot; if cor_test[1] &lt; SL else &quot;not significant&quot;} cor_test_list.append(cor_dict) return pd.DataFrame(cor_test_list) check_linearity(insurance, &quot;charges&quot;) . . Predictor Type Correlation P-Value Conclusion . 0 age | Pearson | 0.299 | 0.000 | significant | . 1 sex | Spearman | 0.009 | 0.729 | not significant | . 2 bmi | Pearson | 0.198 | 0.000 | significant | . 3 children | Pearson | 0.068 | 0.013 | significant | . 4 smoker | Spearman | 0.663 | 0.000 | significant | . 5 region | Spearman | -0.044 | 0.111 | not significant | . . Note: From the correlation test above, we can conclude that sex and region will not have significant correlation to charges. But is this statement also hold if we construct a Linear model from the data? We&#8217;ll see in the next section. . Modeling . From previous section, we can summarize: . Beneficiary who is a smoker have relatively high charges | Numerical candidate predictors (age, bmi, children) have positive correlation to charges | sex and region have no significant correlation to charges | We have to keep in mind about the outliers of charges | . In this section, we try to construct a linear model to further analyze and interpret the result. . Data Preparation . We have to do some data preparation specifically for modeling: . Separate the target variable from the predictors | Create dummy variables for the categorical predictors | Train test split in order to evaluate our model, with 80% train and 20% test | . X_raw = insurance.drop([&quot;charges&quot;], axis=1) y = insurance.charges.values # dummy variables X = pd.get_dummies(X_raw, columns=insurance.select_dtypes(&#39;category&#39;).columns, drop_first=True) # train-test split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=333) print(&quot;X Train:&quot;, X_train.shape) print(&quot;X Test:&quot;, X_test.shape) print(&quot;y Train:&quot;, y_train.shape) print(&quot;y Test:&quot;, y_test.shape) . X Train: (1070, 8) X Test: (268, 8) y Train: (1070,) y Test: (268,) . Linear model with all predictors . This is the equation of Multiple Linear Regression that we have to estimate: $ hat{Y} = beta_0 + beta_1 X_1 + beta_2 X_2 + ... + beta_n X_n$ where: . $ hat{Y}$ is the predicted value of target variable | $X_1, X_2, ..., X_n$ are the predictors | $ beta_0$ is the intercept / constant | $ beta_1, beta_2, ..., beta_n$ are the slope of respective predictors $X_1, X_2, ..., X_n$ | $n$ is the number of predictors | . As a starting point, let&#39;s us fit a linear model with all existing predictors and see how it goes. . model_all = sm.OLS(y_train, sm.add_constant(X_train)) result_all = model_all.fit() print(result_all.summary()) . OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.740 Model: OLS Adj. R-squared: 0.738 Method: Least Squares F-statistic: 378.2 Date: Thu, 21 Oct 2021 Prob (F-statistic): 2.00e-304 Time: 00:42:30 Log-Likelihood: -10846. No. Observations: 1070 AIC: 2.171e+04 Df Residuals: 1061 BIC: 2.175e+04 Df Model: 8 Covariance Type: nonrobust ==================================================================================== coef std err t P&gt;|t| [0.025 0.975] const -1.224e+04 1112.140 -11.010 0.000 -1.44e+04 -1.01e+04 age 267.4967 13.392 19.974 0.000 241.218 293.775 bmi 336.8498 32.103 10.493 0.000 273.857 399.843 children 406.3633 155.040 2.621 0.009 102.143 710.584 sex_male -219.7955 376.724 -0.583 0.560 -959.004 519.413 smoker_yes 2.369e+04 475.879 49.777 0.000 2.28e+04 2.46e+04 region_northwest -80.1959 532.812 -0.151 0.880 -1125.681 965.289 region_southeast -985.3338 546.166 -1.804 0.072 -2057.022 86.354 region_southwest -842.4895 538.704 -1.564 0.118 -1899.536 214.557 ============================================================================== Omnibus: 273.543 Durbin-Watson: 2.012 Prob(Omnibus): 0.000 Jarque-Bera (JB): 708.949 Skew: 1.329 Prob(JB): 1.13e-154 Kurtosis: 5.973 Cond. No. 310. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . Wow, there&#39;s a lot of information and numbers! Here are some important terms: . . Top Section . . R-squared tells about the goodness of the fit, ranges between 0 and 1. The closer the value to 1, the better it explains the dependent variables variation in the model. However, it is biased in a way that it never decreases when we add new variables. | Adj. R-squared has a penalising factor. It decreases or stays identical to the previous value as the number of predictors increases. If the value keeps increasing on removing the unnecessary parameters go ahead with the model or stop and revert. | F-statistic used to compare two variances and the value is always greater than 0. In regression, it is the ratio of the explained to the unexplained variance of the model. | AIC stands for Akaike’s information criterion. It estimates the relative amount of information lost by a given model. The lower the AIC, the higher the quality of that model. | . . Mid Section . . coef is the coefficient/estimate value of intercept and slope. | $P&gt;|t|$ refers to the p-value of partial tests with the null hypothesis $H_0$ that the coefficient is equal to zero (no effect). A low p-value (&lt; 0.05) indicates that the predictor has significant effect to the target variable. | . . Bottom Section . . Omnibus - D&#39;Angostino&#39;s test. It provides a combined statistical test for the presence of skewness and kurtosis. | Skew informs about the data symmetry about the mean. | Kurtosis measures the shape of the distribution (i.e. the amount of data close to the mean than far away from the mean). | . . Note: From the summary result above, we can conclude that the performance of model is moderate (Adj. R-squared of 73.8%) and there are two insignificant predictors (sex and region), which align with the statement on Exploratory Data Analysis section. . Feature Selection . How we can further improve our model? One way is by feature selection using step-wise regression, specifically backward elimination and forward selection. On each step, we remove/select the predictors which results a smaller AIC. Besides that, we also have to really consider the business case also. This plays a role in making decision about which predictors are going to be used in our model. . Backward Elimination . def backwardEliminationByAIC(X, y, show_iter=True): X_step = X.copy() num_iter = 1 drop_list = [] while True: res_list = [] for col in [&#39;none&#39;] + list(X_step.columns): X_curr = X_step.drop(col, axis=1) if col != &quot;none&quot; else X_step model = sm.OLS(y, sm.add_constant(X_curr)).fit() res_list.append({&quot;drop&quot;: col, &quot;aic&quot;: model.aic}) curr_res = pd.DataFrame(res_list).sort_values(&quot;aic&quot;) col_to_be_removed = list(curr_res[&quot;drop&quot;])[0] if show_iter: print(&quot;Iteration {}: Drop {}&quot;.format(num_iter, col_to_be_removed)) display(HTML(curr_res.to_html(index=False))) if col_to_be_removed == &quot;none&quot;: break else: drop_list.append(col_to_be_removed) X_step = X_step.drop(col_to_be_removed, axis=1) num_iter += 1 X_back = X.drop(drop_list, axis=1) model_back = sm.OLS(y, sm.add_constant(X_back)) return model_back model_back = backwardEliminationByAIC(X, y) predictor_back = model_back.exog_names[1:] predictor_back . . Iteration 1: Drop sex_male . drop aic . sex_male | 27111.662 | . region_northwest | 27112.059 | . none | 27113.506 | . region_southwest | 27115.562 | . region_southeast | 27116.204 | . children | 27123.439 | . bmi | 27246.117 | . age | 27513.667 | . smoker_yes | 28790.423 | . Iteration 2: Drop region_northwest . drop aic . region_northwest | 27110.213 | . none | 27111.662 | . region_southwest | 27113.713 | . region_southeast | 27114.354 | . children | 27121.551 | . bmi | 27244.141 | . age | 27512.317 | . smoker_yes | 28792.896 | . Iteration 3: Drop none . drop aic . none | 27110.213 | . region_southwest | 27111.806 | . region_southeast | 27112.503 | . children | 27119.957 | . bmi | 27242.621 | . age | 27510.818 | . smoker_yes | 28792.253 | . [&#39;age&#39;, &#39;bmi&#39;, &#39;children&#39;, &#39;smoker_yes&#39;, &#39;region_southeast&#39;, &#39;region_southwest&#39;] . . Note: The predictors that backward elimination method recommends are age, bmi, children, smoker_yes, region_southeast, region_southwest . Forward Selection . def forwardSelectionByAIC(X, y, show_iter=True): X_step = pd.DataFrame(sm.add_constant(X)[&#39;const&#39;]) num_iter = 1 add_list = [] while True: res_list = [{&quot;add&quot;: &quot;none&quot;, &quot;aic&quot;: sm.OLS( y, sm.add_constant(X_step)).fit().aic}] for col in list(set(X.columns) - set(add_list)): X_curr = X[add_list + [col]] model = sm.OLS(y, sm.add_constant(X_curr)).fit() res_list.append({&quot;add&quot;: col, &quot;aic&quot;: model.aic}) curr_res = pd.DataFrame(res_list).sort_values(&quot;aic&quot;) col_to_be_added = list(curr_res[&quot;add&quot;])[0] if show_iter: print(&quot;Iteration {}: Add {}&quot;.format(num_iter, col_to_be_added)) display(HTML(curr_res.to_html(index=False))) if col_to_be_added == &quot;none&quot;: break else: add_list.append(col_to_be_added) X_step = X[add_list] num_iter += 1 X_forward = X[add_list] model_forward = sm.OLS(y, sm.add_constant(X_forward)) return model_forward model_forward = forwardSelectionByAIC(X, y) predictor_forward = model_forward.exog_names[1:] predictor_forward . . Iteration 1: Add smoker_yes . add aic . smoker_yes | 27665.464 | . age | 28833.949 | . bmi | 28905.564 | . region_southeast | 28951.920 | . children | 28953.062 | . sex_male | 28954.864 | . region_southwest | 28956.763 | . region_northwest | 28957.131 | . none | 28957.263 | . Iteration 2: Add age . add aic . age | 27251.324 | . bmi | 27525.890 | . children | 27653.886 | . none | 27665.464 | . region_southeast | 27666.041 | . region_southwest | 27666.760 | . region_northwest | 27667.051 | . sex_male | 27667.438 | . Iteration 3: Add bmi . add aic . bmi | 27121.836 | . children | 27242.027 | . region_southeast | 27250.719 | . none | 27251.324 | . region_southwest | 27251.930 | . region_northwest | 27252.805 | . sex_male | 27253.270 | . Iteration 4: Add children . add aic . children | 27112.035 | . region_southeast | 27121.349 | . none | 27121.836 | . region_southwest | 27122.498 | . region_northwest | 27123.102 | . sex_male | 27123.729 | . Iteration 5: Add region_southeast . add aic . region_southeast | 27111.806 | . none | 27112.035 | . region_southwest | 27112.503 | . region_northwest | 27113.443 | . sex_male | 27113.886 | . Iteration 6: Add region_southwest . add aic . region_southwest | 27110.213 | . none | 27111.806 | . sex_male | 27113.655 | . region_northwest | 27113.713 | . Iteration 7: Add none . add aic . none | 27110.213 | . region_northwest | 27111.662 | . sex_male | 27112.059 | . [&#39;smoker_yes&#39;, &#39;age&#39;, &#39;bmi&#39;, &#39;children&#39;, &#39;region_southeast&#39;, &#39;region_southwest&#39;] . set(predictor_back) == set(predictor_forward) . True . . Note: The predictors that forward selection method recommends are just the same as backward selection recommends: age, bmi, children, smoker_yes, region_southeast, region_southwest . Business Case . When building a model, we have to take into consideration about the business perspective also. According to healthcare.gov, five factors that can affect a plan’s premium are location, age, tobacco use, plan category, and whether the plan covers dependents. Insurance companies can’t charge women and men different prices for the same plan. . . Note: In conclusion, based on the step-wise regression and business case, we should have the following predictors: age, bmi, children, smoker, region. . Model Comparison . In this section, we are going to compare the following models: . Model with all predictors | Model without predictor sex | Model without predictor sex and region | All of the above models with outlier removed | We define the function experimentModel() to compare these models later. . def experimentModel(X_train, X_test, y_train, ignore_var=[]): X_train_new = X_train.drop(ignore_var, axis=1) X_test_new = X_test.drop(ignore_var, axis=1) model_new = sm.OLS(y_train, sm.add_constant(X_train_new)) result_new = model_new.fit() return X_test_new, result_new . Second and Third Model . X_test_wo_sex, result_wo_sex = experimentModel(X_train, X_test, y_train, ignore_var=[&#39;sex_male&#39;]) # 3. Model without predictor sex and region X_test_wo_sex_region, result_wo_sex_region = experimentModel(X_train, X_test, y_train, ignore_var=[ &#39;sex_male&#39;, &#39;region_northwest&#39;, &#39;region_southeast&#39;, &#39;region_southwest&#39;]) . Models without Outliers . Recall that 16.592% of our target variable are outliers. Next, we try to build the same three models above with the outliers being removed: . Model with all predictors | Model without predictor sex | Model without predictor sex and region | X_wo_outlier = X.iloc[insurance_wo_outlier.index] y_wo_outlier = insurance_wo_outlier.charges.values X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier, y_test_wo_outlier = train_test_split( X_wo_outlier, y_wo_outlier, test_size=0.2, random_state=333) print(&quot;X Train:&quot;, X_train_wo_outlier.shape) print(&quot;X Test:&quot;, X_test_wo_outlier.shape) print(&quot;y Train:&quot;, y_train_wo_outlier.shape) print(&quot;y Test:&quot;, y_test_wo_outlier.shape) . . X Train: (892, 8) X Test: (224, 8) y Train: (892,) y Test: (224,) . X_test_wo_outlier_all, result_wo_outlier_all = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier) X_test_wo_outlier_wo_sex, result_wo_outlier_wo_sex = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier, ignore_var=[&#39;sex_male&#39;]) X_test_wo_outlier_wo_sex_region, result_wo_outlier_wo_sex_region = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier, ignore_var=[ &#39;sex_male&#39;, &#39;region_northwest&#39;, &#39;region_southeast&#39;, &#39;region_southwest&#39;]) . Evaluation . In order to choose the best model, we are going to consider several metrics: . R-squared, the higher the better | Adjusted R-squared, the higher the better | Root Mean Squared Error (RMSE), the lower the better | Mean Absolute Percentage Error (MAPE), the lower the better | . def evalRegression(model, X_true, y_true, outlier=True, decimal=5): y_pred = model.predict(sm.add_constant(X_true)) metric = { &quot;Predictor&quot;: sorted(model.model.exog_names[1:]), &quot;Outlier&quot;: &quot;Included&quot; if outlier else &quot;Excluded&quot;, &quot;R-sq&quot;: round(model.rsquared, decimal), &quot;Adj. R-sq&quot;: round(model.rsquared_adj, decimal), &quot;RMSE&quot;: round(np.sqrt(metrics.mean_squared_error(y_true, y_pred)), decimal), &quot;MAPE&quot;: round(np.mean(np.abs((y_true - y_pred) / y_true)) * 100, decimal) } return metric . . eval_df = pd.DataFrame([ evalRegression(result_all, X_test, y_test), evalRegression(result_wo_sex, X_test_wo_sex, y_test), evalRegression(result_wo_sex_region, X_test_wo_sex_region, y_test), evalRegression(result_wo_outlier_all, X_test_wo_outlier_all, y_test_wo_outlier, outlier=False), evalRegression(result_wo_outlier_wo_sex, X_test_wo_outlier_wo_sex, y_test_wo_outlier, outlier=False), evalRegression(result_wo_outlier_wo_sex_region, X_test_wo_outlier_wo_sex_region, y_test_wo_outlier, outlier=False)], index=range(1, 7)) eval_df.index.name = &quot;Model&quot; eval_df . . Predictor Outlier R-sq Adj. R-sq RMSE MAPE . Model . 1 [age, bmi, children, region_northwest, region_southeast, region_southwest, sex_male, smoker_yes] | Included | 0.740 | 0.738 | 5790.481 | 40.135 | . 2 [age, bmi, children, region_northwest, region_southeast, region_southwest, smoker_yes] | Included | 0.740 | 0.739 | 5787.291 | 39.909 | . 3 [age, bmi, children, smoker_yes] | Included | 0.739 | 0.738 | 5797.358 | 40.540 | . 4 [age, bmi, children, region_northwest, region_southeast, region_southwest, sex_male, smoker_yes] | Excluded | 0.683 | 0.680 | 2582.015 | 21.894 | . 5 [age, bmi, children, region_northwest, region_southeast, region_southwest, smoker_yes] | Excluded | 0.682 | 0.679 | 2592.232 | 21.893 | . 6 [age, bmi, children, smoker_yes] | Excluded | 0.677 | 0.675 | 2602.342 | 22.538 | . We are going to select the best model out of the six models, here are the thought process: . Compare the first three models (with outlier) with the last three models (without outlier). We can see that models with outlier have better R-squared and Adjusted R-squared, but worst in terms of error. In this case, we prefer models without outlier, since the drop of MAPE is quite significant. | Out of the three models without outlier, the model with all predictors has the highest Adj. R-sq and the lowest RMSE. But according to the statistical test and business case, we should remove sex predictor. On the other hand, region must be included due to the business case even though it is not significant statistically. So, in this case we choose model 5, having the lowest MAPE. | final_X_test = X_test_wo_outlier_wo_sex final_y_test = y_test_wo_outlier final_result = result_wo_outlier_wo_sex . Regularized Linear Regression . Here, we try to improve the model 5 performance by using regularized linear regression. There are two simple techniques to reduce model complexity and prevent overfitting: . Ridge Regression, the cost function is added by a penalty weight equivalent to the square of the coefficients. This shrinks the coefficients and helps to reduce the model complexity and multicollinearity. | Lasso (Least Absolute Shrinkage and Selection Operator) Regression, which helps reducing overfitting and also used as feature selection. | def evalRegularizedRegression(model_result, X_test, y_test): model = model_result.model eval_regularized_list = [] for alpha in np.linspace(0, 10, 101): for fit_type in [0, 1]: result_regularized = model.fit_regularized(alpha=round(alpha, 2), L1_wt=fit_type, start_params=model_result.params) final = sm.regression.linear_model.OLSResults(model, result_regularized.params, model.normalized_cov_params) metric = {} metric[&quot;alpha&quot;] = alpha metric[&quot;Fit Type&quot;] = &quot;Ridge&quot; if fit_type == 0 else &quot;Lasso&quot; metric.update(evalRegression(final, X_test, y_test, outlier=False)) metric.pop(&quot;Predictor&quot;) eval_regularized_list.append(metric) return pd.DataFrame(eval_regularized_list) . . eval_regularized = evalRegularizedRegression( final_result, final_X_test, final_y_test) for metric in [&quot;Adj. R-sq&quot;, &quot;MAPE&quot;]: facet = sns.FacetGrid(eval_regularized, col=&quot;Fit Type&quot;) facet = facet.map(plt.plot, &quot;alpha&quot;, metric) facet.set_axis_labels(&quot;Penalty Weight&quot;) facet.fig.suptitle( &quot;Evaluate Regularized Linear Regression by {}&quot;.format(metric), y=1.05) . . Display alpha of Ridge and Lasso regression with maximum Adj. R-squared and minimum MAPE: . eval_regularized[(eval_regularized[&quot;Adj. R-sq&quot;] == max(eval_regularized[&quot;Adj. R-sq&quot;])) &amp; (eval_regularized[&quot;MAPE&quot;] == min(eval_regularized[&quot;MAPE&quot;]))] . . alpha Fit Type Outlier R-sq Adj. R-sq RMSE MAPE . 0 0.000 | Ridge | Excluded | 0.682 | 0.679 | 2592.232 | 21.893 | . 1 0.000 | Lasso | Excluded | 0.682 | 0.679 | 2592.232 | 21.893 | . . Note: Based on the result, it recommends us to penalize the parameters using $ alpha = 0$ which means no regularization is needed. Unfortunately, we couldn&#8217;t improve the model performance by this technique. So we stick with the previous model 5. . Model Interpretation . Here is our final model: . print(final_result.summary()) . OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.682 Model: OLS Adj. R-squared: 0.679 Method: Least Squares F-statistic: 270.6 Date: Thu, 21 Oct 2021 Prob (F-statistic): 7.44e-215 Time: 00:42:45 Log-Likelihood: -8426.1 No. Observations: 892 AIC: 1.687e+04 Df Residuals: 884 BIC: 1.691e+04 Df Model: 7 Covariance Type: nonrobust ==================================================================================== coef std err t P&gt;|t| [0.025 0.975] const -2242.1832 615.674 -3.642 0.000 -3450.537 -1033.830 age 234.3111 7.570 30.954 0.000 219.455 249.168 bmi 33.8275 18.178 1.861 0.063 -1.850 69.505 children 391.3095 83.633 4.679 0.000 227.166 555.453 smoker_yes 1.279e+04 394.893 32.378 0.000 1.2e+04 1.36e+04 region_northwest -601.5380 298.373 -2.016 0.044 -1187.140 -15.936 region_southeast -1059.6102 300.616 -3.525 0.000 -1649.615 -469.605 region_southwest -873.3672 289.939 -3.012 0.003 -1442.416 -304.318 ============================================================================== Omnibus: 731.400 Durbin-Watson: 2.077 Prob(Omnibus): 0.000 Jarque-Bera (JB): 11357.656 Skew: 3.857 Prob(JB): 0.00 Kurtosis: 18.687 Cond. No. 315. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . print(final_result.params) . const -2242.183 age 234.311 bmi 33.828 children 391.309 smoker_yes 12785.840 region_northwest -601.538 region_southeast -1059.610 region_southwest -873.367 dtype: float64 . We can interpret the coefficient of model summary as follows: . One unit increase in age, bmi, and children will increase insurance charges by 234.3, 33.8, 391.3 respectively | Being a smoker will cost 12785.84 more charges than those who aren&#39;t (what a significant increase!) | If two beneficiaries have the same age, bmi, children, and smoker, then people who are living in southeast will have the smallest charges than other region. On the other hand, people who are living in northeast will have the largest charges. | . Check Model Assumptions . The linear regression model is the most rigid model among others. Several assumptions must be met in order to satisfy the best linear unbiased estimator (BLUE). . Normality of Residual . The residuals have to distribute around mean 0 and some variance, which follows a standard normal distribution. We plot a histogram to visualize the distribution of the residuals. . def plotResidualNormality(residual): residual_histogram = sns.histplot(residual) residual_histogram.set_xlabel(&quot;Residual&quot;) residual_histogram.set_ylabel(&quot;Relative Frequency&quot;) residual_histogram.set_title(&quot;Distribution of Residuals&quot;, size=25) return residual_histogram y_pred = final_result.predict(sm.add_constant(final_X_test)) residual = final_y_test - y_pred plotResidualNormality(residual) . . Another way is to statistically test the normality using Shapiro-Wilk test. . Null Hypothesis ($H_0$): Residuals are normally distributed | Alternative Hypothesis ($H_1$): Residuals are not normally distributed | . def statResidualNormality(residual): W, pvalue = stats.shapiro(residual) print(&quot;Shapiro-Wilk Normality Test&quot;) print(&quot;W: {}, p-value: {}&quot;.format(W, pvalue)) statResidualNormality(residual) . . Shapiro-Wilk Normality Test W: 0.4523528814315796, p-value: 1.0307846384403909e-25 . . Note: From Shapiro-Wilk Normality Test, we reject the null hypothesis since the p-value &lt; 0.05 and conclude that the residuals are not normally distributed. Thus, assumption is not passed. . Homoscedasticity . The residuals have to distributed homogeneously which means residuals has a constant variance and doesn&#39;t form any pattern. We can visualize it using a scatter plot. . def plotResidualHomoscedasticity(y_true, y_pred): residual = y_true - y_pred residual_scatter = sns.scatterplot(x=y_pred, y=residual) residual_scatter.axhline(0, ls=&#39;--&#39;, c=&quot;red&quot;) residual_scatter.set_xlabel(&quot;Fitted Value&quot;) residual_scatter.set_ylabel(&quot;Residual&quot;) residual_scatter.set_title(&quot;Scatter Plot of Residuals&quot;, size=25) return residual_scatter plotResidualHomoscedasticity(final_y_test, y_pred) . . Another way is to statistically test the homoscedasticity using Breusch-Pagan test. . Null Hypothesis ($H_0$): Variation of residual is constant (Homoscedasticity) | Alternative Hypothesis ($H_1$): Variation of residual is not constant (Heteroscedasticity) | . def statResidualHomoscedasticity(X, residual): lm, lm_pvalue, fvalue, f_pvalue = sm.stats.diagnostic.het_breuschpagan( residual, np.array(sm.add_constant(X))) print(&quot;Studentized Breusch-Pagan Test&quot;) print(&quot;Lagrange Multiplier: {}, p-value: {}&quot;.format(lm, lm_pvalue)) print(&quot;F: {}, p-value: {}&quot;.format(fvalue, f_pvalue)) statResidualHomoscedasticity(final_X_test, residual) . . Studentized Breusch-Pagan Test Lagrange Multiplier: 10.58590985799841, p-value: 0.1577290355280313 F: 1.5305968436467638, p-value: 0.15816141333372216 . . Note: From Studentized Breusch-Pagan Test, we fail to reject the null hypothesis since the p-value &gt; 0.05 and conclude that variation of residual is constant (Homoscedasticity). Thus, assumption is passed. . No Multicollinearity . Multicollinearity is a condition where at least two predictors have a strong linear relationship. We expect the model to have little to no multicollinearity. It exists if the Variance Inflation Factor (VIF) value is greater than 10. . vif_list = [] for idx, col in enumerate(final_result.model.exog_names[1:]): vif_dict = {&quot;Variable&quot;: col, &quot;VIF&quot;: variance_inflation_factor(final_result.model.exog, idx+1)} vif_list.append(vif_dict) pd.DataFrame(vif_list) . . Variable VIF . 0 age | 1.039 | . 1 bmi | 1.183 | . 2 children | 1.003 | . 3 smoker_yes | 1.090 | . 4 region_northwest | 1.499 | . 5 region_southeast | 1.624 | . 6 region_southwest | 1.541 | . . Note: From VIF value, we can conclude that there is only little multicollinearity exist in our model. Thus, assumption is passed. . Conclusions . Linear regression is a highly interpretable model, meaning we can understand and account the predictors that are being included and excluded from the model. From the model summary, we know that being a smoker will cause a significant increase in beneficiary&#39;s charges than those who aren&#39;t. . In the end, we choose the model which excludes the outlier data since it gives us a better prediction (lower error performance, but the trade-off is a lower Adjusted R-squared. Using regularization worsen the overall model performance. . The normality of residual assumption is not fulfilled. This can be handled by several ways: . Try to transform the target variable using Box-Cox (logarithm or square-root function) transformation. | If we only focus on building a robust model without interpretation, then we can try ensemble methods to prevent rigid model assumptions, such as Random Forest Regressor. | .",
            "url": "https://tomytjandra.github.io/blogs/python/regression/statsmodels/2020/03/19/medical-cost-analysis-rm.html",
            "relUrl": "/python/regression/statsmodels/2020/03/19/medical-cost-analysis-rm.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Text Mining for Indonesian Online News Articles about Corona",
            "content": "Preface . Import Packages . First thing first, we import a bunch of packages used in our analysis, mainly for scraping, text preprocessing, analyzing text, and visualization. . from bs4 import BeautifulSoup import requests import pandas as pd # text preprocessing import re import dateparser from datetime import datetime from itertools import repeat from gensim.utils import simple_preprocess # analyzing text from gensim.models import Word2Vec from nltk.probability import FreqDist from sklearn.manifold import TSNE from sklearn.decomposition import PCA # visualization import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates from wordcloud import WordCloud, ImageColorGenerator from PIL import Image from adjustText import adjust_text # other import os import time import math import multiprocessing from tqdm import tqdm . Set Notebook Options . Set color of plot to match with chosen jupyterthemes style | Suppress warning of pandas chaining assignment | . from jupyterthemes import jtplot jtplot.style() pd.options.mode.chained_assignment = None . . Topic of Interest . Before we jump into scraping step, let&#39;s us define a query which will be our topic of interest. At the time of writing this post, Coronavirus is considered as the next viral pandemic of public health - tens of thousands of people are infected which causing thousands of death. Here, we are interested to see how the Indonesian news media reports on &quot;corona&quot;. . query = &quot;corona&quot; . Scraping . There will be two sites as our main source of analysis: tirto.id and detik.com. The workflow for scraping is as follow: . Access the link, followed by the search query parameter | Get the total page number found based on query | Get the title and URL for each article | Loop through each URL to scrape the content of each article and its detail (Category, Author Name, Posted Date) | Save to .tsv file | Scrape from tirto.id . Access the Link . In this step, we access tirto.id followed by the &quot;corona&quot; query and scrape the raw HTML content. . url_base = &quot;https://tirto.id&quot; url_query = url_base + &quot;/search?q=&quot; + query req = requests.get(url_query) soup = BeautifulSoup(req.content, &quot;html.parser&quot;) str(soup)[:500] . &#39;&lt;!DOCTYPE html&gt; n n&lt;html data-n-head=&#34;lang&#34; data-n-head-ssr=&#34;&#34; lang=&#34;id&#34;&gt; n&lt;head&gt; n&lt;meta charset=&#34;utf-8&#34; data-hid=&#34;charset&#34; data-n-head=&#34;true&#34;/&gt;&lt;meta content=&#34;width=device-width, initial-scale=1&#34; data-hid=&#34;viewport&#34; data-n-head=&#34;true&#34; name=&#34;viewport&#34;/&gt;&lt;meta content=&#34;yes&#34; data-hid=&#34;mobile-web-app-capable&#34; data-n-head=&#34;true&#34; name=&#34;mobile-web-app-capable&#34;/&gt;&lt;meta content=&#34;corona&#34; data-n-head=&#34;true&#34; name=&#34;title&#34;/&gt;&lt;meta content=&#34;900&#34; data-n-head=&#34;true&#34; http-equiv=&#34;refresh&#34;/&gt;&lt;title data-n-head=&#34;true&#34;&gt;Cari &#39; . Get Total Page Number . Before we loop through each page, we identify the total_page which can be found on the pagination. . try: find_pagination = soup.findAll(&quot;li&quot;, attrs={&quot;class&quot;: &quot;pagination-item&quot;}) pagination_list = [row.a.text for row in find_pagination] total_page = int(pagination_list[-2]) print(&quot;Total Page: {}&quot;.format(total_page)) except: print(&quot;Article Not Found&quot;) . Total Page: 59 . Get Title and URL . Get the title and URL for each article by looping from the first page until total_page. Store it as an article dictionary and append to tirtoid_articles list. . tirtoid_articles = [] for page_num in tqdm(range(1, total_page+1)): url = url_query + &quot;&amp;p=&quot; + str(page_num) r = requests.get(url) s = BeautifulSoup(r.content, &quot;html.parser&quot;) find_article = s.findAll(&quot;div&quot;, attrs={&quot;class&quot;: &quot;news-list-fade&quot;}) for row in find_article: article = {} article[&#39;title&#39;] = row.h1.text article[&#39;url&#39;] = url_base + row.a[&#39;href&#39;] tirtoid_articles.append(article) . 100%|██████████████████████████████████████████████████████████████████████████████████| 59/59 [00:39&lt;00:00, 1.49it/s] . print(&quot;Total Articles: {}&quot;.format(len(tirtoid_articles))) . Total Articles: 530 . tirtoid_articles[:3] . [{&#39;title&#39;: &#39;Update Virus Corona: Daftar 30 Negara yang Terjangkit COVID-19&#39;, &#39;url&#39;: &#39;https://tirto.id/-update-virus-corona-daftar-30-negara-yang-terjangkit-covid-19-eA1F&#39;}, {&#39;title&#39;: &#39;Corona Meluas di Italia, Fashion Show Giorgio Armani Digelar Online&#39;, &#39;url&#39;: &#39;https://tirto.id/-corona-meluas-di-italia-fashion-show-giorgio-armani-digelar-online-eA1X&#39;}, {&#39;title&#39;: &#39;Sembilan WNI Kru Kapal Diamond Princess Terinfeksi Virus Corona&#39;, &#39;url&#39;: &#39;https://tirto.id/-sembilan-wni-kru-kapal-diamond-princess-terinfeksi-virus-corona-eA1o&#39;}] . Get Article Details . Get article details such as article category, author name, posted date, and content by looping through each dictionary on tirtoid_articles list. . counter = 0 for article in tqdm(tirtoid_articles): counter += 1 # access the article url req_article = requests.get(article[&#39;url&#39;]) soup_article = BeautifulSoup(req_article.content, &quot;html.parser&quot;) # preprocessing html for s in soup_article([&#39;script&#39;, &#39;style&#39;]): s.decompose() for br in soup_article.find_all(&quot;br&quot;): br.replace_with(&quot; &quot;) # get article category find_category = soup_article.findAll(&quot;a&quot;, attrs={&quot;itemprop&quot;: &quot;item&quot;}) article[&#39;category&#39;] = find_category[-1].text if len(find_category) else &quot;&quot; # get author name and posted date find_author_date = soup_article.find( &quot;span&quot;, attrs={&quot;class&quot;: &quot;detail-date&quot;}) match = re.search(&quot;:[a-zA-Z . s]+-&quot;, find_author_date.text) if match is not None: article[&#39;author_name&#39;] = re.sub( r&#39; t&#39;, &#39;&#39;, match.group(0)[2:-2].title()) match = re.search(&quot; d{1,2} [a-zA-Z]+ d{4}&quot;, find_author_date.text) if match is not None: article[&#39;posted_date&#39;] = match.group(0) # get article content (but exclude the &quot;Baca juga&quot; section) find_baca_juga_section = soup_article.find( &quot;div&quot;, attrs={&quot;class&quot;: &quot;baca-holder&quot;}) try: if find_baca_juga_section is not None: row.decompose() except: pass content = &quot;&quot; article_table = soup_article.findAll( &quot;div&quot;, attrs={&quot;class&quot;: &quot;content-text-editor&quot;})[:-1] article[&#39;content&#39;] = &quot; &quot;.join( [re.sub(r&#39; s+&#39;, &#39; &#39;, row.text) for row in article_table]) . 100%|████████████████████████████████████████████████████████████████████████████████| 530/530 [12:33&lt;00:00, 1.42s/it] . Save to .tsv File . Finally, convert the list of dictionaries into DataFrame and save it into a .tsv file for further analysis. . tirtoid_df = pd.DataFrame(tirtoid_articles) tirtoid_df.to_csv(&quot;output/tirtoid_{}.tsv&quot;.format(query), sep=&quot; t&quot;, index=False) . Scrape from detik.com . Access the Link . In this step, we access detik.com followed by the &quot;corona&quot; query and scrape the raw HTML content. . url_base = &quot;https://www.detik.com&quot; url_query = url_base + &quot;/search/searchnews?query=&quot; + query req = requests.get(url_query) soup = BeautifulSoup(req.content, &quot;html.parser&quot;) str(soup)[:500] . &#39;&lt;!DOCTYPE html&gt; n n&lt;html lang=&#34;id-ID&#34;&gt; n&lt;head&gt; n&lt;meta charset=&#34;utf-8&#34;/&gt; n&lt;meta content=&#34;IE=edge&#34; http-equiv=&#34;X-UA-Compatible&#34;/&gt; n&lt;meta content=&#34;initial-scale = 1.0, user-scalable = no, width=device-width, height=device-height, maximum-scale=1.0&#34; name=&#34;viewport&#34;/&gt; n&lt;meta content=&#34;yes&#34; name=&#34;apple-mobile-web-app-capable&#34;&gt; n&lt;meta content=&#34;desktop&#34; name=&#34;platform&#34;&gt; n&lt;title&gt;detiksearch&lt;/title&gt; n&lt;!--s:dtkprv--&gt; n&lt;!--e:dtkprv--&gt;&lt;link href=&#34;https://cdn.detik.net.id/detik2/images/favicon.ico&#34; rel=&#34;shortcut icon&#34; ty&#39; . Get Total Page Number . Before we loop through each page, we calculate the total_article which can be found on the search result page. Each result page contains 9 articles, and detik only provides maximum up to 1111 pages. . try: find_total_article = soup.find(&quot;div&quot;, attrs={&quot;class&quot;: &quot;search-result&quot;}) total_article_match = re.search(&quot; d+&quot;, find_total_article.span.text) total_article = int(total_article_match.group(0)) total_page = int(math.ceil(total_article/9)) total_page = min(1111, total_page) # detik only provides max. 1111 pages print(&quot;Total Page: {}&quot;.format(total_page)) except: print(&quot;Article Not Found&quot;) . Total Page: 504 . Get Article Details . Get the details for each article by looping from the first page until total_page. Store it as an article dictionary and append to detikcom_articles list. The article details include URL, title, category, and posted date. . detikcom_articles = [] counter = 0 for page_num in tqdm(range(1, total_page+1)): counter += 1 url = url_query + &quot;&amp;page=&quot; + str(page_num) r = requests.get(url) s = BeautifulSoup(r.content, &quot;html.parser&quot;) find_article = s.findAll(&quot;article&quot;) for row in find_article: article = {} # get url article[&#39;url&#39;] = row.a[&#39;href&#39;] # get title article[&#39;title&#39;] = row.h2.text # get category find_category = row.find(&quot;span&quot;, attrs={&quot;class&quot;: &quot;category&quot;}) article[&#39;category&#39;] = find_category.text find_category.decompose() # get posted date article[&#39;posted_date&#39;] = row.find(&quot;span&quot;, attrs={&quot;class&quot;: &quot;date&quot;}).text detikcom_articles.append(article) . 100%|████████████████████████████████████████████████████████████████████████████████| 504/504 [03:00&lt;00:00, 2.79it/s] . print(&quot;Total Articles: {}&quot;.format(len(detikcom_articles))) . Total Articles: 4535 . detikcom_articles[:3] . [{&#39;url&#39;: &#39;https://health.detik.com/berita-detikhealth/d-4912250/sama-sama-virus-corona-ini-bedanya-sars-cov-2-dan-covid-19&#39;, &#39;title&#39;: &#34;Sama-sama &#39;Virus Corona&#39;, Ini Bedanya SARS-CoV-2 dan COVID-19&#34;, &#39;category&#39;: &#39;detikHealth&#39;, &#39;posted_date&#39;: &#39;Senin, 24 Feb 2020 16:50 WIB&#39;}, {&#39;url&#39;: &#39;https://news.detik.com/berita/d-4912245/jika-dievakuasi-wni-di-diamond-princess-juga-akan-diobservasi-di-pulau-sebaru&#39;, &#39;title&#39;: &#39;Jika Dievakuasi, WNI di Diamond Princess Juga Akan Diobservasi di Pulau Sebaru&#39;, &#39;category&#39;: &#39;detikNews&#39;, &#39;posted_date&#39;: &#39;Senin, 24 Feb 2020 16:46 WIB&#39;}, {&#39;url&#39;: &#39;https://hot.detik.com/art/d-4912235/koreografer-jerman-kolaborasi-bareng-seniman-indonesia-dari-5-kota&#39;, &#39;title&#39;: &#39;Koreografer Jerman Kolaborasi Bareng Seniman Indonesia dari 5 Kota&#39;, &#39;category&#39;: &#39;detikHot&#39;, &#39;posted_date&#39;: &#39;Senin, 24 Feb 2020 16:41 WIB&#39;}] . Get Article Contents . Get articles contents and author name by looping through each dictionary on detikcom_articles list. . counter = 0 for article in tqdm(detikcom_articles): counter += 1 # access the article url try: req_article = requests.get(article[&#39;url&#39;] + &quot;?single=1&quot;) except: continue soup_article = BeautifulSoup(req_article.content, &quot;html.parser&quot;) # preprocessing html for s in soup_article([&#39;script&#39;, &#39;style&#39;]): s.decompose() for br in soup_article.find_all(&quot;br&quot;): br.replace_with(&quot; &quot;) # get author name match = re.search(&quot;[a-zA-Z . s]+-&quot;, find_author_date.text) if match is not None: article[&#39;author_name&#39;] = match.group(0)[:-2].title() # get article content content = &quot;&quot; find_div = soup_article.find(&quot;div&quot;, attrs={&quot;class&quot;: &quot;detail__body-text&quot;}) if find_div is None: find_div = soup_article.find(&quot;div&quot;, attrs={&quot;class&quot;: &quot;itp_bodycontent&quot;}) if find_div is None: find_div = soup_article.find(&quot;div&quot;, attrs={&quot;class&quot;: &quot;detail_text&quot;}) if find_div is not None: article_content = find_div.findAll(&quot;p&quot;) if len(article_content) == 0: article_content = [find_div] article[&#39;content&#39;] = &quot; &quot;.join( [re.sub(r&#39; s+&#39;, &#39; &#39;, row.text) for row in article_content]) else: article[&#39;content&#39;] = &quot;&quot; . 100%|████████████████████████████████████████████████████████████████████████████| 4535/4535 [1:36:21&lt;00:00, 1.27s/it] . Save to .tsv File . Finally, convert the list of dictionaries into DataFrame and save it into a .tsv file for further analysis. . detikcom_df = pd.DataFrame(detikcom_articles) detikcom_df.to_csv(&quot;output/detikcom_{}.tsv&quot;.format(query), sep=&quot; t&quot;, index=False) . Combine Articles . In this section, we read both of the saved .tsv files from output folder and do some preprocessing as follows: . Parse the posted_date column into yyyy-mm-dd format | Replace NaN with empty string &#39;&#39; | Add new column site to identify the source of articles | tirtoid_articles = pd.read_csv(&quot;output/tirtoid_{}.tsv&quot;.format(query), sep=&quot; t&quot;, parse_dates=[&#39;posted_date&#39;], date_parser=dateparser.parse).replace(np.nan, &#39;&#39;, regex=True) tirtoid_articles[&#39;site&#39;] = &#39;tirtoid&#39; tirtoid_articles.head() . title url category author_name posted_date content site . 0 Update Virus Corona: Daftar 30 Negara yang Ter... | https://tirto.id/-update-virus-corona-daftar-3... | Kesehatan | Dipna Videlia Putsanra | 2020-02-24 | Update virus corona 24 Februari: COVID-19 tela... | tirtoid | . 1 Corona Meluas di Italia, Fashion Show Giorgio ... | https://tirto.id/-corona-meluas-di-italia-fash... | Sosial Budaya | Dewi Adhitya S. Koesno | 2020-02-24 | Akibat wabah virus corona meluas di Italia, Gi... | tirtoid | . 2 Sembilan WNI Kru Kapal Diamond Princess Terinf... | https://tirto.id/-sembilan-wni-kru-kapal-diamo... | Kesehatan | Andrian Pratama Taher | 2020-02-24 | Kapal pesiar mewah Diamond Princess tengah dik... | tirtoid | . 3 Dampak Virus Corona: Samsung Tutup Sementara P... | https://tirto.id/-dampak-virus-corona-samsung-... | Teknologi | Ibnu Azis | 2020-02-24 | Samsung tutup sementara pabrik yang bikin Gala... | tirtoid | . 4 Virus Corona 24 Februari: 2.470 Meninggal, 79.... | https://tirto.id/-virus-corona-24-februari-247... | Kesehatan | Dipna Videlia Putsanra | 2020-02-24 | Update jumlah korban virus corona: 2.470 orang... | tirtoid | . detikcom_articles = pd.read_csv( &quot;output/detikcom_{}.tsv&quot;.format(query), sep=&quot; t&quot;).replace(np.nan, &#39;&#39;, regex=True) detikcom_articles[&#39;posted_date&#39;] = detikcom_articles[&#39;posted_date&#39;].str.extract( &#39;( d{2} [A-Za-z]+ d{4})&#39;)[0].apply(dateparser.parse) detikcom_articles[&#39;site&#39;] = &#39;detikcom&#39; detikcom_articles.head() . url title category posted_date author_name content site . 0 https://health.detik.com/berita-detikhealth/d-... | Sama-sama &#39;Virus Corona&#39;, Ini Bedanya SARS-CoV... | detikHealth | 2020-02-24 | Selfie Miftahul Jannah | Sejak kemunculannya di Desember 2019 lalu, pen... | detikcom | . 1 https://news.detik.com/berita/d-4912245/jika-d... | Jika Dievakuasi, WNI di Diamond Princess Juga ... | detikNews | 2020-02-24 | Selfie Miftahul Jannah | Pemerintah masih bernegosiasi dengan pihak Jep... | detikcom | . 2 https://hot.detik.com/art/d-4912235/koreografe... | Koreografer Jerman Kolaborasi Bareng Seniman I... | detikHot | 2020-02-24 | Selfie Miftahul Jannah | Koreografer ternama Jerman, Claudia Bosse, ber... | detikcom | . 3 https://inet.detik.com/cyberlife/d-4912211/rus... | Rusia Dituding Sebar Teori Konspirasi AS Dalan... | detikInet | 2020-02-24 | Selfie Miftahul Jannah | Rusia dituding oleh pejabat Amerika Serikat ba... | detikcom | . 4 https://news.detik.com/detiktv/d-4912236/itali... | Italia Dikacaukan Virus Corona | detikNews | 2020-02-24 | Selfie Miftahul Jannah | Liga Italia dan sejumlah kegiatan yang melibat... | detikcom | . Another things to do are: . Combine both of the DataFrame into one | Convert column category and site as categorical data type | combined_articles = pd.concat( [tirtoid_articles, detikcom_articles], ignore_index=True) combined_articles[[&#39;category&#39;, &#39;site&#39;]] = combined_articles[[ &#39;category&#39;, &#39;site&#39;]].astype(&#39;category&#39;) combined_articles.dtypes . title object url object category category author_name object posted_date datetime64[ns] content object site category dtype: object . Here&#39;s the article count for each site: . combined_articles.groupby(&#39;site&#39;)[&#39;content&#39;].count() . site detikcom 4535 tirtoid 530 Name: content, dtype: int64 . Analyze Posting Pattern . Let&#39;s us analyze how tirto and detik post articles about &quot;corona&quot; from time to time. . Data Preparation . We are only interested with the articles which are posted in 2020. So, let&#39;s subset the combined_articles into articles_2020. Turns out more than 90% of the scraped articles posted in 2020, and the remaining 10% will be ignored. . start_date = &#39;2020-01-01&#39; current_date = datetime.now().strftime(&quot;%Y-%m-%d&quot;) articles_2020 = combined_articles[(combined_articles[&#39;posted_date&#39;] &gt;= start_date) &amp; ( combined_articles[&#39;posted_date&#39;] &lt; current_date)] print(&#39;Percentage of Articles before {}: ~{:.2f}%&#39;.format( start_date, 100*(1-articles_2020.shape[0]/combined_articles.shape[0]))) articles_2020.head() . . Percentage of Articles before 2020-01-01: ~10.09% . title url category author_name posted_date content site . 8 Dampak Virus Corona: Serie A Tunda Jadwal Tiga... | https://tirto.id/-dampak-virus-corona-serie-a-... | Olahraga | Gilang Ramadhan | 2020-02-23 | Wabah virus corona (covid-19) di Italia mengak... | tirtoid | . 19 Cristiano Ronaldo Samai Rekor Gabriel Batistut... | https://tirto.id/-cristiano-ronaldo-samai-reko... | Olahraga | Gilang Ramadhan | 2020-02-23 | Cristiano menjadi pemain pertama yang mencetak... | tirtoid | . 20 Allianz Antisipasi Virus Corona dengan Tingkat... | https://tirto.id/-allianz-antisipasi-virus-cor... | Sosial Budaya | Yandri Daniel Damaledo | 2020-02-21 | Allianz Indonesia melakukan sejumlah langkah a... | tirtoid | . 21 Pembangunan Kereta Cepat Indo-Cina Terhambat K... | https://tirto.id/-pembangunan-kereta-cepat-ind... | Ekonomi | Hendra Friana | 2020-02-21 | Para pekerja proyek kereta cepat Jakarta-Bandu... | tirtoid | . 22 Menkes Sebut Corona Bisa Buka Peluang Industri... | https://tirto.id/-menkes-sebut-corona-bisa-buk... | Sosial Budaya | Yandri Daniel Damaledo | 2020-02-21 | Menkes Terawan Agus Putranto menyebutkan, viru... | tirtoid | . Let&#39;s do aggregation for articles_2020 as follow: . Count articles by each site and posted_date | Total the aggregated articles_count across each row | Set posted_date as index | Replace NaN with 0 | articles_count = articles_2020.groupby([&#39;site&#39;, &#39;posted_date&#39;])[ &#39;content&#39;].count().unstack(level=0) # total articles_count.columns = articles_count.columns.add_categories([&#39;total&#39;]) articles_count[&#39;total&#39;] = articles_count.sum(axis=1) # reindex date_2020 = pd.date_range(start=start_date, end=max(articles_count.index)) articles_count = articles_count.reindex(date_2020) # replace articles_count = articles_count.fillna(0) articles_count.tail() . site detikcom tirtoid total . 2020-02-19 84.0 | 18.0 | 102.0 | . 2020-02-20 74.0 | 19.0 | 93.0 | . 2020-02-21 103.0 | 15.0 | 118.0 | . 2020-02-22 34.0 | 2.0 | 36.0 | . 2020-02-23 39.0 | 2.0 | 41.0 | . Create Line Plot . The data is now ready to be visualized using line plot. . # line plot ax = articles_count.plot(xlim=pd.Timestamp(start_date), grid=True, colormap=&#39;jet&#39;) # mean line mean_value = articles_count[&#39;total&#39;].mean() plt.axhline(y=mean_value, color=&#39;r&#39;, linestyle=&#39;--&#39;, lw=1) plt.text(s=&quot;Total count mean&quot;, x=pd.Timestamp(&#39;2020-01-04&#39;), y=mean_value + 10, color=&#39;r&#39;) # set x axis ticks ax.xaxis.set_major_locator(mdates.MonthLocator()) ax.xaxis.set_major_formatter(mdates.DateFormatter(&quot;%b&quot;)) ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=4)) ax.xaxis.set_minor_formatter(mdates.DateFormatter(&quot;%d&quot;)) plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment=&#39;right&#39;) # modify legend title handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels) # add label plt.xlabel(&#39;Posted Date&#39;) plt.ylabel(&#39;Count&#39;) plt.title(&quot;Articles Count on {} (2020)&quot;.format(query.title())) plt.show() . . . Note: Each tick on x-axis corresponds to Monday. . From the line plot, we can conclude that: . The articles on &quot;corona&quot; blew up starting from the third week of January 2020 and reach its peak on the first week of February 2020. | There is a seasonality which cycles every week. Each cycle peaks during the first half of a week and lowers during the weekend. | detik.com post &quot;corona&quot; articles about four times more frequent than tirto.id did. | . Analyze Article Contents . Text Preprocessing . Before we jump into the analysis, the scraped text must be preprocessed by: . Removing stopwords, which are generally a list of most common words used in a language | Removing numbers and punctuation | These words are removed because do not provide any useful information to decide the context of a sentences. Here are the references for Bahasa Indonesia stopwords available on data_input/stopwords-list folder: . sastrawi-stopwords.txt | pebbie-pebahasa.txt | aliakbars-bilp.txt | fpmipa-stopwords.txt | week-month-name-id.txt: self-defined stopwords containing week name and month name in Indonesian. | . stopwords_path = &quot;data_input/stopwords-list/stopwords-id&quot; stopwords_list = [] for filename in os.listdir(stopwords_path): stopwords = list( open(&quot;{}/{}&quot;.format(stopwords_path, filename)).read().splitlines()) stopwords_list.extend(stopwords) stopwords_list = sorted(set(stopwords_list)) week_month_name = list(open( &quot;data_input/stopwords-list/stopwords-id/week-month-name-id.txt&quot;).read().splitlines()) custom_stopwords = [&#39;tirto.id&#39;, &#39;baca juga&#39;, &#39;gambas&#39;, &#39;video detik&#39;, &#39;rp&#39;] . . def remove_words(sentence, words2remove): for word in words2remove: sentence = re.sub(r&#39; b&#39; + word + r&#39; b&#39;, &#39;&#39;, sentence.lower()) sentence = re.sub(r&#39; s+&#39;, &#39; &#39;, sentence).strip() return sentence def text_cleaning(text): col_name = text.name # remove stopwords print(&quot;Removing stopwords of {}&quot;.format(col_name)) time.sleep(0.5) text = list(map(remove_words, tqdm(text), repeat( stopwords_list + custom_stopwords + week_month_name))) # remove numbers and punctuations text = list(map(simple_preprocess, text)) clean_text = list(map(&#39; &#39;.join, text)) return clean_text . . clean_title = text_cleaning(articles_2020[&#39;title&#39;]) clean_content = text_cleaning(articles_2020[&#39;content&#39;]) . Removing stopwords of title . 100%|██████████████████████████████████████████████████████████████████████████████| 4554/4554 [06:05&lt;00:00, 12.45it/s] . Removing stopwords of content . 100%|██████████████████████████████████████████████████████████████████████████████| 4554/4554 [21:50&lt;00:00, 3.47it/s] . Save the preprocessed title and content to a seperate .tsv file. . articles_2020[&#39;clean_title&#39;] = clean_title articles_2020[&#39;clean_content&#39;] = clean_content articles_2020.to_csv(&quot;output/articles_2020_clean_{}.tsv&quot;.format(query), sep=&quot; t&quot;, index=False) . articles_2020_clean = pd.read_csv(&quot;output/articles_2020_clean_{}.tsv&quot;.format(query), sep=&quot; t&quot;) articles_2020_clean.head() . title url category author_name posted_date content site clean_title clean_content . 0 Dampak Virus Corona: Serie A Tunda Jadwal Tiga... | https://tirto.id/-dampak-virus-corona-serie-a-... | Olahraga | Gilang Ramadhan | 2020-02-23 | Wabah virus corona (covid-19) di Italia mengak... | tirtoid | dampak virus corona serie tunda jadwal pertand... | wabah virus corona covid italia mengakibatkan ... | . 1 Cristiano Ronaldo Samai Rekor Gabriel Batistut... | https://tirto.id/-cristiano-ronaldo-samai-reko... | Olahraga | Gilang Ramadhan | 2020-02-23 | Cristiano menjadi pemain pertama yang mencetak... | tirtoid | cristiano ronaldo samai rekor gabriel batistut... | cristiano pemain mencetak rekor gol beruntun k... | . 2 Allianz Antisipasi Virus Corona dengan Tingkat... | https://tirto.id/-allianz-antisipasi-virus-cor... | Sosial Budaya | Yandri Daniel Damaledo | 2020-02-21 | Allianz Indonesia melakukan sejumlah langkah a... | tirtoid | allianz antisipasi virus corona tingkatkan keb... | allianz indonesia langkah antisipasi virus cor... | . 3 Pembangunan Kereta Cepat Indo-Cina Terhambat K... | https://tirto.id/-pembangunan-kereta-cepat-ind... | Ekonomi | Hendra Friana | 2020-02-21 | Para pekerja proyek kereta cepat Jakarta-Bandu... | tirtoid | pembangunan kereta cepat indo cina terhambat c... | pekerja proyek kereta cepat jakarta bandung ci... | . 4 Menkes Sebut Corona Bisa Buka Peluang Industri... | https://tirto.id/-menkes-sebut-corona-bisa-buk... | Sosial Budaya | Yandri Daniel Damaledo | 2020-02-21 | Menkes Terawan Agus Putranto menyebutkan, viru... | tirtoid | menkes corona buka peluang industri farmasi ne... | menkes terawan agus putranto virus corona memb... | . Here are the list of preprocessed text to be analyze: . clean_text = list(pd.concat([articles_2020_clean[&#39;clean_title&#39;], articles_2020_clean[&#39;clean_content&#39;]]).sort_index().dropna()) clean_text[:2] . . [&#39;dampak virus corona serie tunda jadwal pertandingan&#39;, &#39;wabah virus corona covid italia mengakibatkan jadwal pertandingan serie ditunda perdana menteri italia giuseppe conte memerintahkan penundaan olahraga kawasan lombardy emilia romagna veneto penundaan mencegah penyebaran virus corona covid akibatnya jadwal pertandingan serie giornata laga inter milan vs sampdoria hellas verona vs cagliari atalanta vs sassuolo ditunda konferensi pers menteri olahraga italia vincenzo spadafora melarang wilayah izin berkomunikasi perdana menteri federasi menangguhkan olahraga provinsi terdampak virus corona spadafora dikutip tuttomercatoweb mengevaluasi keamanan maksimal penduduk sipil jam spadafora diumumkan pertandingan laga serie ascoli vs cremonese pisa vs venezia jadwal serie serie ditunda walikota milan giuseppe sala resmi menunda laga inter milan vs sampdoria diumumkan inter laman resmi jadwal laga tunda diumumkan italia mengonfirmasi korban meninggal virus corona total resmi didiagnosa terjangkit virus corona negara infeksi virus corona berasal regional lombardy veneto emilia romagna lazio turis china piedmont italia negara eropa laporan korban virus corona laporan teranyar la gazzetta dello sport positif terjangkit virus corona lombardy bertambah laga leg babak liga eropa inter milan vs ludogorets dijadwalkan terancam ditunda gazzetta pertandingan dilanjutkan dilangsungkan tertutup penjagaan ketat kompleks suning center dibutuhkan izin dokumen area olahraga stadion san siro baca allianz antisipasi virus corona tingkatkan kebersihan update virus corona dilaporkan korea polisi hong kong positif corona kepolisian karantina personel&#39;] . Word Count . We tokenize each sentences in the clean_text list and calculate the frequency of each word in our scraped articles. Here, we are only interested with the top 50 most frequent words. . top_n_words = 50 . tokens = &#39; &#39;.join(clean_text).split(&#39; &#39;) fd = FreqDist(tokens) word_freq = pd.DataFrame(list(fd.items()), columns=[&quot;Word&quot;, &quot;Frequency&quot;]) .sort_values(by=&#39;Frequency&#39;, ascending=False) top_50_words = word_freq[:top_n_words] top_50_words.head() . . Word Frequency . 1 virus | 19760 | . 2 corona | 18294 | . 80 china | 10816 | . 232 indonesia | 6208 | . 364 wuhan | 5371 | . Frequency Distribution . Let&#39;s us visualize the top_50_words using bar chart. . ax = top_50_words.plot(kind=&#39;barh&#39;, x=&#39;Word&#39;) ax.invert_yaxis() ax.get_legend().remove() # add label plt.xlabel(&#39;Frequency&#39;) plt.ylabel(&#39;Word&#39;) plt.title(&quot;Top {} Word Count on {} Articles&quot;.format( top_n_words, query.title())) plt.gcf().set_size_inches(6, 8) plt.show() . . From the bar chart, it&#39;s obvious that &#39;virus&#39; and &#39;corona&#39; will appear as the most frequent words. But there are several words from the same category being mentioned, such as: . Country/city: &#39;china&#39;, &#39;indonesia&#39;, &#39;wuhan&#39;, &#39;jakarta&#39;, &#39;natuna&#39;, &#39;singapura&#39;, &#39;jepang&#39;, &#39;as&#39;, &#39;hubei&#39; | Institute: &#39;kesehatan&#39;, &#39;pemerintah&#39;, &#39;kementerian&#39;, &#39;menteri&#39;, &#39;who&#39; | Impact: &#39;masker&#39;, &#39;kapal&#39;, &#39;penerbangan&#39;, &#39;pesawat&#39;, &#39;ekonomi&#39;, &#39;bandara&#39;, &#39;harga&#39;, &#39;pasar&#39; | . Word Cloud . To make the frequency distribution looks more visually appealing, let&#39;s make a word cloud with data_input/china-map.png as the mask. . # define mask mask = np.array(Image.open(&quot;data_input/china-map.png&quot;)) mask[mask == 0] = 255 # create wordcloud wordcloud_text = &#39; &#39;.join(clean_text) wordcloud = WordCloud( max_words=top_n_words, background_color=&quot;white&quot;, mask=mask).generate(wordcloud_text) # visualize image_colors = ImageColorGenerator(mask) plt.figure(figsize=[10, 10]) plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=&#39;bilinear&#39;) plt.axis(&quot;off&quot;) plt.show() . . Word2Vec . So far we only focus on the frequency in which each word appears, but haven&#39;t look into the context of a word yet. Word2Vec is one technique to learn word embeddings using shallow neural network, which capable to capture semantic and syntactic similarity of words. . Inspect Corpus . Let&#39;s us get some statistics of our clean corpus stored in tokens. . unique_words = set(tokens) print(&quot;Token size: {} words&quot;.format(len(tokens))) print(&quot;Unique token size: {} words&quot;.format(len(unique_words))) avg_freq_token = len(tokens)/len(unique_words) print(&quot;Average Frequency for each token: {0:.2f} times&quot;.format(avg_freq_token)) . . Token size: 690601 words Unique token size: 32879 words Average Frequency for each token: 21.00 times . Training Model . We will be using gensim packages to train the model. Here are the explanation of each parameters (Documentation): . sentences = list of tokens to be trained | size = dimensionality of word vectors | window = max. distance between the current and predicted word within a sentence | min_count = ignore tokens with total frequency below this number | workers = number of workers thread to train the model | iter = number of iteration over the corpus | seed = seed for the random number generator | . clean_token = list(map(simple_preprocess, clean_text)) model = Word2Vec(clean_token, size=500, window=5, min_count=math.ceil(avg_freq_token), workers=multiprocessing.cpu_count() - 1, iter=1000, seed=123) model.save(&quot;cache/word2vec_{}.model&quot;.format(query)) . Rather than we train the same model every time we load this notebook, let&#39;s us use the pre-trained model. . model = Word2Vec.load(&quot;cache/word2vec_{}.model&quot;.format(query)) . Dimensionality Reduction . The size of previous Word2Vec model is defined to be 500 dimensions. In order to visualize it in a plot, we have to reduce the dimensionality to 2 or 3 dimensions. In this section, we use PCA and t-SNE to reduce the dimension into 2 (x and y coordinates). The output of the two methods will be different, giving us two plots from different perspective. . Principal Component Analysis (PCA) . vocab = list(model.wv.vocab) pca = PCA(n_components=2, random_state=123) X_pca = pca.fit_transform(model[vocab]) word_plot_pca = pd.DataFrame(X_pca, index=vocab, columns=[&#39;x&#39;, &#39;y&#39;]) word_plot_pca.head() . . x y . dampak -6.705246 | 10.770574 | . virus -14.035304 | -0.235443 | . corona -15.069201 | -0.007217 | . tunda -0.249787 | -0.554645 | . jadwal -0.775143 | 1.593133 | . t-Distributed Stochastic Neighbor Embedding (t-SNE) . vocab = list(model.wv.vocab) tsne = TSNE(n_components=2, random_state=123) X_tsne = tsne.fit_transform(model[vocab]) word_plot_tsne = pd.DataFrame(X_tsne, index=vocab, columns=[&#39;x&#39;, &#39;y&#39;]) word_plot_tsne.head() . . x y . dampak -0.639069 | -1.231255 | . virus 0.360826 | 0.692630 | . corona 0.329772 | 0.735669 | . tunda -1.624454 | 1.872935 | . jadwal -0.719885 | -0.252611 | . Cluster of Words . We can visualize word_plot_pca and word_plot_tsne as cluster of words in two dimensions. Two similar words will be plotted close to each other. . fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2) ax1.scatter(word_plot_pca[&#39;x&#39;], word_plot_pca[&#39;y&#39;]) for word, row in word_plot_pca.iterrows(): ax1.annotate(word, (row[&#39;x&#39;], row[&#39;y&#39;])) ax2.scatter(word_plot_tsne[&#39;x&#39;], word_plot_tsne[&#39;y&#39;]) for word, row in word_plot_tsne.iterrows(): ax2.annotate(word, (row[&#39;x&#39;], row[&#39;y&#39;])) ax1.title.set_text(&quot;Cluster of Words (PCA)&quot;) ax2.title.set_text(&quot;Cluster of Words (T-SNE)&quot;) plt.setp([ax1, ax2], xticks=[], yticks=[]) plt.gcf().set_size_inches(15, 5) plt.tight_layout() plt.show() . . But unfortunately both of the plot looks very unpleasant, crowded with the tokens. Therefore, it&#39;s better for us to construct a word similarity network. . Word Similarity Network . In this very last section, we try to build a word similarity network for a cleaner visualization. Two words will be linked together if they are similar to each other based on cosine similarity. This value can be obtained by using model.wv.most_similar() function. Here&#39;s the explanation of each parameter for plot_word_similarity_network() function: . word_plot = DataFrame which contains the vector representation of each word | root = word of interest | neighbour = number of most similar words to be linked together | levels = max. number of connection levels, root is considered as level = 0 | . def plot_word_similarity_network(word_plot, root, neighbour, levels): fig, ax = plt.subplots() colors = &#39;rbycgmk&#39; def text_sizes(x): return 26-3*x hierarchy_dict = {} plotted_words = [root] annotation_list = [] avoid_list = [] for level in range(levels+1): if level == 0: # only plot root word coord = (word_plot.loc[root][&#39;x&#39;], word_plot.loc[root][&#39;y&#39;]) # plot point p = ax.scatter(coord[0], coord[1], s=100, c=colors[level], label=&quot;Root&quot;) avoid_list.append(p) # annotate txt = ax.text(coord[0], coord[1], root, size=text_sizes(level), color=colors[level]) annotation_list.append(txt) similar_words = [root] else: current_hierarchy_words = [] hierarchy_words = [] for word in similar_words: next_similar_words = [word for word, sim in model.wv.most_similar(word)[ :neighbour]] for sim_word in next_similar_words: if sim_word not in plotted_words: hierarchy_words.append( &quot;{} -&gt; {}&quot;.format(word, sim_word)) current_hierarchy_words.append(sim_word) coord = (word_plot.loc[sim_word][&#39;x&#39;], word_plot.loc[sim_word][&#39;y&#39;]) # plot line l = ax.annotate(&#39;&#39;, xy=(word_plot.loc[word][&#39;x&#39;], word_plot.loc[word][&#39;y&#39;]), xytext=coord, arrowprops=dict(arrowstyle=&#39;-&#39;, lw=1, color=colors[level-1])) # plot point p = ax.scatter(coord[0], coord[1], s=100, c=colors[level], label=&quot;Level {}&quot;.format(level)) avoid_list.append(p) # annotate txt = ax.text(coord[0], coord[1], sim_word, size=text_sizes(level), color=colors[level]) annotation_list.append(txt) plotted_words.append(sim_word) similar_words = current_hierarchy_words hierarchy_dict[level] = hierarchy_words # show legend, without duplicate handles, labels = ax.get_legend_handles_labels() unique = [(h, l) for i, (h, l) in enumerate( zip(handles, labels)) if l not in labels[:i]] ax.legend(*zip(*unique), loc=&quot;upper right&quot;, bbox_to_anchor=(0, 1)) # modify axis, title, and size plt.xticks([]) plt.yticks([]) plt.title(&quot;Word Similarity Network of &#39;{}&#39; n(Neighbour: {})&quot;.format( root.title(), neighbour)) plt.gcf().set_size_inches(15, 20) # repel adjust_text(annotation_list, add_objects=avoid_list, force_points=0.25, precision=0.1, arrowprops=dict(arrowstyle=&#39;&lt;-&#39;, color=&#39;k&#39;, alpha=0.1) ) plt.show() return hierarchy_dict . . hierarchy_dict_pca = plot_word_similarity_network( word_plot_pca, root=&quot;corona&quot;, neighbour=5, levels=2) . hierarchy_dict_tsne = plot_word_similarity_network( word_plot_tsne, root=&quot;corona&quot;, neighbour=5, levels=2) . hierarchy_dict_pca . {1: [&#39;corona -&gt; mematikan&#39;, &#39;corona -&gt; korona&#39;, &#39;corona -&gt; ncov&#39;, &#39;corona -&gt; covid&#39;, &#39;corona -&gt; china&#39;], 2: [&#39;mematikan -&gt; wabah&#39;, &#39;korona -&gt; sars&#39;, &#39;korona -&gt; kian&#39;, &#39;korona -&gt; manusia&#39;, &#39;korona -&gt; sentimen&#39;, &#39;ncov -&gt; virus&#39;, &#39;ncov -&gt; coronavirus&#39;, &#39;ncov -&gt; pneumonia&#39;, &#39;covid -&gt; meninggal&#39;, &#39;covid -&gt; dilaporkan&#39;, &#39;china -&gt; cina&#39;, &#39;china -&gt; negara&#39;, &#39;china -&gt; wuhan&#39;, &#39;china -&gt; indonesia&#39;]} . Both of the similarity network visualize the same connection, only with a different perspective. . From the visualization, we can analyze the first level of similarity: . &#39;ncov&#39;, &#39;korona&#39;, and &#39;covid&#39; which basically the synonym/epidemic name for &#39;corona&#39;. | &#39;china&#39; as the corona virus was first identified in Wuhan, China. | &#39;mematikan&#39; as the number of death cases keep increasing from time to time (source) | . Interestingly, the Word2Vec model can capture the fact that corona is also related to other respiratory problem such as &#39;sars&#39; (Severe Acute Respiratory Syndrome) and &#39;pneumonia&#39; (lung infection) just within two levels of similarity. . Conclusion . In this post, we successfully scrape thousands of article from tirto.id and detik.com. From there, we analyze the seasonality present in the posting pattern plot and also train a Word2Vec model to capture the context of words. There are several things that could be improved in the future: . Predict the posting pattern using time series forecasting. | Increase corpus size for a more robust model. | Using skip-gram architecture for Word2Vec model, and compare it with CBOW (Continuous Bag-of-Words). |",
            "url": "https://tomytjandra.github.io/blogs/python/web-scraping/natural-language-processing/gensim/2020/02/25/text-mining-for-indonesian-online-news-articles-about-corona.html",
            "relUrl": "/python/web-scraping/natural-language-processing/gensim/2020/02/25/text-mining-for-indonesian-online-news-articles-about-corona.html",
            "date": " • Feb 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "🚧 This page is under construction 🚧 . Graduated with double bachelor’s degree in Computer Science and Mathematics from Binus University, Jakarta. | Currently committed to dive deeper into Data Science field as an instructor, specifically Machine Learning and Deep Learning using Python and R programming language. | As an ISFJ-T, I am a detail-oriented, organized, and persistent individual who have a high willingness towards learning new knowledges and experiences. | I believe life is a non-stop learning process and learning-by-teaching is the most effective method in the process. Continuous learning is the key :) | .",
          "url": "https://tomytjandra.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tomytjandra.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}